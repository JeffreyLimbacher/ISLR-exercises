---
output:
  pdf_document: default
  html_document: default
---
# Classification {#chap4}

1. Note that
\begin{align*}
1 - p(X) &= 1 - \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \\
&= \frac{1 + e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} - \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \\
&= \frac{1}{1 + e^{\beta_0+\beta_1X}}\, .
\end{align*}
Then
\begin{align*}
\frac{p(X)}{1-p(X)} &= \frac{\frac{e^{\beta_0+\beta_1X}}{1 + e^{\beta_0+\beta_1X}}}{\frac{1}{1+e^{\beta_0+\beta_1X}}} \\
&= e^{\beta_0+\beta_1X}\, .
\end{align*}

2. Since $p_k(x)$ has the same denominator for all $k$, so we can ignore it, so we are left with
$$p_k^*(x) = \pi_k \frac{1}{\sqrt{2 \pi }\sigma} \exp \left(-\frac{1}{2 \sigma^2}(x - \mu_k)^2 \right)\, .$$
In addition, the $\frac{1}{\sqrt{2 \pi}\sigma}$ term is the same, so we just need to maximize
$$p_k^{**}(x) = \pi_k \exp \left(-\frac{1}{2 \sigma^2}(x - \mu_k)^2 \right)\, . $$
Logarithms preserve order, i.e. if $p^{**}_i(x) < p^{**}_j(x)$, then $\log(p^{**}_i(x)) < \log(p^{**}_j(x))$. Taking a log of $p^{**}_k(x)$ gives
\begin{align*}
\log(p^{**}_k(x)) &= \log \left( \pi_k \exp \left(-\frac{1}{2 \sigma^2}(x - \mu_k)^2 \right) \right) \\
&= \log(\pi_k) -\frac{1}{2 \sigma^2}(x - \mu_k)^2 \\
&= \log(\pi_k) -\frac{1}{2 \sigma^2}(x^2 - 2x \mu_k + \mu_k^2) \\
&= \log(\pi_k) - \frac{x^2}{2 \sigma^2} + \frac{x \mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} \, .
\end{align*}
Since $\sigma^2$ is constant, we have that the $\frac{x^2}{2 \sigma^2}$ term is the same for all $\log(\pi^{**}(x))$ terms, so we can ignore it when maximizing. Removing this term from the last equation gives
$$ \delta_k(x) = \log(\pi_k) + \frac{x \mu_k}{\sigma^2} -\frac{\mu_k^2}{2 \sigma^2} \, .$$
From this we can conclude that maximizing our original equation is equivalent to maximizing the last equation.

3. We can follow the arguments as the last problem, but we will end up with
\begin{equation}
p_k^*(x) = \pi_k \frac{1}{\sqrt{2 \pi }\sigma_k} \exp \left(-\frac{1}{2 \sigma^2}(x - \mu_k)^2 \right)\, .
(\#eq:sigmak)
\end{equation}
Note that we can't drop the $\sigma_k$ as in problem 2 since it is unique for each population. Taking the log of the \@ref(eq:sigmak) we get
\begin{align}
\log(p_k^*(x)) &= \log(\pi_k) - \log(\sqrt(2 \pi) \sigma_k) + \frac{1}{2 \sigma_k^2} (x - \mu_k)^2 \nonumber \\
&= \log(\pi_k) - \log(\sqrt(2 \pi) \sigma_k) + \frac{1}{2 \sigma_k^2} (x^2 - 2x\mu_k + \mu_k^2) \nonumber \\
\end{align}
Note that we cannot drop the $1/(2 \sigma_k^2)$ term since it varies between each group. Therefore, the above equation is quadratic.

4. 
    a. The question is essentially asking us to compute $P(|X_2-X_1|<.05)$ where $X_2$ is our current observation and $X_1$ is our data. However, there is some issues along the edges of the interval that make it annoying to compute. I am going to ignore these since it won't really add much more value even though the answer is more accurate.
    Given some $x$, the probability that our observation falls within $\pm 0.05$ of $x$ is $P(x-0.05 \leq X \leq x+0.05) = .1$. 
    b. Same as before, given $(x_1,x_2) \in \mathbb{R}^2$, $P(x_1 - 0.05 \leq X_1 \leq x_1 + 0.05, x_2 - 0.05 \leq X_2 \leq x_2 + 0.05)$ $=P(x_1 - 0.05 \leq X_1 \leq x_1)P(x_2 - 0.05 \leq X_2 \leq x_2 + 0.05) $ $=P(x_1 - 0.05 \leq X_1 \leq x_1)^2 = 0.1^2 = 0.01$. I used independence to separate the probability into two probabilities, and then use i.i.d. assumption to conclude that the probabilities were equal.
    c. It's clear what the pattern is at this point, $P(x_1 - 0.05 \leq X_1 \leq x_1 + 0.05, \dots, x_{100} - 0.05 \leq X_{100} \leq x_{100} + 0.05) = P(x_1 - 0.05 \leq X_1 \leq x_1 + 0.05) ^ 100 = 0.1^{100}$. That number is miniscule.
    d. We can expect to find a very small amount of training data "near" our data set when $p$ is large. This means that we are using observations that are completely unlike our prediction value to try and guess the characteristics of $Y$.
    e. If $l$ is the length of hypercube ($l \leq 1$), then $P(x_1 - \frac{l}{2} \leq X_1 \leq x_1 + \frac{l}{2}, \dots, x_p - \frac{l}{2} \leq X_p \leq x_p + \frac{l}{2}) = l^p$. Thus, if we want 10%, we need to find $l$ such that $\sqrt[p]{.1} = l$. I use `R` to calculate the lengths for some values of $p$.
    ```{r include=TRUE, echo=TRUE}
    p=c(1,2,3,10,25,50,100)
    (cbind(p,.1^(1/p)))
    ```
    
5.
    a. Initially, I expected the QDA to outperform the LDA on every training set (similar to least squares), but below is an example where the QDA performs worse on the training set.
    ```{r include=TRUE, echo=TRUE}
    require(MASS)
    set.seed(1)
    n <- 1000
    n1 <- 500
    n2 <- n-n1
    x <- c(rnorm(n1,-.25,1),rnorm(n2,.25,1))
    y <- factor(rep(c(0,1),times=c(n1,n2)))
    lda_wrong <- sum(predict(lda(y~x))$class!=y)
    qda_wrong <- sum(predict(qda(y~x))$class!=y)
    cbind(LDA=lda_wrong,QDA=qda_wrong)
    ```
    It's a little hard to say which is expected to do better, but I think the the LDA is better than QDA in general. The reason I suspect this has to do with the way the variance is calculated. If the Bayes decision boundary is linear, then know that for all $k$, $\sigma_k=\sigma$. So when we estimate $\sigma$, we can use the observations from all the different populations. But when we do QDA, we have to estimate $\sigma_k$ differently for each $k$. This means that we have to divide up our observations to estimate each $\sigma_k$. Thus, when using QDA, we might have a bunch of $\sigma_k$ that are slightly off from $\sigma$, creating a source of error. 
    I think it is safe to concldue that LDA would do better on the test set. Otherwise, why even bother doing LDA?
    b. This question depends on the non-linearity of the boundary. In general, we would expect QDA to outperform LDA on both training and test sets. However, one can image a tiny pertubration to linearity might still be better suited for LDA. For example, if $\sigma_1=\sigma+\epsilon$ and $\sigma_2=\sigma$ where $\epsilon$ is some number small relative to $\sigma$.
    c. We would expect the test accuracy of QDA to improve versus LDA. This is because as we get more $n$, we get much better estimates to $\sigma_k$. If it turns out that $\sigma_k=\sigma$ for all $k$, then QDA will still capture this really well. Especially since variance estimates have diminishing returns relative to $n$.
    d. False, for the reasons stated in a.
    
6.
    a.
    ```{r include=TRUE, echo=TRUE}
    b_0 <- -5; b_1 <- 0.05; b_2 <- 1
    p <- exp(b_0 + b_1*40 + b_2*3.5)
    p/(1+p)
    ```
    b. Note if $p = .5$, that means that
    $$ \frac{1}{2} = \frac{\exp(\beta \cdot x)}{1+\exp(\beta \cdot x)} $$
    or that $\exp(\beta \cdot x) = 1$. Note that I switched to vector notation. Taking logs of both sides, we have $\beta \cdot x = 0$. Going back to our problem-specific model, we have $\beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0$. Solving for $x_2$ gives
    $$x_2 = \frac{-\beta_0 - \beta_2 x_2}{\beta_1}$$
    ```{r include=TRUE, echo=TRUE}
    (-b_0 - b_2*3.5)/b_1
    ```
    
7. 
```{r include=TRUE, echo=TRUE}
sigma <- 6
mu1 <- 10
mu2 <- 0
pi1 <- .8
pi2 <- 1 - pi1
f1 <- dnorm(4,mu1,sigma)
f2 <- dnorm(4,mu2,sigma)
pi1*f1/(pi1*f1+pi2*f2)
```

8. Since we can expect a 1-nearest neighbor to have perfect test error, we know and the data set is divided evenly, that the test error was 36%, so we would go with the logsitic regression.

9.
    a. Let $O$ be the odds. $1/(1-p) = O$ gives $p=O/(1+O)$.
    ```{r include=TRUE, echo=TRUE}
    .37/(1+.37)
    ```
    b. 
    ```{r include=TRUE, echo=TRUE}
    .16/(1-.16)
    ```
    
10.
```{r include=TRUE, echo=TRUE}
data(Weekly,package="ISLR")
head(Weekly)
```
    a.
    ```{r include=TRUE, echo=TRUE}
    summary(Weekly)
    cor(Weekly[,-c(1,9)])
    ```
    This graph is all pairwise comparisons of the predictors with the color denoting the direction. It's hard to see a trend here.
    ```{r include=TRUE, echo=TRUE}
    pairs(Weekly[-(8:9)],col=Weekly$Direction)
    ```
    Plotting the predictors against the `Today` variable also makes it hard to see any trends. 
    ```{r include=TRUE, echo=TRUE}
    par(mfcol=c(3,2))
    predictors <- c(paste0("Lag",1:5),"Volume")
    for (p in predictors){
      plot(Weekly[,p],Weekly[,"Today"],
           ylab="Today",
           xlab=p,
           col=Weekly[,"Direction"])
    }
    ```
    Boxplots prove unhelpful too.
    ```{r include=TRUE, echo=TRUE}
    par(mfcol=c(3,2))
    for(p in predictors){
      boxplot(as.formula(paste0(p,"~Direction")),data=Weekly)
    }
    ```
    I'm out of ideas for plots. This is not an easy data set to work with.
    b.
    ```{r include=TRUE, echo=TRUE}
    glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,data=Weekly, family=binomial)
    summary(glm.fit)
    ```
    The Lag2 is statistically significant at a p-value of .03, but this is a pretty low bar considering we have 6 predictors.
    c. 
    ```{r include=TRUE, echo=TRUE}
    glm.probs <- predict(glm.fit,type="response")
    glm.pred <- rep("Down",nrow(Weekly))
    glm.pred[glm.probs > .5] <- "Up"
    table(glm.pred,Weekly$Direction) #confusion table
    sum(glm.pred==Weekly$Direction)/nrow(Weekly) #overall fraction of correct answers
    ```
    From the confusion table, we can see that our model is simply predicting `Up` a lot of times. Our overall accuracy of 56% isn't bad. Since the data is roughly divided between down and up, the overall accuracy is a good measure of how good our model is. Our model seems to say that just saying the market is going up is a pretty good bet most of the time.
    d.
    ```{r include=TRUE, echo=TRUE}
    train.inds <- Weekly$Year <= 2008
    train.data <- Weekly[train.inds,]
    test.data <- Weekly[!train.inds,]
    test.n <- nrow(test.data)
    lag2.mod <- glm(Direction ~ Lag2, data=Weekly, family=binomial, subset=train.inds)
    summary(lag2.mod)
    ```
    ```{r include=TRUE, echo=TRUE}
    lag2.pred <- rep("Down",test.n)
    lag2.probs <- predict(glm.fit,test.data,type="response")
    lag2.pred[lag2.probs>.5] <- "Up"
    table(lag2.pred,test.data$Direction)
    sum(lag2.pred==test.data$Direction)/test.n
    ```
    e. 
    ```{r include=TRUE, echo=TRUE}
    library(MASS) #For lda function
    lag2.lda<-lda(Direction ~ Lag2, data=train.data)
    lag2.lda.pred <- predict(lag2.lda, test.data)$class
    table(lag2.lda.pred,test.data$Direction)
    sum(lag2.lda.pred==test.data$Direction)/test.n
    ```
    f.
    ```{r include=TRUE, echo=TRUE}
    library(MASS) #For lda function
    lag2.qda<-qda(Direction ~ Lag2, data=train.data)
    lag2.qda.pred <- predict(lag2.qda, test.data)$class
    table(lag2.qda.pred,test.data$Direction)
    sum(lag2.qda.pred==test.data$Direction)/test.n
    ```
    g. 
    ```{r include=TRUE, echo=TRUE}
    library(class)
    set.seed(1)
    train.X <- as.matrix(train.data[,"Lag2"])
    train.Y <- as.matrix(train.data[,"Direction"])
    test.X <- as.matrix(test.data[,"Lag2"])
    test.Y <- as.matrix(test.data[,"Direction"])
    lag2.knn.pred <- knn(train.X,test.X,train.Y,k=1)
    table(lag2.knn.pred,test.Y)
    sum(lag2.knn.pred == test.Y)/test.n
    ```
    h. It appears that both the LDA and the logistic regression performed similarly on the test set. Let's see which one has lower training error.
    ```{r include=TRUE, echo=TRUE}
    glm.probs <- predict(lag2.mod) #logistic regression
    glm.preds <- rep("Down",length(glm.probs))
    glm.preds[glm.probs>.5] <- "Up"
    glm.train.acc <- sum(glm.preds == train.data$Direction)/nrow(train.data)
    lda.preds <- predict(lag2.lda,type="response")$class
    lda.train.acc <- sum(lda.preds == train.data$Direction)/nrow(train.data)
    cbind(lda.train.acc,glm.train.acc)
    ```
    Since the LDA has higher training accuraccy, I will say that the LDA provides the best results.
    i. I will simply experiment with different values of $K$.
    ```{r include=TRUE, echo=TRUE}
    k.vals <- 1:50
    err <- double(50)
    for (k in k.vals){
      train.X <- as.matrix(train.data[,"Lag2"])
      train.Y <- as.matrix(train.data[,"Direction"])
      test.X <- as.matrix(test.data[,"Lag2"])
      test.Y <- as.matrix(test.data[,"Direction"])
      lag2.knn.pred <- knn(train.X,test.X,train.Y,k=k)
      table(lag2.knn.pred,test.Y)
      err[k]<-sum(lag2.knn.pred == test.Y)/test.n
    }
    plot(k.vals,err,type="b")
    ```
    The results are everywhere, but surprisingly, it seems like $k=1$ gives the best result on the test data.
    
11.
```{r include=TRUE, echo=TRUE}
data(Auto,package="ISLR")
head(Auto)
```
    a. 
    ```{r include=TRUE, echo=TRUE}
    Auto01 <- data.frame(Auto,mpg01=(as.integer(Auto$mpg >= median(Auto$mpg))))
    ```
    b. After fiddling around, I felt like boxplots were the best plot for almost all the views. The only view I don't use a boxplot for is `mpg01 ~ origin`. It seems like a lot of the variables are pretty good predictors. 
    ```{r include=TRUE, echo=TRUE}
    par(mfcol=c(2,3))
    for (p in 2:7){
      var<-names(Auto01)[p]
      f <- as.formula(paste0(var,"~mpg01"))
      boxplot(f ,xlab=names(Auto01)[p], data = Auto01)
    }
    plot(factor(Auto01$mpg01),factor(Auto01$origin),xlab="mpg01",ylab="origin")
    ```
    c. I will split the data into a 60% training set and a 40% test set.
    ```{r include=TRUE, echo=TRUE}
    set.seed(927)
    n <- nrow(Auto01)
    sample.inds<-sample.int(n, round(n*.6))
    train<-Auto01[sample.inds,]
    test<-Auto01[-sample.inds,]
    ```
    d. I exclude acceleration because the association appears weak. In addition, I leave out origin since I am not sure how it handles factor variables. The test error comes out to 11.5%.
    ```{r include=TRUE, echo=TRUE}
    require(MASS)
    form <- mpg01 ~ cylinders + horsepower + displacement + weight + year
    lda.mod <- lda(form, data=train)
    lda.pred <- predict(lda.mod,test)$class
    sum(lda.pred != test$mpg01)/nrow(test) # test error
    ```
    e. The test error for the QDA s 12.7%.
    ```{r include=TRUE, echo=TRUE}
    qda.mod <- qda(form, data=train)
    qda.pred <- predict(qda.mod,test)$class
    sum(qda.pred != test$mpg01)/nrow(test)
    ```
    f. Logistic regression gives 9.5% test error.
    ```{r include=TRUE, echo=TRUE}
    log.mod <- glm(form, data=train, family=binomial)
    log.pred <- predict(log.mod, test, type="response")
    log.pred <- ifelse(log.pred > .5, 1, 0)
    sum(log.pred != test$mpg01)/nrow(test)
    ```
    g. It appears that $k=10$ is probably the best choice.
```{r echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
    require(class)
    set.seed(927)
    res<-data.frame(ks=1:20,errs=1:20)
    preds<-c(2:4,6)
    for(i in seq(along=res$ks)){
      k <- res$ks[i]
      knn.pred <- knn(train[,preds],test[,preds],train[,"mpg01"],k=k)
      res$errs[i] <- sum(knn.pred!=test$mpg01)/nrow(test)
    }
    plot(res$ks,res$err,type="b")
    head(res[order(res$errs),])
    ```
    
12. 
    a.
    ```{r include=TRUE, echo=TRUE}
    Power <- function() print(2^3) 
    Power()
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    Power2 <- function(x,a) print(x^a)
    Power2(3,8)
    ```
    c.
    ```{r include=TRUE, echo=TRUE}
    Power2(10,3)
    Power2(8,17)
    Power2(131,3)
    ```
    d.
    ```{r include=TRUE, echo=TRUE}
    Power3 <- function(x,a) x^a
    ```
    e.
    ```{r include=TRUE, echo=TRUE}
    plot(1:10,Power3(1:10,2),main="f(x)=x^2",xlab="x",ylab="x^2",log="y")
    ```
    f.
    ```{r include=TRUE, echo=TRUE}
    PlotPower <- function(x, a) plot(x,Power3(x,a),main=paste0("f(x)=x^",a),ylab=paste0("x^",a))
    PlotPower(1:10,3)
    ```
    
13. 
```{r include=TRUE, echo=TRUE}
data(Boston,package="MASS")
head(Boston)
```
create our indicator variable to see if crime is above or below the median
```{r include=TRUE, echo=TRUE}
Boston$crim01 <- Boston$crim > median(Boston$crim)
summary(Boston$crim01)
```
Let's look for relationships
```{r include=TRUE, echo=TRUE}
bp_vars <- c("zn", "indus", "nox", "rm","age","dis","tax","ptratio","black","lstat","medv") # variables I want to make boxplots of
par(mfcol=c(3,4))
for (v in bp_vars) {
  boxplot(as.formula(paste0(v,"~ crim01")),data=Boston,main=v)
}
```