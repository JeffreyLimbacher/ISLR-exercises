---
output:
  pdf_document: default
  html_document: default
---

# Linear Model Selection and Regularization {#chap6}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1.
    a. Best subset will have the best training RSS. This is because it exhaustively searches every possible choice, whereas forward and backward-selection do not, so they might miss the optimal choice.
    b. There is not enough information to conclude which one will have the smallest test RSS.
    c.
        i. True. Forward selection works by retaining the selectors chosen in the previous step.
        ii. True. Backward selection removes the worst predictor at each step, retaining the predictors at the previous step.
        iii. False. Backwards might select a different set than the forward selection at a given $k$.
        iv. False.
        v. False. Best subset might replace a predictor going from $k$ to $k+1$. See Table 6.1 on page 209.

2.
    a. iii is Correct. The tuning parameter allows us to select a simpler model. Since simpler models have less bias, we expect the error to go down as long as the increase in variance is less than the increase in bias.
    b. iii, for the same reason.
    c. ii is correct for non-linear models. They can fit a wider class of functions than just linear models (often linear is a subset). This means the model does not bias towards a specific class of functions. However, this comes at the cost of increased variance.

3.
    a. iv. is correct. As we increase $s$, we allow the $\beta_j$ more freedom is minimizing the square residuals. The more freedom we allow, the better we will fit the training data.
    b. ii. is correct. At this point, we are facing the bias-variance trade-off which comes in the usual U-shaped curve. 
    c. iii. We allow more flexibility in the model which means we will have more variance.
    d. iv. As we increase $\lambda$, we bias away from only allowing a few large (relative to their least squares values) coefficients.
    e. v. The irreducible error is not affected by the choice of model.
    
4. These answers are the same as 3.
5. My solution to this is a little hand-wavey and not nearly rigorous enough. I will have to come back to it if it really irks me.
    a. We wish to minimize 
    \begin{equation}
    (y_1 - \beta_1 x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_1 x_{21} - \beta_2 x_{22})^2 - \lambda( \beta_1^2 + \beta_2^2).
    \end{equation}
    b. If $x_{11} = x_{12}$ and $x_{21}=x_{22}$, then we can write 
    \begin{equation}
    \begin{split}
    (y_1 - \beta_1 x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_1 x_{21} - \beta_2 x_{22})^2 = \\
    (y_1 - x_{11}(\beta_1 + \beta_2))^2 + (y_2 - x_{21}(\beta_1 + \beta_2 ))^2
    \end{split}
    (\#eq:rewritten)
    \end{equation}
    Then we want to minimize
    \begin{equation}
    (y_1 - x_{11}(\beta_1 + \beta_2))^2 + (y_2 - x_{21}(\beta_1 + \beta_2 ))^2 + \lambda(\beta_1^2+\beta_2^2)
    \end{equation}
    there is some $\beta_1 + \beta_2 = c$ that minimizes the above solution. However, there are ininfitely many $\beta_1,\beta_2$ combinations such that $\beta_1 + \beta_2 = c$. Then note that $\beta_1 = c - \beta_2$. Plugging into $\lambda(\beta_1^2 - (c-\beta_1)^2)$ which has a known minimum at $\beta_1 = \beta_2$, thus there is only one solution, that is $\beta_1 = \beta_2$.
    c. 
    \begin{equation}
    (y_1 - \beta_1x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_1 x_{21} - \beta_2 x_{22})^2 - \lambda( |\beta_1| + |\beta_2|).
    \end{equation}
    d. Same argument as before, but now we have that 
    \begin{equation}
    \begin{split}
    (y_1 - \beta_1x_{11} - \beta_2 x_{12})^2 + (y_2 - \beta_1 x_{21} - \beta_2 x_{22})^2 = \\
    (y_1 - x_{11}(\beta_1 + \beta_2))^2 + (y_2 - x_{21}(\beta_1 + \beta_2 ))
    \end{split}
    \end{equation}
    and
    \begin{equation}
    (y_1 - x_{11}(\beta_1 + \beta_2))^2 + (y_2 - x_{21}(\beta_1 + \beta_2 ))^2 + \lambda(|\beta_1|+|\beta_2|).
    \end{equation}
    Now note that there is some $\beta_1+\beta_2 = c$ that minimizes the above equation. However, now $|\beta_1| + |\beta_2|$ has an infiniten umber of minimizers. Any such $\beta_1$ and $\beta_2$ such that $\beta_1+\beta_2=c$ works. Note that if $\beta_2 = c$, then $\beta_1=0$. Likewise, we can have $\beta_1 = c$ and $\beta_2 = 0$. The valid solutions are on the line $\beta_1 + \beta_2 = c$ such that $0 \leq \beta_1,\beta_2 \leq c$.

6. 
    a. The plot below shows $\beta$ versus $(y_1 - \beta)^2 + \lambda \beta^2$ on the solid black line. The red dotted line corresponds to where $y_1/(1+\lambda)$ is. We can see that the ridge regression value is minimized at the red dotted line.
    ```{r include=TRUE, echo=TRUE}
    y1 <- 1 #y
    l <- 10 #lambda
    betas <- seq(0,1,.01)
    estimates <- (y1-betas)^2+l*betas^2
    plot(betas,estimates,type="l")
    abline(v=y1/(1+l),col="red",lty=2)
    ```
    b.The same plot is shown below. Since there is three cases, I made a plot for each
    ```{r include=TRUE, echo=TRUE}
    par(mfcol=c(2,2))
    ys=c(-1,0,1)
    for(y1 in ys){
      l <- 1 #lambda
      betas <- seq(-1,1,.01)
      estimates <- (y1-betas)^2+l*abs(betas)
      plot(betas,estimates,type="l")
      best_est <- y1 - l/2
      if(y1 < -l/2) best_est <- y1 + l/2
      else if(abs(y1) <= l/2) best_est<-0;
      abline(v=best_est,col="red",lty=2)
    }
    ```
    
7.
    a. The likelihood for this function will be 
    \begin{equation}
    \begin{split}
    L &= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{(y_i - (\beta_0 + \sum_{j=1}^px_{ij}\beta_j))^2}{2\sigma^2} \right)\\
    &= \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij}\beta_j))^2}{2 \sigma^2} \right)
    \end{split}
    \end{equation}
    b. The posterior is
    \begin{equation}
    \begin{split}
    &\left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij}\beta_j))^2}{2 \sigma^2} \right) \prod_{j=1}^n \frac{1}{2b} \exp \left(\frac{-|\beta_j|}{b} \right)\\
    =& \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij}\beta_j))^2 - \frac{2\sigma^2}{b} \sum_{j=1}^n |\beta_j|}{2 \sigma^2} \right)
    \end{split}
    (\#eq:likelihood)
    \end{equation}
    c. Looking at this last equation, we can see that if we let $\lambda = \frac{2 \sigma^2}{b}$, then the when we minimize
    \begin{equation}
    \sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij}\beta_j))^2 - \lambda \sum_{j=1}^n |\beta_j|
    \end{equation}
    then we maximize \@ref(eq:likelihood). Maximizing \@ref(eq:likelihood) is equivalent to the mode.
    d.
    \begin{equation}
    \begin{split}
    &\left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij}\beta_j))^2}{2 \sigma^2} \right) \prod_{j=1}^n \frac{1}{\sqrt{2 \pi c}} \exp \left(-\frac{\beta_j^2}{2c} \right) \\
    =& \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij} \beta_j))^2 + \frac{\sigma^2}{c} \sum_{j=1}^p \beta_j^2}{2 \sigma^2} \right)
    \end{split}
    (\#eq:likelihoodridge)
    \end{equation}
    e. Now let $\lambda = \frac{\sigma^2}{c}$. 
    \begin{equation}
    \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left(-\frac{\sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij} \beta_j))^2 + \lambda \sum_{j=1}^p \beta_j^2}{2 \sigma^2} \right)
    \end{equation}
    Ridge regression minimizes
    \begin{equation}
    \sum_{i=1}^n (y_i - (\beta_0 + \sum_{j=1}^p x_{ij} \beta_j))^2 + \lambda \sum_{j=1}^p \beta_j^2
    (\#eq:quad)
    \end{equation}
    and therefore minimizes \@ref(eq:likelihoodridge), so is the mode. It is also the mean since the exponent \@ref(eq:quad) is a quadratic function which is symmetric around its minimum.
    
8.
    a.
    ```{r include=TRUE, echo=TRUE}
    set.seed(927)
    x <- rnorm(100)
    eps <- rnorm(100)
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    b_0 <- 2; b_1 <- 2; b_2 <- .5; b_3 <- .5
    y <- b_0 + b_1 * x + b_2 * x^2 + b_3 * x^3
    ```
    c.
    ```{r include=TRUE, echo=TRUE}
    library(leaps)
    regfit.full <- regsubsets(y ~ poly(x, degree=10, raw=TRUE), data=data.frame(x,y),nvmax=11)
    reg.summary <- summary(regfit.full)
    print("Best adjr2 model")
    coef(regfit.full, id=which.max(reg.summary$adjr2))
    print("Best BIC model")
    coef(regfit.full, id=which.min(reg.summary$bic))
    print("Best Cp model")
    coef(regfit.full, id=which.min(reg.summary$cp))
    ```
    We can see that they all return the 3 variable model, which is good. Below is a plot of the three different measures. 
    ```{r include=TRUE, echo=TRUE}
    par(mfcol=c(2,2))
    plot(reg.summary$adjr2,xlab="Number of Variables", ylab="Adjusted R^2", type="l")
    plot(reg.summary$bic,xlab="Number of Variables", ylab="BIC", type="l")
    plot(reg.summary$cp,xlab="Number of Variables", ylab="AIC", type="l")
    ```
    We can see that the plots all cap out or minimize basically at three. 
    d. 
    ```{r include=TRUE, echo=TRUE}
    regfit.fwd <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data.frame(x,y),nvmax=11, method="forward")
    reg.summary.f <- summary(regfit.fwd)
    print("Best foward adjr2 model")
    coef(regfit.full, id=which.max(reg.summary.f$adjr2))
    print("Best forward BIC model")
    coef(regfit.full, id=which.min(reg.summary.f$bic))
    print("Best forward Cp model")
    coef(regfit.full, id=which.min(reg.summary.f$cp))
    ```
    The forward still returns the three variable models.
    ```{r include=TRUE, echo=TRUE}
    regfit.bwd <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data.frame(x,y),nvmax=11, method="backward")
    reg.summary.b <- summary(regfit.bwd)
    print("True coefficients")
    c(b_0,b_1,b_2,b_3)
    print("Best backward adjr2 model")
    coef(regfit.full, id=which.max(reg.summary.b$adjr2))
    print("Best backward BIC model")
    coef(regfit.full, id=which.min(reg.summary.b$bic))
    print("Best backward Cp model")
    coef(regfit.full, id=which.min(reg.summary.b$cp))
    ```
    The backwards also chooses the 3 variable models. Let's compare the results for the BIC to see if they were the same for all the models.
    ```{r include=TRUE, echo=TRUE}
    rbind(full=reg.summary$bic,
          forward=reg.summary.f$bic,
          backward=reg.summary.b$bic)
    ```
    e. 
    ```{r include=TRUE, echo=TRUE}
    library(glmnet)
    set.seed(1)
    x.mod <- model.matrix(y ~ poly(x,degree=10,raw=TRUE), data=data.frame(x=y,y=y))[,-1]
    cv.out <- cv.glmnet(x.mod,y,alpha=1)
    plot(cv.out)
    bestlam<-cv.out$lambda.min
    lambda.mod <- glmnet(x.mod,y,alpha=1)
    predict(lambda.mod,type="coefficients",s=bestlam)
    ```
    The results of the Lasso aren't great. I imagine this is because the linear model is perfect, so I don't see how the Lasso would improve on anything.
    f. 
    ```{r include=TRUE, echo=TRUE}
    b_7 <- 2.5
    y7 <- b_0 + b_0 * x^7 + eps
    deg7.full <- regsubsets(y7 ~ poly(x,degree=10,raw=TRUE),data.frame(x,y7))
    print("Best Cp model")
    coef(deg7.full, id=which.min(reg.summary$cp))
    x7.mod <- model.matrix(y7 ~ poly(x,degree=10), data=data.frame(x,y7))[,-1]
    cv.out <- cv.glmnet(x7.mod,y7,alpha=1)
    bestlam <- cv.out$lambda.min
    lambda.mod <- glmnet(x.mod,y7,alpha=1)
    predict(lambda.mod,type="coefficients",s=bestlam)
    ```
    Once again, the regular linear regression out performs the lasso. Lasso is performing pretty terribly. Am I doing something wrong?
    
9.
```{r include=TRUE, echo=TRUE}
data(College,package="ISLR")
n<-nrow(College)
```
    a.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    train <- sample(1:n, n/2)
    test <- (-train)
    #reference MSE if using just the mean
    y.test <- College$Apps[test]
    y.train <- College$Apps[train]
    (ref.err<-mean((mean(y.test)-y.test)^2))
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    lm.mod <- lm(Apps ~ ., data=College[train,])
    lm.pred <- predict(lm.mod, College[test,])
    (lm.err<-mean((lm.pred-y.test)^2))
    ```
    c.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    x.mod <- model.matrix(Apps ~ ., data=College)[,-1]
    ridge.mod.cv<-cv.glmnet(x.mod[train,],y.train,alpha=0)
    ridge.best.lam <- ridge.mod.cv$lambda.min
    ridge.mod <- glmnet(x.mod[train,],y.train,alpha=0)
    ridge.pred <- predict(ridge.mod,newx=x.mod[test,],s=ridge.best.lam)
    (ridge.err<-mean((ridge.pred-y.test)^2))
    ```
    d.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    x.mod <- model.matrix(Apps ~ ., data=College)[,-1]
    lasso.mod.cv <- cv.glmnet(x.mod[train,],y.train,alpha=1)
    lasso.best.lam <- lasso.mod.cv$lambda.min
    lasso.mod <- glmnet(x.mod[train,],y.train,alpha=1)
    lasso.pred <- predict(lasso.mod,newx=x.mod[test,],s=lasso.best.lam)
    (lasso.err<-mean((lasso.pred-y.test)^2))
    ```
    e.
    ```{r include=TRUE, echo=TRUE}
    library(pls)
    set.seed(1)
    pcr.mod <- pcr(Apps ~ ., data=College, subset=train, scale=TRUE, validation="CV")
    validationplot(pcr.mod,val.type="MSEP")
    summary(pcr.mod)
    pcr.pred <- predict(pcr.mod,College[test,],ncomp=16)
    (pcr.err<-mean((pcr.pred-y.test)^2))
    ```
    f. For Partial Least Squares, a lot of the componenets have roughly the same CV after 4 components.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    pls.mod <- plsr(Apps ~ ., data=College, subset=train, validation="CV")
    validationplot(pls.mod, val.type="MSEP")
    summary(pls.mod)
    pls.pred <- predict(pls.mod,College[test,],ncomp=4)
    (pls.err<-mean((pls.pred-y.test)^2))
    ```
    g. The lasso and ridge performed better than the rest of the methods. We can see that the lasso did the best, whereas PLS did the worst. In addition, just a linear model outperformed both PCR and PLS.
    ```{r include=TRUE, echo=TRUE}
    rbind(ref.err,lm.err,ridge.err,lasso.err,pcr.err,pls.err)
    ```
  
10.
    a.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    X <- rnorm(20*1000)
    dim(X) <- c(1000,20)
    (B <- sample(c(-2,-1,-.5,0,.5,1,2),size=20,replace=TRUE,prob=c(1,1,1,10,1,1,1)))
    Y<-X%*%B+rnorm(20)
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    train <- sample.int(1000,size=100)
    test <- - train
    ```
    c.
    ```{r include=TRUE, echo=TRUE}
    library(leaps)
    full.fit <- regsubsets(Y~.,data=data.frame(Y,X)[train,],nvmax=20)
    X.mod <- model.matrix(Y ~ X)
    val.errors <- double(20)
    for(i in 1:20){
      coefi<-coef(full.fit,id=i)
      pred <- X.mod[train,names(coefi)]%*%coefi
      val.errors[i]=mean((Y[train]-pred)^2)
    }
    plot(val.errors,type='l')
    points(val.errors,pch=4)
    abline(v=which.min(val.errors),col="red",lty=2)
    ```
    d. The minimum occurs at 8 variables, which is true to the original $\beta$.
    ```{r include=TRUE, echo=TRUE}
    test.errs <- double(20)
    for(i in 1:20){
      coefi<-coef(full.fit,id=i)
      pred <- X.mod[test,names(coefi)]%*%coefi
      test.errs[i]=mean((Y[test]-pred)^2)
    }
    plot(test.errs,type='l')
    points(test.errs,pch=4)
    abline(v=which.min(test.errs),lty=2,col="red")
    ```
    f. The minimum value occurs at 8 variables, which is true to our model, `sum(B==0)=` `r sum(B==0)`. Comparing the coefficients:
    ```{r include=TRUE, echo=TRUE}
    names(B) <- colnames(X.mod)[2:21]
    True.B <- B[B!=0]
    True.B <- c(`(Intercept)`=0,True.B)
    Fitted <- coef(full.fit,id=8)
    rbind(True.B, Fitted, Err=(True.B-Fitted))
    ```
    g. 
    ```{r include=TRUE, echo=TRUE}
    norms <- double(20)
    B2 <- c(0,B)
    names(B2) <- colnames(X.mod)
    for(i in 1:20){
      coefi <- coef(full.fit,id=i)
      temp<-rep(0,21)
      names(temp)<-colnames(X.mod)
      temp[names(coefi)] <- coefi
      norms[i]=sqrt(sum((B2-temp)^2))
    }
    plot(norms,xlab="r",ylab="norm",type="l")
    ```
    We can see that it looks pretty similar to the test MSE plot. The minimum of the plot occurs at `r which.min(norms)`, which is the same as the index of the test errors, `r which.min(test.errs)`.
    
11.
```{r include=TRUE, echo=TRUE}
data(Boston,package="MASS")
head(Boston)
Boston.mat <- model.matrix(crim ~ ., Boston)
```
    a. First, split up the set into a test set and a training set.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    train<-sample.int(nrow(Boston),size=nrow(Boston)*.5)
    test<--train
    y.train <- Boston$crim[train]
    y.test <- Boston$crim[test]
    ```
    First linear models. I use best subsets to select the best train set using BIC, then get the MSE on the test set.
    ```{r include=TRUE, echo=TRUE}
    full.fit <- regsubsets(crim ~ ., data=Boston[train,],nvmax=ncol(Boston))
    lm.best.coef <- coef(full.fit,id=which.min(summary(full.fit)$bic))
    lm.pred <- Boston.mat[test,names(lm.best.coef)]%*%lm.best.coef
    (lm.err <- mean((lm.pred-y.test)^2))
    ```
    ```{r include=TRUE, echo=TRUE}
    full.lm.mod <- lm(crim ~ ., Boston, subset=train)
    lm.full.pred <- predict(full.lm.mod, Boston[test,])
    (lm.full.err <- mean((lm.full.pred - y.test)^2))
    ```
    Next, Ridge and Lasso using cross validation.
    ```{r include=TRUE, echo=TRUE}
    library(glmnet)
    runGlmNet <- function(alpha) {
      set.seed(927)
      cv <- cv.glmnet(Boston.mat[train,],y.train,alpha=alpha)
      best.lam <- cv$lambda.min
      mod <- glmnet(Boston.mat[train,],y.train,alpha=alpha)
      pred <- predict(mod,newx=Boston.mat[test,],s=best.lam)
      err<-mean((pred-y.test)^2)
      list(mod=mod,cv=cv,best.lam=best.lam,err=err)
    }
    #Ridge first
    ridge.mod<-runGlmNet(0)
    ridge.mod$err
    #Lasso
    lasso.mod<-runGlmNet(1)
    lasso.mod$err
    ```
    Last, PCR. 9 components looks pretty good.
    ```{r include=TRUE, echo=TRUE}
    library(pls)
    set.seed(927)
    pcr.mod <- pcr(crim~.,data=Boston,subset=train,scale=TRUE,validation="CV")
    summary(pcr.mod)
    validationplot(pcr.mod,val.type = "MSEP")
    pcr.pred <- predict(pcr.mod,Boston[test,],ncomp=9)
    (pcr.err <- mean((pcr.pred-y.test)^2))
    ```
    Now let's summarize the errors:
    ```{r include=TRUE, echo=TRUE}
    lasso.err <- lasso.mod$err
    ridge.err <- ridge.mod$err
    rbind(lm.err, lm.full.err, ridge.err, lasso.err, pcr.err)
    ```
    It appears Lasso outperforms the rest of the models, but ridge is close behind. PCR doesn't do too well. Our best subsets didn't improve on the full lm either using the validation set approach, but they are close that it might not be significant.
    b. I'll just propose the Lasso model
    ```{r include=TRUE, echo=TRUE}
    coef(lasso.mod$mod, s=lasso.mod$best.lam)
    ```
    c. We can see that Lasso only removed two coefficients. 