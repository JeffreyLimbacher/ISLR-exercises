---
output:
  pdf_document: default
  html_document: default
---

# Moving Beyond Linearity {#chap7}

1.
    a. Note that if we have $x \leq \xi$, then $(x- \xi)^3_+= 0$, so $f(x)$ reduces to
    \begin{align}
    f(x) &= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4(x-\xi)^3_+ \\
    &= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3\, .
    \end{align}
    This gives coefficients $a_1 = \beta_0$, $b_1 = \beta_1$, $c_1 = \beta_2$, $d_1 = \beta_3$.
    b. Remember that $(x-\xi)^3 = x^3 - 3x^2 \xi + 3x \xi^2 - \xi^3$. Then expanding $(x-\xi)^3_+$ within $f(x)$ gives
    \begin{align}
    f(x) &= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4(x-\xi)^3_+ \nonumber \\
    &= \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4(x^3 - 3x^2 \xi + 3x \xi^2 - \xi^3) \nonumber \\
    &= \beta_0 - \beta_4\xi^3 + (\beta_1+ 3 \beta_4 \xi^2)x + (\beta_2 - 3 \beta_4 \xi)x^2 + (\beta_3 + \beta_4) x^3 \, .
    (\#eq:expandedf)
    \end{align}
    Equating $f_2(x)$ with \@ref(eq:expandedf), we get that $a_2 = \beta_0 - \beta_4 \xi^3$, $b_2 = \beta_1 + 3 \beta_4 \xi^3$, $c_2 = \beta_2 - 3 \beta_4 \xi$, and $d_4 = \beta_3 + \beta_4$.
    c. Directly plugging into $f_1(x)$,
    \begin{equation}
    f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3\, .
    \end{equation}
    Plugging into $f_2(x)$,
    \begin{align}
    f_2(\xi) &= \beta_0 - \beta_4 \xi^3 + (\beta_1 + 3\beta_4 \xi^2)\xi +(\beta_2 - 3 \beta_4 \xi)\xi^2 + (\beta_3 + \beta_4) \xi^3 \\
    &= \beta_0 - \beta_4 \xi^3 + \beta_1\xi + 3\beta_4\xi^3 + \beta_2\xi^2 - 3\beta_4\xi^3 + \beta_3 + \beta_4 \xi^3 \\
    &= \beta_0 - \beta_1 \xi + \beta_2 \xi^2 + \beta_3\xi^3\, .
    \end{align}
    This gives us that $f_1(\xi) = f_2(\xi)$.
    d. First note that
    \begin{align}
    f_1'(x) &= \beta_1+2\beta_2x + 3\beta_3x^2  \, ,\\
    f_2'(x) &= \beta_1 + 3\beta_4\xi^2 + 2(\beta_2- 3\beta_4\xi)x + 3(\beta_3+\beta_4)x^2 \, .
    \end{align}
    Plugging in $\xi$ into $f_1'(x)$,
    \begin{equation}
    f_1'(\xi) = \beta_1+2\beta_2 \xi + 3\beta_3 \xi^2
    \end{equation}
    Plugging $\xi$ into $f_2'(x)$,
    \begin{align}
    f_2'(\xi) &= \beta_1 + 3\beta_4\xi^2 + 2(\beta_2- 3\beta_4\xi)\xi + 3(\beta_3+\beta_4)\xi^2\\
    &= \beta_1 + 3 \beta_4 \xi^2 + 2\beta_2 \xi - 6\beta_4 \xi^2 + 3\beta_3\xi^2 + 3\beta_4\xi^2\\
    &= \beta_1 + 2\beta_2 \xi + 3\beta_3 \xi^2
    \end{align}
    which is $f_1'(\xi)$.
    e. 
    We have
    \begin{align}
    f_1''(x) &= 2\beta_2 + 6\beta_3 x \, , \\
    f_2''(x) &= 2\beta_2 - 6\beta_4 \xi + (6\beta_3 + 6\beta_4) x \, .
    \end{align}
    So,
    \begin{equation}
    f_1''(\xi) = 2\beta_2 + 6\beta_3 \xi \, .
    \end{equation}
    And plugging $\xi$ into $f_2''(x)$, we have
    \begin{align}
    f_2''(\xi) &= 2\beta_2 - 6\beta_4 \xi + (6\beta_3 + 6\beta_4) \xi \\
    &= 2 \beta_2 - 6\beta_4 \xi + 6 \beta_3 \xi + 6 \beta_4 \xi \\
    &= 2\beta_2 + 6\beta_3 \xi \, .
    \end{align}
    This shows that $f_1''(\xi) = f_2''(\xi)$.
    
2. I'm not interested in sketching as much as describing the class of solutions and which solution is best within each one.
    a. If $m=0$, then we are saying then if $g(x)>0$ at any point (ignoring issues of measure), then $\lambda \int g(x)^2 dx \rightarrow \infty$ as $\lambda \rightarrow \infty$, so the only solution of $\hat{g}$ is $\hat{g}(x) = 0$.
    b. This must be a constant function since $f'(x) = 0$ for any constant function $f$. In this case, the solution that minimizes the error would be $\hat{g}(x) = \bar{y}$. 
    c. In this case, it is a linear function since $f''(x) = 0$ for any linear function. This reduces to the linear least squares solution.
    d. This would be a quadratic solution linear least squares fit.
    e. Since $\lambda=0$, we would probably be the same in the case of $\lambda=0$ and $m=2$, so we would get an interpolating spline.
    
3.
```{r include=TRUE, echo=TRUE}
x <- seq(-2,2,.01)
y <- 1 + 1*x + (x-1)^2*(x>=1)
plot(x,y,type="l")
```

4. Before we even plot, we can note that $b_2$ actually has no effect for $x \in [-2,2]$. 
```{r include=TRUE, echo=TRUE}
x <- seq(-2, 2, .01)
b1 <- function(x) (x >= 0 & x<=2)-(x-1)*(x>=1 & x<=2)
b2 <- function(x) (x-3)*(x>=3 & x<=4)+(x>4 & x<=5)
y <- 1 + b1(x) + 3*b2(x)
plot(x,y,type="l")
```

5.
    a. As $\lambda \rightarrow \infty$, $\hat{g}_4$ will have the smaller training RSS. That is because it is allowed to have more variability.
    b. As $\lambda \rightarrow \infty$, we cannot know if $\hat{g}_1$ or $\hat{g}_2$ will have lower test RSS. We do not know what the true response is, so it is possible it is a very high degree polynomial, in which case $\hat{g}_2$ will fit it better. However, if the true response is a line, then $\hat{g}_1$ will have a better test RSS.
    c. For $\lambda=0$, the two models degenerate into the same model, so they will have the same training and test RSS.
    
6.
```{r include=TRUE, echo=TRUE}
data(Wage,package="ISLR")
head(Wage)
```
    a.
    ```{r include=TRUE, echo=TRUE}
    library(boot)
    set.seed(1)
    cv.errors <- double(10)
    for(i in 1:10) {
      glm.fit <- glm(wage ~ poly(age,i), data=Wage)
      cv.errors[i] <- cv.glm(Wage, glm.fit, K=10)$delta[1]
    }
    plot(cv.errors,type="l",xlab="degree",ylab="error")
    abline(v=which.min(cv.errors),col="red",lty=2)
    ```
    A polynomial of degree 4 is the best fit to the data.
    ```{r include=TRUE, echo=TRUE}
    lm.mods<-list()
    for(i in 1:10) {
      lm.fit <- lm(wage ~ poly(age,i),data=Wage)
      lm.mods[[i]] <- lm.fit
    }
    do.call("anova",lm.mods)
    ```
    From the analysis of variance, we can see that the stops being significant at the 4th degree polynomial. This agrees with the cross-validaiton results.
    A plot of the polynomial fit is below.
    ```{r include=TRUE, echo=TRUE}
    plot(Wage$age,Wage$wage,xlab="age",ylab="wage")
    lines(predict(lm.mods[[i]],list(age=seq(from=min(Wage$age),to=max(Wage$age),length.out=100))),col="red")
    legend("topright",c("data","fit"),pch=c(1,NA),lty=c(NA,1),col=c("black","red"))
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    cv.errors <- double(10)
    for(i in 1:10) {
      n_ints <- i+2
      k_folds <- 10 
      folds <- cut(seq(1,nrow(Wage)),breaks=k_folds,labels=FALSE)
      errors <- double(k_folds)
      breaks <- quantile(Wage$age,p=seq(0,1,by=1/n_ints))
      Wage$age.cut <- cut(Wage$age,breaks=n_ints)
      for(k in 1:k_folds){
        train <- folds != k
        glm.fit <- lm(wage ~ age.cut, data=Wage, subset=train)
        y.pred <- predict(glm.fit,Wage[!train,])
        errors[k] <- mean((y.pred-Wage$wage[!train])^2)
      }
      cv.errors[i] <-mean(errors)
    }
    plot(3:12,cv.errors,type="l",xlab="number of intervals")
    abline(v=which.min(cv.errors)+2,col="red",lty=2)
    ```
    A plot of the model is below.
    ```{r include= TRUE, echo=TRUE}
    plot(Wage$age,Wage$wage,xlab="age",ylab="wage")
    lm.fit <- lm(wage ~ cut(age,breaks=8), data=Wage)
    age.grid <- seq(min(Wage$age),max(Wage$age))
    y.pred<-predict(lm.fit,list(age=age.grid))
    lines(age.grid,y.pred,col="red")
    legend("topright",c("data","fit"),pch=c(1,NA),lty=c(NA,1),col=c("black","red"))
    ```
    
8.
```{r include=TRUE, echo=TRUE}
data(Auto,package="ISLR")
head(Auto)
```
We will find the relationship between horsepower, weight, and mpgs.
```{r include=TRUE, echo=TRUE}
library(splines)
set.seed(1)
wgt.cv.errs <- double(20)
for(i in 1:20){
  folds <- cut(seq(1,nrow(Auto)),breaks=k_folds,labels=FALSE)
  errors <- double(10)
  for(k in 1:10) {
    train <- folds != k
    mod<-lm(mpg ~ ns(weight,df=i),data=Auto,subset=train)
    y.pred<-predict(mod,Auto[!train,])
    errors[k]<-mean((y.pred-Auto$mpg[!train])^2)
  }
  wgt.cv.errs[i] <- mean(errors)
}
plot(1:length(wgt.cv.errs),wgt.cv.errs,xlab="DF",ylab="Cross validation error",type="l")
abline(v=which.min(wgt.cv.errs),lty=2,col="red")
```
Although 8 is the minimum, 2 appears just as good in addition to being simpler.
```{r include=TRUE, echo=TRUE}
mod <- lm(mpg ~ ns(weight,df=2),data=Auto)
wgt.grid = seq(min(Auto$weight),max(Auto$weight),by=1)
mpg.pred<-predict(mod,list(weight=wgt.grid))
plot(Auto$weight,Auto$mpg,xlab="weight",ylab="mpg")
lines(wgt.grid,mpg.pred,col="red")
```
```{r include=TRUE, echo=TRUE}
library(splines)
set.seed(1)
fit <- smooth.spline(Auto$horsepower,Auto$mpg,cv=TRUE)
plot(fit,type="l",col="red")
points(Auto$horsepower,Auto$mpg)
```
Now try with a GAM, but use ANOVA to see if we are adding anything by using the smoothing spline with the natural spline.
```{r include=TRUE, echo=TRUE}
library(gam)
gam.m1 <- gam(mpg ~ ns(weight,2),data=Auto)
gam.m2 <- gam(mpg ~ ns(weight,2) + s(horsepower,fit$lambda), data=Auto)
anova(gam.m1,gam.m2)
```

9.
```{r include=TRUE, echo=TRUE}
data(Boston,package="MASS")
head(Boston)
```
    
    a. 
    ```{r include=TRUE, echo=TRUE}
    poly.fit <- lm(nox ~ poly(dis,degree=3), data=Boston)
    summary(poly.fit)
    ```
    Plotting the resulting regression:
    ```{r include=TRUE, echo=TRUE}
    dis.grid <- with(Boston, seq(min(dis), max(dis), by=(max(dis)-min(dis))/1000))
    y.pred <- predict(poly.fit,list(dis=dis.grid))
    with(Boston, plot(dis,nox))
    lines(dis.grid, y.pred, type="l", col="red")
    ```
    
    b. Note that we expect the RSS to drop for every higher degree of polynomial here because we will fit to the model better with least squares.
    ```{r include=TRUE, echo=TRUE}
    rss <- double(10)
    for(i in 1:10){
      poly.fit <- lm(nox ~ poly(dis,degree=i), data=Boston)
      poly.sum <- summary(poly.fit)
      rss[i] <- sum(poly.sum$residuals^2)
    }
    data.frame(degree=1:10, rss)
    ```
    
    c.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    cv.errs <- double(10)
    for(i in 1:10){
      k.folds <- 10
      folds <- rep(1:k_folds,length.out = nrow(Boston))
      errors <- double(10)
      for(k in 1:10) {
        train <- folds != k
        mod<-lm(nox ~ poly(dis,degree=i),data=Boston,subset=train)
        y.pred<-predict(mod,Boston[!train,])
        errors[k]<-mean((y.pred-Boston$nox[!train])^2)
      }
      cv.errs[i] <- mean(errors)
    }
    plot(1:10,cv.errs,xlab="degree",ylab="CV error",type="l")
    abline(v=which.min(cv.errs),col="red",lty=2)
    ```
    The cross-validation suggests that a 3rd degree polynomial gives the best results, but the 2nd degree could also be used for more simplicity.
    
    d. 
    ```{r include=TRUE, echo=TRUE}
    library(splines)
    bs.fit <- lm(nox ~ bs(dis,df=4), data=Boston)
    y.pred <- predict(bs.fit, list(dis=dis.grid))
    with(Boston,plot(dis,nox))
    lines(dis.grid, y.pred, type="l", col="red")
    
    ```
    I let `bs` choose the knot for me. `bs` chooses the median in this case.
    
    e. We expect it to be lower again since we are fitting a more flexible model to the data.
    ```{r include=TRUE, echo=TRUE}
    dfs <- c(3, 4, 5, 7, 10, 15, 25, 50)
    rss <- double(length(dfs))
    for(i in seq_along(dfs)){
      df <- dfs[i]
      bs.fit <- lm(nox ~ bs(dis,df=df), data=Boston)
      y.pred <- predict(bs.fit)
      rss[i] <- sum((y.pred-Boston$nox)^2)
    }
    data.frame(df=dfs,rss)
    ```
    
    f.
    ```{r include=TRUE, echo=TRUE}
    set.seed(927)
    dfs <- 3:50
    cv.errs <- double(length(dfs))
    for(i in seq_along(dfs)) {
      df <- dfs[i]
      k.folds <- 10
      folds <- rep(1:k_folds,length.out = nrow(Boston))
      errors <- double(10)
      for(k in 1:10) {
        train <- folds != k
        mod<-lm(nox ~ bs(dis,df=df),data=Boston,subset=train)
        y.pred<-predict(mod,Boston[!train,])
        errors[k]<-mean((y.pred-Boston$nox[!train])^2)
      }
      cv.errs[i] <- mean(errors)
    }
    plot(dfs,cv.errs,type="l")
    abline(v=dfs[which.min(cv.errs)],lty=2,col="red")
    ```
    The minimum occurs at `r dfs[which.min(cv.errs)]`. A plot of it is below.
    ```{r include=TRUE, echo=TRUE}
    best.bs.fit <- lm(nox ~ bs(dis,df=dfs[which.min(cv.errs)]), data=Boston)
    y.pred <- predict(best.bs.fit, list(dis=dis.grid))
    with(Boston,plot(dis,nox))
    lines(dis.grid,y.pred,col="red")
    ```
    That kind of looks like overfitting. It appears that a df of 5 also gives comparable results while retaining much more accuracy. 
    ```{r include=TRUE, echo=TRUE}
    bs.5.fit <- lm(nox ~ bs(dis,df=5), data=Boston)
    y.pred <- predict(bs.5.fit, list(dis=dis.grid))
    with(Boston,plot(dis,nox))
    lines(dis.grid,y.pred,col="red")
    ```

10. 
    a. 
    ```{r include=FALSE}
    data(College,package="ISLR")
    library(leaps)
    set.seed(927)
    train<-sample(nrow(College),nrow(College)*.5)
    leaps.subs<-regsubsets(Outstate ~ .,data=College,subset=train, nvmax=Inf)
    coef(leaps.subs,id=which.min(summary(leaps.subs)$bic))
    ```
    b. Let's visualize these relationships before choosing how we want to model these.
    ```{r echo=TRUE, fig.height=6}
    library(gam)
    sig.vars <- c("Private", "Accept", "Enroll", "Room.Board","Terminal", "perc.alumni", "Expend", "Grad.Rate")
    par(mfcol=c(3,3))
    for( v in sig.vars){
      plot(College[train,v],College[train,"Outstate"],xlab=v,ylab="Outstate")
    }
    ```
    Now build a gam. I really have no intuition for how to select which ones. I suppose there must be a lot of cross-validation, then comparing which cross-validation worked best for each technique.
    ```{r echo=TRUE, fig.height=6}
    gam.mod <- gam(Outstate ~ Private + s(Room.Board) + lo(Expend,span=.1) + lo(Accept,span=.1) + s(Terminal,df=10) + bs(Grad.Rate, df=4) + lo(Enroll,span=.1) + ns(perc.alumni,df=6), data=College, subset=train)
    par(mfcol=c(3,3))
    plot(gam.mod)
    ```
    c.
    ```{r echo=TRUE}
    gam.pred <- predict(gam.mod, College[-train,])
    lm.mod <- lm(as.formula(paste(c("Outstate ~ ", sig.vars),"+",TRUE)),data=College, subset=train)
    lm.pred <- predict(lm.mod, College[-train,])
    mean((gam.pred-College[-train,"Outstate"])^2)
    mean((lm.pred-College[-train,"Outstate"])^2)
    ```
    In either case, our model performs better than the linear model.
    d. Terminal, Grad.Rate, and Expend seem to be non-linear.

11.
    a.
    ```{r echo=TRUE}
    set.seed(927)
    x1 <- rnorm(100,20,5)
    x2 <- rnorm(100,10,5)
    y <- 3 + 2*x1 + x2 + rnorm(100)
    ```
    b.
    ```{r echo=TRUE}
    b1 <- -1
    ```
    c.
    ```{r echo=TRUE}
    a <- y-b1*x1
    b2 <- lm(a~x2)$coef[2]
    ```
    d.
    ```{r echo=TRUE}
    a <- y-b2*x2
    b1 <- lm(a~x1)$coef[2]
    ```
    e. It converges very quickly, so I only plot the first 10 or so.
    ```{r echo=TRUE}
    n<-10
    b0s <- double(n)
    b1s <- double(n)
    b2s <- double(n)
    b0s[1] <- mean(y - b1*x1 - b2*x2)
    b1s[1] <- b1
    b2s[1] <- b2
    for(i in 2:n){
      a <- y-b1*x1
      b2 <- lm(a~x2)$coef[2]
      a <- y-b2*x2
      b1 <- lm(a~x1)$coef[2]
      b0 <- mean(y - b1*x1 - b2*x2)
      b0s[i] <- b0
      b1s[i] <- b1
      b2s[i] <- b2
    }
    matplot(x=1:n,y=cbind(b0s,b1s,b2s),type="l",lty=c(2,2,2))
    legend(x="topright",c("beta_0","beta_1","beta_2"),lty=c(2,2,2),col=c(1,2,3))
    ```
    f.
    ```{r echo=TRUE}
    y.lm <- lm(y ~ x1 + x2)
    matplot(x=1:n,y=cbind(b0s,b1s,b2s),type="l",lty=c(2,2,2))
    legend(x="topright",c("beta_0","beta_1","beta_2"),lty=c(2,2,2),col=c(1,2,3))
    abline(h=y.lm$coef[1],lty=1,col="black")
    abline(h=y.lm$coef[2],lty=1,col="red")
    abline(h=y.lm$coef[3],lty=1,col="green")
    ```
12.
```{r echo=TRUE}
p <- 100
n<- 1000
iters <- 10
B <- sample(-5:5,p,replace=TRUE) #true coefficients
Bs <- matrix(nrow=iters,ncol=p) #Store the coefficient estimates here for each iteration
Bs[1,] <- rep(-100,p) #First guess is -100 for each coefficient
X <- rnorm(p*n)
dim(X) <- c(n,p)
y <- X%*%B + rnorm(n) #add noise
for (i in 1:iters-1){
  for(j in 1:p){
    a <- y-X[,-j]%*%Bs[i,-j] #remove all but the jth estimate
    Bs[i+1,j] <- lm(a ~ X[,j])$coef[2] #fit the jth dimension on the data and store the result
  }
}
```
    