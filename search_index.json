[
["index.html", "Jeffrey’s Answers to the ISLR Exercises Chapter 1 Introduction", " Jeffrey’s Answers to the ISLR Exercises Jeffrey Limbacher 2018-01-28 Chapter 1 Introduction I am writing this to write down my answers to “An Introduction to Statistical Learning: With Applications in R” by James, Witten, Hastie, and Tibshirani. I want to read through the book to learn the basic statistical methods that I see being used so often. Answering the questions allows me to feel like I am challenging myself to learn the material in the book as I read through it. I also want to get used to creating simple bookdowns. I will break up the solutions to the exercises by chapter. There is no guarantees that I will solve every problem or finish the book. I have a background in mathematics, so hopefully I should the background to breeze through this book and get the correct answers, but we are all hubmled from time to time. Please feel free to contact me with any questions. "],
["chap2.html", "Chapter 2 Statistical Learning", " Chapter 2 Statistical Learning In the case of large \\(n\\), small \\(p\\), it would be best to use a complex learning method. Since there are only a few \\(p\\) relative to the number of observations, the flexible method should be able to accurately capture the true \\(f\\) without the complexity of the flexible model becoming out of control. In the case of small \\(n\\), large \\(p\\), it would be worse to use a flexible model. Flexible models require a large number of observations or we start fitting to the noise too much. In addition, our problem may even be underdetermined. If the relationship between \\(X\\) and \\(Y\\) is highly non-linear, we would expect flexible methods to be better. Flexible methods can fit to arbitrary \\(f\\) much better. If \\(Var(\\epsilon)\\) is extremely high, a flexible model would be worse. This is because the flexible model will start fitting to the high variance whereas the rigid model will be more robust in ignoring the variance. Regression problem. CEO salary should be treated like a continuous response to the other factors. This is an inference problem since we are trying to understand how the factors interact with the response rather than simply predicting CEO salaries. \\(n=500\\) and \\(p=3\\) (profit, number of employees, and industry). Classification problem. The output is qualitative, success or failure. We are interested in prediction since we only care to guess whether our product will succeed. \\(n=20\\), \\(p=13\\) (price charged, marketing budget, competition price, and 10 others). As further justification that we are interested in prediction, the interplay between the variables and the outcome of the product will probably be highly non-linear resulting in using flexible models with uninterpretable results. Regression problem. The % change is quantitative. This is most likely a prediction problem since one imagines we are trying to beat the market, so the goal of the algorithm may be to find strange patterns to boost the prediction results (similar to PCA). \\(n=52\\) (weekly data), \\(p=3\\). Skipping this for now. Pass. Flexible models are able to capture a much wider range of \\(f\\)’s. They however, come at the cost of needing more training data in proportion to the observed response variables. In addition, their results may be hard to interpret. A more flexible approach may be appropriate when given a large \\(n\\) (see #2, a.), when the relationship between \\(X\\) and \\(Y\\) is not obvious. A less flexible approach is preferred if we have a small \\(n\\), interpretation is the goal, or when the possible shapes of \\(f\\) is limited and understood. A parametric approach has parameters that control the model in an fixed way. These may be coefficients of a line, parameters for parametrized distrubtions, or even statistics of central tendency. Non-parametric approaches general try to estimate the shape of \\(f\\) directly from the data, like using nearest neighbors to guess \\(Y\\). A parametric approach allows one to understand the interaction between the \\(X\\) and the \\(Y\\). For example, if we have the coefficient of some \\(X_i\\), say \\(\\beta_i\\), then we can figure out how changing \\(X_i\\) might change \\(Y\\). Parametric models also require less data. However, if we get the model assumptions wrong, then parametric approaches can give poor results. We’ll write some code to do this for us. d&lt;-data.frame(Obs=c(1,2,3,4,5,6), X_1=c(0,2,0,0,-1,1), X_2=c(3,0,1,1,0,1), X_3=c(0,0,3,2,1,1), Y=c(&quot;Red&quot;,&quot;Red&quot;,&quot;Red&quot;,&quot;Green&quot;,&quot;Green&quot;,&quot;Red&quot;) ) d$Dist=apply(cbind(d$X_1,d$X_2,d$X_3), 1, function(x) sqrt(sum(x^2))) d ## Obs X_1 X_2 X_3 Y Dist ## 1 1 0 3 0 Red 3.000000 ## 2 2 2 0 0 Red 2.000000 ## 3 3 0 1 3 Red 3.162278 ## 4 4 0 1 2 Green 2.236068 ## 5 5 -1 0 1 Green 1.414214 ## 6 6 1 1 1 Red 1.732051 Our neighbor closest to the origin is observation 5, so we would predict Green. The three closest neighbors are 5,6, and 2 which are Green, Red, Red respectively. We would predict Red since there are more Red observations than Green observations. We would expect it the best \\(K\\) to be small. The reason is that the higher the \\(K\\) is, the more far away points we bring in. Over longer distances, the Bayes decision boundary can change a lot. This may be cheating, but I would rather just read in with the data function. data(&quot;College&quot;,package=&quot;ISLR&quot;) head(College) ## Private Apps Accept Enroll Top10perc ## Abilene Christian University Yes 1660 1232 721 23 ## Adelphi University Yes 2186 1924 512 16 ## Adrian College Yes 1428 1097 336 22 ## Agnes Scott College Yes 417 349 137 60 ## Alaska Pacific University Yes 193 146 55 16 ## Albertson College Yes 587 479 158 38 ## Top25perc F.Undergrad P.Undergrad Outstate ## Abilene Christian University 52 2885 537 7440 ## Adelphi University 29 2683 1227 12280 ## Adrian College 50 1036 99 11250 ## Agnes Scott College 89 510 63 12960 ## Alaska Pacific University 44 249 869 7560 ## Albertson College 62 678 41 13500 ## Room.Board Books Personal PhD Terminal ## Abilene Christian University 3300 450 2200 70 78 ## Adelphi University 6450 750 1500 29 30 ## Adrian College 3750 400 1165 53 66 ## Agnes Scott College 5450 450 875 92 97 ## Alaska Pacific University 4120 800 1500 76 72 ## Albertson College 3335 500 675 67 73 ## S.F.Ratio perc.alumni Expend Grad.Rate ## Abilene Christian University 18.1 12 7041 60 ## Adelphi University 12.2 16 10527 56 ## Adrian College 12.9 30 8735 54 ## Agnes Scott College 7.7 37 19016 59 ## Alaska Pacific University 11.9 2 10922 15 ## Albertson College 9.4 11 9727 55 #rownames(College)=College[,1] #This is already done in the package data set. head(College) #note that this pops up a little window, so you may want to run this ## Private Apps Accept Enroll Top10perc ## Abilene Christian University Yes 1660 1232 721 23 ## Adelphi University Yes 2186 1924 512 16 ## Adrian College Yes 1428 1097 336 22 ## Agnes Scott College Yes 417 349 137 60 ## Alaska Pacific University Yes 193 146 55 16 ## Albertson College Yes 587 479 158 38 ## Top25perc F.Undergrad P.Undergrad Outstate ## Abilene Christian University 52 2885 537 7440 ## Adelphi University 29 2683 1227 12280 ## Adrian College 50 1036 99 11250 ## Agnes Scott College 89 510 63 12960 ## Alaska Pacific University 44 249 869 7560 ## Albertson College 62 678 41 13500 ## Room.Board Books Personal PhD Terminal ## Abilene Christian University 3300 450 2200 70 78 ## Adelphi University 6450 750 1500 29 30 ## Adrian College 3750 400 1165 53 66 ## Agnes Scott College 5450 450 875 92 97 ## Alaska Pacific University 4120 800 1500 76 72 ## Albertson College 3335 500 675 67 73 ## S.F.Ratio perc.alumni Expend Grad.Rate ## Abilene Christian University 18.1 12 7041 60 ## Adelphi University 12.2 16 10527 56 ## Adrian College 12.9 30 8735 54 ## Agnes Scott College 7.7 37 19016 59 ## Alaska Pacific University 11.9 2 10922 15 ## Albertson College 9.4 11 9727 55 #code chunk in your own session. summary(College) ## Private Apps Accept Enroll Top10perc ## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 ## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 ## Median : 1558 Median : 1110 Median : 434 Median :23.00 ## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 ## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 ## Top25perc F.Undergrad P.Undergrad Outstate ## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 ## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 ## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 ## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 ## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 ## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 ## Room.Board Books Personal PhD ## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 ## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 ## Median :4200 Median : 500.0 Median :1200 Median : 75.00 ## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 ## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 ## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 ## Terminal S.F.Ratio perc.alumni Expend ## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 ## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 ## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 ## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 ## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 ## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 ## Grad.Rate ## Min. : 10.00 ## 1st Qu.: 53.00 ## Median : 65.00 ## Mean : 65.46 ## 3rd Qu.: 78.00 ## Max. :118.00 pairs(College[,1:10]) Private colleges have higher out of state tuition. No surprises here. boxplot(Outstate ~ Private, data=College) College$Elite=as.factor(ifelse(College$Top10perc&gt;50,&quot;Yes&quot;,&quot;No&quot;)) summary(College$Elite) ## No Yes ## 699 78 plot(Outstate ~ Elite,data=College) I’m not really highlighting anything insightful here. It is interesting that book costs are so condensed. I’m guessing professors are sensitive to book prices and maybe just assign one so that cost is so well centered. I do feel for the few outliers that are out around $2,000 though. par(mfrow=c(2,2)) hist(College$Outstate,breaks=15) hist(College$Expend,breaks=20) hist(College$Personal,breaks=10) hist(College$Books,breaks=15) Students at non-private (does this necessarily mean public?) report spending more. boxplot(Personal ~ Private, data=College) data(Auto,package=&quot;ISLR&quot;) head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 The quantitative descriptors are mpg, cylinders, horsepower, weight, acceleration, and year. The qualitative are origin and name. I think cylinders might can be argued to be a qualitative variable, but there is definitel a sense of order to it. For example, we would expect for mpg’s to go up as cylinders goes down. Auto$origin&lt;-factor(Auto$origin) Auto_d&lt;-Auto[,sapply(Auto,is.double)] t(sapply(Auto_d,range)) ## [,1] [,2] ## mpg 9 46.6 ## cylinders 3 8.0 ## displacement 68 455.0 ## horsepower 46 230.0 ## weight 1613 5140.0 ## acceleration 8 24.8 ## year 70 82.0 I apply range to each column of Auto that is a double with the above code. For each row, the first number is the minimum value and the second number is highest value. Auto_d&lt;-Auto[,sapply(Auto,is.double)] cbind(Mean=sapply(Auto_d,mean),StdDev=sapply(Auto_d,sd)) ## Mean StdDev ## mpg 23.445918 7.805007 ## cylinders 5.471939 1.705783 ## displacement 194.411990 104.644004 ## horsepower 104.469388 38.491160 ## weight 2977.584184 849.402560 ## acceleration 15.541327 2.758864 ## year 75.979592 3.683737 Auto_d_rmobs&lt;-Auto_d[-c(10:85),] cbind(Mean=sapply(Auto_d_rmobs,mean),StdDev=sapply(Auto_d_rmobs,sd)) ## Mean StdDev ## mpg 24.404430 7.867283 ## cylinders 5.373418 1.654179 ## displacement 187.240506 99.678367 ## horsepower 100.721519 35.708853 ## weight 2935.971519 811.300208 ## acceleration 15.726899 2.693721 ## year 77.145570 3.106217 Displacement, horsepower, and weight show some very strong linear trends.We can also see that acceleration seems inversely proportions to the aforementioned three. pairs(Auto_d) "],
["chap3.html", "Chapter 3 Linear Regression", " Chapter 3 Linear Regression The first null hypothesis, related to the intercept term stats that we can conclude that sales will not be zero if we spend no money on TV, radio, or newspaper advertisting. The rest of the null hypotheses test whether TV, radio, or newspaper have any effect on sales. From the Intercept p-value, we can conclude that if we spend no money on advertising, we would expect to some sales. The second and third p-values state that if we spend money or TV and radio, we can expect some sort of return on investment in terms of sales. From the newspaper p-value, we cannot reject the null hypothesis that radio has no effect on sales. The KNN classifier attempts to fit the \\(Y\\) into some set of categories. For example, if we have the MPG, weight, and horsepower of a car, we might try to determine if it is a SUV, sedan, sports car, etc. KNN regression attempts to try and guess some quantitative values from the neighbors of a given point. For example, if we had the height and gender of someone, we can guess their weight by trying to find the \\(k\\) nearest neighbors in a height \\(\\times\\) gender and average the \\(k\\) weight values to come up with a best guess for the weight of the someone. Our equation would look like \\[ 50+20X_1 + 0.7X_2 + 35 X_3 + 0.01X_1 X_2 - 10 X_1 X_3.\\] To figure out when females earn more, we check \\[50+20X_1 + 0.7X_2 + 35 + 0.01X_1 X_2 - 10 X_1 &gt; 50+20X_1 + 0.7X_2 + 0.01X_1 X_2 \\] \\[ -10X_1 +35 &gt; 0\\] \\[ X_1 &lt; 3.5 \\] In otherwords, in order for women to have a higher salary, they need to have a GPA of less than 3.5, so iii. is the correct answer. We can use R to calculate this. x1 &lt;- 4.0; x2 &lt;- 110; x3 &lt;- 1; (50 + 20 * x1 + .7 * x2 + 35*x3 + 0.01*x1*x2 - 10 * x1*x3 ) ## [1] 206.4 Or about $206,000. False. If the standard error for \\(\\beta_4\\) is very small (\\(&lt;.0001\\) for example), then the t-statistics will be very large giving a large p-value. We cannot say anything about the evidence of an interaction without the p-value. We would expect the cubic regression to have a lower RSS. Note that if we set \\(\\beta_2=0\\) and \\(\\beta_3=0\\), then we get the linear regression model. Thus, if the linear regression model had a lower RSS, then we wouldn’t have gotten the least squares fit of cubic model. This is a contradiction, so we have that the cubic model must have a lower RSS. We would expect that the test RSS to be lower for the linear regression model. The cubic model would have fit to noise, and therefore would have higher variance. We would still expect the training RSS to be lower for the cubic polynomial. There is not enough information to tell. For example, maybe it is linear for the most part, but curves slightly at one end of the data set. Then we would expect the linear model to perform very well except for maybe some bias towards the end. The cubic regression would still overfit on the bulk of the data, but capture this tiny curve at the end well, but overall would be worse. On the other hand, we could have a truly cubic relationship between \\(X\\) and \\(Y\\). \\[\\begin{align*} \\hat{y}_i &amp;= x_i \\hat{\\beta} \\\\ &amp;= x_i \\left(\\sum_{j=1}^n x_{j}y_j \\right) / \\left(\\sum_{i&#39;=1}^n x_{i&#39;}^2 \\right) \\\\ &amp;= \\sum_{j=1}^n \\left( \\frac{x_i x_j }{\\sum_{i&#39;=1}^n x_{i&#39;}^2} \\right) y_j \\\\ &amp;= \\sum_{j=1}^n a_j y_j \\end{align*}\\] where \\(a_j = \\frac{x_i x_j }{\\sum_{i&#39;=1}^n x_{i&#39;}^2}\\). Our least squares line is \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x\\). If we plug in \\(\\bar{x}\\) for \\(x\\), we get \\(\\hat{y} = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\\). This means \\((\\bar{x},\\bar{y})\\) satisfies our equation, so the line passes through it. This will take a lot of math. Maybe another time. Lab Exercises. data(Auto,package=&quot;ISLR&quot;) head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 a. ```r mpg_mod &lt;- lm(mpg ~ horsepower, data=Auto) summary(mpg_mod) ``` ``` ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 ``` i. We see that the response has a p-value that is extremely small, so we reject the null hypothesis that `horsepower` does not influence `mpg`. ii. The $R^2$ is .6095 indicating that the relationship is moderate. The RSE is almost 5, so we could expect to be off by about 5 MPGs. iii. Negative. iv. ```r ans&lt;-sapply(c(&quot;confidence&quot;,&quot;prediction&quot;), function(x) predict(mpg_mod,data.frame(horsepower=98),interval=x)) rownames(ans)&lt;-colnames(predict(mpg_mod,data.frame(horsepower=98),interval=&quot;prediction&quot;)) ans ``` ``` ## confidence prediction ## fit 24.46708 24.46708 ## lwr 23.97308 14.80940 ## upr 24.96108 34.12476 ``` b. ```r plot(Auto$horsepower,Auto$mpg) abline(mpg_mod) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-20-1.png&quot; width=&quot;672&quot; /&gt; c. From the plots below, we can see that there is a clear non-linearity to the residual line versus the fitted values. It has a curve remniscient of the residuals from the `sales` example. In addition, this affects the normal QQ plot where we can see we have residuals that are too large and too small. ```r plot(mpg_mod) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-21-1.png&quot; width=&quot;672&quot; /&gt;&lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-21-2.png&quot; width=&quot;672&quot; /&gt;&lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-21-3.png&quot; width=&quot;672&quot; /&gt;&lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-21-4.png&quot; width=&quot;672&quot; /&gt; pairs(Auto) cor(Auto[,!(names(Auto) %in% c(&quot;name&quot;))]) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 ## acceleration year origin ## mpg 0.4233285 0.5805410 0.5652088 ## cylinders -0.5046834 -0.3456474 -0.5689316 ## displacement -0.5438005 -0.3698552 -0.6145351 ## horsepower -0.6891955 -0.4163615 -0.4551715 ## weight -0.4168392 -0.3091199 -0.5850054 ## acceleration 1.0000000 0.2903161 0.2127458 ## year 0.2903161 1.0000000 0.1815277 ## origin 0.2127458 0.1815277 1.0000000 I turn the origin into a factor variable since that it is what it is meant to be. Auto2 &lt;- Auto Auto2$origin &lt;- factor(Auto$origin) mpg_all&lt;-lm(mpg ~ .-name, data=Auto2) summary(mpg_all) ## ## Call: ## lm(formula = mpg ~ . - name, data = Auto2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0095 -2.0785 -0.0982 1.9856 13.3608 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.795e+01 4.677e+00 -3.839 0.000145 *** ## cylinders -4.897e-01 3.212e-01 -1.524 0.128215 ## displacement 2.398e-02 7.653e-03 3.133 0.001863 ** ## horsepower -1.818e-02 1.371e-02 -1.326 0.185488 ## weight -6.710e-03 6.551e-04 -10.243 &lt; 2e-16 *** ## acceleration 7.910e-02 9.822e-02 0.805 0.421101 ## year 7.770e-01 5.178e-02 15.005 &lt; 2e-16 *** ## origin2 2.630e+00 5.664e-01 4.643 4.72e-06 *** ## origin3 2.853e+00 5.527e-01 5.162 3.93e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.307 on 383 degrees of freedom ## Multiple R-squared: 0.8242, Adjusted R-squared: 0.8205 ## F-statistic: 224.5 on 8 and 383 DF, p-value: &lt; 2.2e-16 i. We can see the F-statistic is very large which indicates that there is a relationship between response and predictor. ii. It appears that the intercept, `displacement`, `weight`, `year`, and `origin` are all statistically significant at pretty low confidence levels. It is interesting to see that `horsepower` has a low p-value. This is because `horsepower` is highly colinear with `displacement` and `weight`. iii. The coefficient for the year variable is positive indicating that cars have gotten more gas efficient over the years. Looking at the diagnostic plots, we can see that mpg has a nonlinear trend after fitting the data. In addition, we see that some of the residuals have extremely large standardized residuals which seems to violate heteroscedascity. These large values appear towards the right side of the residuals vs fitted plot creating a funnel shape. In addition, we have a one (two?) high leverage points. plot(mpg_all) I (somewhat arbitrarily) chose to interact origin and year, and then horsepower with weight. mpg_int&lt;-lm(mpg ~ .-name + origin*year + horsepower*weight, data=Auto2) summary(mpg_int) ## ## Call: ## lm(formula = mpg ~ . - name + origin * year + horsepower * weight, ## data = Auto2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.5007 -1.7339 -0.0269 1.4737 12.0826 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.107e+01 5.271e+00 2.099 0.036443 * ## cylinders -8.323e-02 2.834e-01 -0.294 0.769149 ## displacement 6.615e-03 6.870e-03 0.963 0.336254 ## horsepower -2.207e-01 2.354e-02 -9.375 &lt; 2e-16 *** ## weight -1.108e-02 7.240e-04 -15.301 &lt; 2e-16 *** ## acceleration -9.228e-02 8.735e-02 -1.056 0.291453 ## year 6.661e-01 5.934e-02 11.226 &lt; 2e-16 *** ## origin2 -3.006e+01 9.122e+00 -3.295 0.001075 ** ## origin3 -1.326e+01 8.448e+00 -1.570 0.117250 ## year:origin2 4.158e-01 1.194e-01 3.483 0.000553 *** ## year:origin3 1.918e-01 1.087e-01 1.765 0.078382 . ## horsepower:weight 5.261e-05 5.207e-06 10.104 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.88 on 380 degrees of freedom ## Multiple R-squared: 0.8677, Adjusted R-squared: 0.8639 ## F-statistic: 226.6 on 11 and 380 DF, p-value: &lt; 2.2e-16 plot(mpg_int) It appears I may have gotten a little lucky with my choices! It appears that there is a strong interaction between year and origin2, which means European cars and year interact pretty well. In addition, the interaction with horsepower and weight ended up pretty significant. The model also managed to get diminished the non-linear effect on the residuals, but we still got the conical effect of the residuals growing vs the fitted values. I’m going to remove the high leverage point because it got a little ridiculous how much leverage it has. I compare the mpg against the inverse of horsepoewr and inverse of weight and its interaction. The result is that they come out statistically significant. mpg_int&lt;-lm(mpg ~ .-name + origin*year + I(1/horsepower)*I(1/weight), data=Auto2[-14,]) summary(mpg_int) ## ## Call: ## lm(formula = mpg ~ . - name + origin * year + I(1/horsepower) * ## I(1/weight), data = Auto2[-14, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0949 -1.4561 0.0835 1.3558 12.0946 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.063e+02 1.769e+01 -6.011 4.35e-09 *** ## cylinders 5.366e-02 3.116e-01 0.172 0.8634 ## displacement -9.979e-05 7.448e-03 -0.013 0.9893 ## horsepower 9.155e-02 2.311e-02 3.962 8.90e-05 *** ## weight 3.087e-03 1.478e-03 2.088 0.0374 * ## acceleration -2.058e-01 1.004e-01 -2.051 0.0410 * ## year 6.824e-01 5.872e-02 11.620 &lt; 2e-16 *** ## origin2 -2.224e+01 9.264e+00 -2.400 0.0169 * ## origin3 -1.523e+01 8.426e+00 -1.807 0.0715 . ## I(1/horsepower) 3.572e+03 7.440e+02 4.801 2.28e-06 *** ## I(1/weight) 1.278e+05 2.550e+04 5.012 8.29e-07 *** ## year:origin2 3.051e-01 1.215e-01 2.511 0.0124 * ## year:origin3 2.136e-01 1.083e-01 1.973 0.0492 * ## I(1/horsepower):I(1/weight) -5.505e+06 1.395e+06 -3.946 9.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.836 on 377 degrees of freedom ## Multiple R-squared: 0.8723, Adjusted R-squared: 0.8678 ## F-statistic: 198 on 13 and 377 DF, p-value: &lt; 2.2e-16 plot(mpg_int) data(Carseats,package=&quot;ISLR&quot;) head(Carseats) ## Sales CompPrice Income Advertising Population Price ShelveLoc Age ## 1 9.50 138 73 11 276 120 Bad 42 ## 2 11.22 111 48 16 260 83 Good 65 ## 3 10.06 113 35 10 269 80 Medium 59 ## 4 7.40 117 100 4 466 97 Medium 55 ## 5 4.15 141 64 3 340 128 Bad 38 ## 6 10.81 124 113 13 501 72 Bad 78 ## Education Urban US ## 1 17 Yes Yes ## 2 10 Yes Yes ## 3 12 Yes Yes ## 4 14 Yes Yes ## 5 13 Yes No ## 6 16 No Yes sales_mod &lt;- lm(Sales ~ Price + Urban + US, data=Carseats) summary(sales_mod) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 plot(sales_mod) The diagnostic plots look pretty good but the model isn’t very good. The price coefficient says that we can expect less units to sell at a higher prince. No surprise there. The urban coefficient would suggest that we sell slightly less in an urban location, but the p-value on this is extremely large, so we can safely dismiss this coefficient. The location being within the US is very important to sales. I’m not sure what the point of this question is \\[ \\mathrm{Sales} = 13.0 - 0.0544X_1 - 0.0219X_2 + 1.20 X_3\\] where \\(X_1\\) is the Price, \\(X_2\\) is 1 if we are in an Urban setting, and \\(X_3\\) is 1 if we are in the United States. We can reject the null-hypothesis safely for Price, USYes. sales_mod2 &lt;- lm(Sales ~ Price + US, data=Carseats) summary(sales_mod2) ## ## Call: ## lm(formula = Sales ~ Price + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.03079 0.63098 20.652 &lt; 2e-16 *** ## Price -0.05448 0.00523 -10.416 &lt; 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: &lt; 2.2e-16 We see that the two models have really low \\(R^2\\), so fit the data not very well. confint(sales_mod2) ## 2.5 % 97.5 % ## (Intercept) 11.79032020 14.27126531 ## Price -0.06475984 -0.04419543 ## USYes 0.69151957 1.70776632 From the book we don’t want our leveraget o be too much larger than \\((p+1)/n = (2+1)/400 = .0075\\). There is one observation that exceeds this by a lot, but its standarized residual isn’t very large. As far as I can tell, the most problematic observation is possibly 368. plot(sales_mod2,5) set.seed(1) x=rnorm(100) y=2*x+rnorm(100); a. ```r y_mod &lt;- lm(y ~ x + 0) summary(y_mod) ``` ``` ## ## Call: ## lm(formula = y ~ x + 0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9154 -0.6472 -0.1771 0.5056 2.3109 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.9939 0.1065 18.73 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9586 on 99 degrees of freedom ## Multiple R-squared: 0.7798, Adjusted R-squared: 0.7776 ## F-statistic: 350.7 on 1 and 99 DF, p-value: &lt; 2.2e-16 ``` ```r confint(y_mod) ``` ``` ## 2.5 % 97.5 % ## x 1.782603 2.205149 ``` We got the estimate $\\hat{\\beta} = 1.9939$. We can see that the 95% confidence interval contains our true slope of 2. b. ```r x_mod &lt;- lm(x ~ y + 0) summary(x_mod) ``` ``` ## ## Call: ## lm(formula = x ~ y + 0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8699 -0.2368 0.1030 0.2858 0.8938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.39111 0.02089 18.73 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4246 on 99 degrees of freedom ## Multiple R-squared: 0.7798, Adjusted R-squared: 0.7776 ## F-statistic: 350.7 on 1 and 99 DF, p-value: &lt; 2.2e-16 ``` ```r confint(x_mod) ``` ``` ## 2.5 % 97.5 % ## y 0.3496717 0.4325574 ``` We get a coefficient estimate of .391 with a standard error of .02. This is far from our expected value of .5. c. We can see that the t-statistic for both tests is the same. d. We start with $$ \\frac{\\frac{\\sum_{i=1} x_i y_i}{\\sum_{i&#39;=1}^n x_{i&#39;}^2}}{\\sqrt{\\frac{\\sum_{i=1}^n(y_i - x_i \\hat{\\beta})^2}{(n-1)\\sum_{i&#39;=1}^n x_{i&#39;}^2}}} = \\frac{(\\sqrt{n-1})\\sum_{i=1}^n x_iy_i}{\\sqrt{(\\sum_{i&#39;=1}x_{i&#39;}^2) \\sum_{i=1}^n(y_i - x_i \\hat{\\beta})^2}} $$ We already got what we wanted on top, so let&#39;s focus on the bottom (and drop the square root) $$ (\\sum_{i&#39;=1}x_{i&#39;}^2) \\sum_{i=1}^n(y_i - x_i \\hat{\\beta})^2 = (\\sum_{i&#39;=1}x_{i&#39;}^2) \\sum_{i=1}^n y_i^2 - 2 y_i x_i \\hat{\\beta} + x_i^2\\hat{\\beta}^2 $$ Substituting $\\hat{\\beta} = (\\sum_{i=1}^n x_i y_i) / (\\sum_{i&#39;=1}^n x_{i&#39;}^2)$ we get $$ (\\sum_{i&#39;=1}x_{i&#39;}^2) \\sum_{i=1}^n y_i^2 - 2 y_i x_i \\frac{(\\sum_{j=1}^n x_j y_j)}{(\\sum_{i&#39;=1}^n x_{i&#39;}^2)} + x_i^2\\frac{(\\sum_{j=1}^n x_j y_j)^2}{(\\sum_{i&#39;=1}^n x_{i&#39;}^2)^2} = (\\sum_{i&#39;=1}x_{i&#39;}^2)(\\sum_{i=1}^n y_i^2 ) - 2 (\\sum_{i=1}^n y_i x_i)(\\sum_{j=1}x_jy_j) + (\\sum_{i=1}x_i^2)\\frac{(\\sum_{j=1}^n x_j y_j)^2}{\\sum_{i&#39;=1}x_{i&#39;}^2} $$ Cancelling terms, we get $$(\\sum_{i&#39;=1}x_{i&#39;}^2)(\\sum_{i=1}^n y_i^2 ) - 2 (\\sum_{i=1}^n y_i x_i)(\\sum_{j=1}x_jy_j) + (\\sum_{i=1}x_i^2)\\frac{(\\sum_{j=1}^n x_j y_j)^2}{\\sum_{i&#39;=1}x_{i&#39;}^2} = (\\sum_{i&#39;=1}x_{i&#39;}^2)(\\sum_{i=1}^n y_i^2 ) - 2 (\\sum_{i=1}^n y_i x_i)^2 + (\\sum_{j=1}^n x_j y_j)^2 = (\\sum_{i&#39;=1}x_{i&#39;}^2)(\\sum_{i=1}^n y_i^2 ) - (\\sum_{i=1}^n y_i x_i)^2 $$ This gives us the identity we wanted. To confirm it numerically, ```r n&lt;-100; xy&lt;-sum(x*y); x2&lt;-sum(x^2); y2&lt;-sum(y^2) beta&lt;-xy/x2 sebeta&lt;-sqrt(sum((y-x*beta)^2)/((n-1)*x2)) t1&lt;-beta/sebeta t2&lt;-sqrt(n-1)*xy/sqrt(x2*y2 - xy^2) print(c(t1,t2)) ``` ``` ## [1] 18.72593 18.72593 ``` e. We can see that the second form the t-statistic is symmetric with respect to $x$ and $y$, so there is no algebraic difference between the -statistic for `y` onto `x` and `x` onto `y`. f. ```r y_int &lt;- lm(y ~ x) x_int &lt;- lm(x ~ y) cbind(`y t-statistic`=coef(summary(y_int))[&quot;x&quot;,&quot;t value&quot;],`x t-statistic`=coef(summary(x_int))[&quot;y&quot;,&quot;t value&quot;]) ``` ``` ## y t-statistic x t-statistic ## [1,] 18.5556 18.5556 ``` Let \\(\\hat{\\beta}_y\\) be the estimate for \\(\\beta\\) of y onto x and \\(\\hat{\\beta}_x\\) be the estimate for \\(\\beta\\) for x onto y. Then \\[\\hat{\\beta}_y = \\hat{\\beta}_x \\implies \\frac{\\sum_{i=1}^n (x_iy_i)}{\\sum_{i&#39;=1} x_{i&#39;}^2} = \\frac{\\sum_{i=1}^n (x_iy_i)}{\\sum_{i&#39;=1} y_{i&#39;}^2} \\implies \\sum_{i&#39;=1} x_{i&#39;}^2 = \\sum_{i&#39;=1} y_{i&#39;}^2\\] The example from 11. has two different \\(\\hat{\\beta}\\)’s. The trick I do below is to create two samples. I then rescale the random y so that the sum of squares is the same as the x variable. This makes the two regression values the same. set.seed(2) n &lt;- 100 x &lt;- rnorm(n) y &lt;- rnorm(n) y &lt;- y/sqrt(sum(y^2))*sqrt(sum(x^2)) #Rescaling so sum of squares is same for x and y coef(summary(lm(y~x+0))) ## Estimate Std. Error t value Pr(&gt;|t|) ## x -0.05620888 0.1003449 -0.5601569 0.5766378 coef(summary(lm(x~y+0))) ## Estimate Std. Error t value Pr(&gt;|t|) ## y -0.05620888 0.1003449 -0.5601569 0.5766378 set.seed(1) n &lt;- 100 x &lt;- rnorm(n) head(x) ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 eps &lt;- rnorm(n, sd=0.25); The length of y is \\(n=100\\). y &lt;- -1+.5*x+eps plot(x,y);abline(-1,.5) The coefficient estimates are \\(\\hat{\\beta}_0 = -1.00942\\), \\(\\hat{\\beta}_1 = 0.49973\\). These are pretty close. The true values are within 1 standard error. y_mod &lt;- lm(y~x) summary(y_mod) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46921 -0.15344 -0.03487 0.13485 0.58654 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.00942 0.02425 -41.63 &lt;2e-16 *** ## x 0.49973 0.02693 18.56 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2407 on 98 degrees of freedom ## Multiple R-squared: 0.7784, Adjusted R-squared: 0.7762 ## F-statistic: 344.3 on 1 and 98 DF, p-value: &lt; 2.2e-16 The linear least squares line is in red. It almost perfectly overlaps the true \\(f\\). plot(x,y);abline(-1,.5);abline(coef(y_mod),col=&quot;red&quot;) legend(x=&quot;bottomright&quot;, c(&quot;True Line&quot;,&quot;Least Squares Line&quot;), col=c(&quot;black&quot;,&quot;red&quot;), lty=c(1,1)) We can see evidence that the fit is better due to the \\(R^2\\) being higher. The \\(R^2\\) increased from 0.7784 to 0.7828. y_quad &lt;- lm(y ~ x+I(x^2)) summary(y_quad) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4913 -0.1563 -0.0322 0.1451 0.5675 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98582 0.02941 -33.516 &lt;2e-16 *** ## x 0.50429 0.02700 18.680 &lt;2e-16 *** ## I(x^2) -0.02973 0.02119 -1.403 0.164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2395 on 97 degrees of freedom ## Multiple R-squared: 0.7828, Adjusted R-squared: 0.7784 ## F-statistic: 174.8 on 2 and 97 DF, p-value: &lt; 2.2e-16 By reducing the standard deviation 5 times, we can see the \\(R^2\\) went up dramatically in the model.The t-values of the coefficients also went way up, so we are much more confident in our guesses for the coefficients. set.seed(1) x &lt;- rnorm(n,sd=.1) eps &lt;- rnorm(n,sd=.1) y &lt;- -1+.5*x+eps y_less &lt;- lm(y ~ x) summary(y_less) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.18768 -0.06138 -0.01395 0.05394 0.23462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.003769 0.009699 -103.495 &lt; 2e-16 *** ## x 0.498940 0.107727 4.632 1.12e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.09628 on 98 degrees of freedom ## Multiple R-squared: 0.1796, Adjusted R-squared: 0.1712 ## F-statistic: 21.45 on 1 and 98 DF, p-value: 1.117e-05 plot(x,y);abline(-1,.5);abline(coef(y_less),col=&quot;red&quot;) legend(x=&quot;bottomright&quot;, c(&quot;True Line&quot;,&quot;Least Squares Line&quot;), col=c(&quot;black&quot;,&quot;red&quot;), lty=c(1,1)) Now for the qudratic model. y_less_q &lt;- lm(y ~ x + I(x^2)) summary(y_less_q) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19650 -0.06254 -0.01288 0.05803 0.22700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.99433 0.01177 -84.512 &lt; 2e-16 *** ## x 0.51716 0.10798 4.789 6.01e-06 *** ## I(x^2) -1.18921 0.84766 -1.403 0.164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0958 on 97 degrees of freedom ## Multiple R-squared: 0.1959, Adjusted R-squared: 0.1793 ## F-statistic: 11.82 on 2 and 97 DF, p-value: 2.557e-05 We see that the \\(R^2\\) is basically the same as the linear model, suggesting that there was no improvement. In fact, the adjusted \\(R^2\\) is lower by .0001, so this model is worse by that standard. I set \\(\\sigma\\) to 1. set.seed(1) x &lt;- rnorm(n) eps &lt;- rnorm(n,sd=1) y2 &lt;- -1+.5*x+eps y_more &lt;- lm(y2 ~ x) summary(y_more) ## ## Call: ## lm(formula = y2 ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8768 -0.6138 -0.1395 0.5394 2.3462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.03769 0.09699 -10.699 &lt; 2e-16 *** ## x 0.49894 0.10773 4.632 1.12e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9628 on 98 degrees of freedom ## Multiple R-squared: 0.1796, Adjusted R-squared: 0.1712 ## F-statistic: 21.45 on 1 and 98 DF, p-value: 1.117e-05 plot(x,y);abline(-1,.5);abline(coef(y_more),col=&quot;red&quot;) legend(x=&quot;bottomright&quot;, c(&quot;True Line&quot;,&quot;Least Squares Line&quot;), col=c(&quot;black&quot;,&quot;red&quot;), lty=c(1,1)) The estimates for the coefficients are still good, but now the \\(R^2\\) is pretty pitiful. Let’s try the quadratic fit. y_more_q &lt;- lm(y2 ~ x + I(x^2)) summary(y_less_q) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.19650 -0.06254 -0.01288 0.05803 0.22700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.99433 0.01177 -84.512 &lt; 2e-16 *** ## x 0.51716 0.10798 4.789 6.01e-06 *** ## I(x^2) -1.18921 0.84766 -1.403 0.164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0958 on 97 degrees of freedom ## Multiple R-squared: 0.1959, Adjusted R-squared: 0.1793 ## F-statistic: 11.82 on 2 and 97 DF, p-value: 2.557e-05 Our \\(R^2\\) went up a little, but more interestingly, the coefficient for the x^2 term became statistically significant. However, the estimate for it is pretty tiny, it would be impercetible. set.seed(1) x1=runif(100) x2=0.5*x1+rnorm(100)/10 y=2+2*x1+0.3*x2+rnorm(100) The linear model is \\(Y=2+2X_1+0.3X_2 + \\epsilon\\). The regression coefficients are \\(\\beta_0=2\\), \\(\\beta_1 =2\\), \\(\\beta_2=0.3\\). The two variables are highly correlated. we can also see that they have a linear relationship. cor(x1,x2) ## [1] 0.8351212 plot(x1,x2) y_mod &lt;- lm(y ~ x1 + x2) summary(y_mod) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8311 -0.7273 -0.0537 0.6338 2.3359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1305 0.2319 9.188 7.61e-15 *** ## x1 1.4396 0.7212 1.996 0.0487 * ## x2 1.0097 1.1337 0.891 0.3754 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 97 degrees of freedom ## Multiple R-squared: 0.2088, Adjusted R-squared: 0.1925 ## F-statistic: 12.8 on 2 and 97 DF, p-value: 1.164e-05 From the summary, \\(\\hat{\\beta}_0=2.1305\\), \\(\\hat{\\beta}_1=1.4296\\), \\(\\hat{\\beta}_2=0.9097\\). We can see that the estimates for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are not terribly close to the true estimate. We can reject the null hypothesis that \\(H_0:\\beta_1=0\\) at \\(\\alpha=0.05\\), but we cannot reject \\(H_0 : \\beta_2 =0\\). y_mod &lt;- lm(y ~ x1) summary(y_mod) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89495 -0.66874 -0.07785 0.59221 2.45560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1124 0.2307 9.155 8.27e-15 *** ## x1 1.9759 0.3963 4.986 2.66e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.055 on 98 degrees of freedom ## Multiple R-squared: 0.2024, Adjusted R-squared: 0.1942 ## F-statistic: 24.86 on 1 and 98 DF, p-value: 2.661e-06 The evidence against the null has become much stronger in this case, and we can safely reject \\(H_0 : \\beta_1 = 0\\) at any reasonable confidence level. The estimate for \\(\\beta_1\\) is much closer to the true value as well. y_mod &lt;- lm(y ~ x2) summary(y_mod) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.62687 -0.75156 -0.03598 0.72383 2.44890 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3899 0.1949 12.26 &lt; 2e-16 *** ## x2 2.8996 0.6330 4.58 1.37e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.072 on 98 degrees of freedom ## Multiple R-squared: 0.1763, Adjusted R-squared: 0.1679 ## F-statistic: 20.98 on 1 and 98 DF, p-value: 1.366e-05 The results here suggest that we can reject \\(H_0: \\beta_2 = 0\\), but it appears that the estimate for the coefficient is off by quite a bit. Due to colinearity, the results do contradict each other. We said that x2 was not significant. Now we say it is highly significant. x1=c(x1,0.1) x2=c(x2,0.8) y=c(y,6) y_mod_12 &lt;- lm(y ~ x1 + x2) summary(y_mod_12) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.73348 -0.69318 -0.05263 0.66385 2.30619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2267 0.2314 9.624 7.91e-16 *** ## x1 0.5394 0.5922 0.911 0.36458 ## x2 2.5146 0.8977 2.801 0.00614 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.075 on 98 degrees of freedom ## Multiple R-squared: 0.2188, Adjusted R-squared: 0.2029 ## F-statistic: 13.72 on 2 and 98 DF, p-value: 5.564e-06 For the multiple linear regression, we cam see x2 has become significant while x1 has become non-significant. y_mod_1 &lt;- lm(y ~ x1) summary(y_mod_1) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8897 -0.6556 -0.0909 0.5682 3.5665 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2569 0.2390 9.445 1.78e-15 *** ## x1 1.7657 0.4124 4.282 4.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 99 degrees of freedom ## Multiple R-squared: 0.1562, Adjusted R-squared: 0.1477 ## F-statistic: 18.33 on 1 and 99 DF, p-value: 4.295e-05 For the linear regression of y onto x1, we can see that the estimate is now worse than before, and the standard error of \\(\\hat{\\beta}_1\\) has increased. The \\(R^2\\) dropped from .2024 to .1562, which is a serious drop for one extra observation. y_mod_2 &lt;- lm(y ~ x2) summary(y_mod_2) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.64729 -0.71021 -0.06899 0.72699 2.38074 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3451 0.1912 12.264 &lt; 2e-16 *** ## x2 3.1190 0.6040 5.164 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.074 on 99 degrees of freedom ## Multiple R-squared: 0.2122, Adjusted R-squared: 0.2042 ## F-statistic: 26.66 on 1 and 99 DF, p-value: 1.253e-06 As for x2, now our estimate is even more off, but the \\(R^2\\) got even higher. There are some interesting trade offs going between the x1 and x2 models. It seems like the one observation reversed the roles of the variables. To see why, let’s investigate where our observation went versus the bulk of the data using a plot of x2 vs x1. plot(x1,x2) We can see from the plot that this observation is a serious outlier. It does not seem that far off from the rest of the data, so it is likely not a high leverage point. data(Boston,package=&quot;MASS&quot;) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 a. A good opportunity to automate things. ```r preds&lt;-names(Boston)[-1] mods&lt;-list() stats&lt;-list() for (i in seq(along=preds)){ p&lt;-preds[i] mods[[i]] &lt;- lm(as.formula(paste0(&quot;crim ~ &quot;,p)),data=Boston) } stats&lt;-t(sapply(mods, function(x) coef(summary(x))[2,])) r_squared&lt;-sapply(mods,function(x) summary(x)$r.squared) stats&lt;-cbind(stats,r_squared) rownames(stats) &lt;-preds (stats2&lt;-stats[order(abs(stats[,&quot;t value&quot;]),decreasing=TRUE),]) ``` ``` ## Estimate Std. Error t value Pr(&gt;|t|) r_squared ## rad 0.61791093 0.034331820 17.998199 2.693844e-56 0.391256687 ## tax 0.02974225 0.001847415 16.099388 2.357127e-47 0.339614243 ## lstat 0.54880478 0.047760971 11.490654 2.654277e-27 0.207590933 ## nox 31.24853120 2.999190381 10.418989 3.751739e-23 0.177217182 ## indus 0.50977633 0.051024332 9.990848 1.450349e-21 0.165310070 ## medv -0.36315992 0.038390175 -9.459710 1.173987e-19 0.150780469 ## black -0.03627964 0.003873154 -9.366951 2.487274e-19 0.148274239 ## dis -1.55090168 0.168330031 -9.213458 8.519949e-19 0.144149375 ## age 0.10778623 0.012736436 8.462825 2.854869e-16 0.124421452 ## ptratio 1.15198279 0.169373609 6.801430 2.942922e-11 0.084068439 ## rm -2.68405122 0.532041083 -5.044819 6.346703e-07 0.048069117 ## zn -0.07393498 0.016094596 -4.593776 5.506472e-06 0.040187908 ## chas -1.89277655 1.506115484 -1.256727 2.094345e-01 0.003123869 ``` As we can see, a lot of these variables come out statistically significant. The only one we might not reject the null hypothesis is `chas`. I&#39;ll plot the first four. ```r best_vars&lt;-rownames(stats)[1:4] par(mfcol=c(2,2)) for (best_var in best_vars) { plot(Boston[,best_var],Boston[,&quot;crim&quot;],ylab=&quot;crim&quot;,xlab=best_var) } ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-61-1.png&quot; width=&quot;672&quot; /&gt; Wow. These plots looks like trash. It appears that even though we have strong evidence against the null hypothesis, the relationships are not very good. b. ```r multi_mod&lt;-lm(crim ~ ., data=Boston) summary(multi_mod) ``` ``` ## ## Call: ## lm(formula = crim ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.924 -2.120 -0.353 1.019 75.051 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.033228 7.234903 2.354 0.018949 * ## zn 0.044855 0.018734 2.394 0.017025 * ## indus -0.063855 0.083407 -0.766 0.444294 ## chas -0.749134 1.180147 -0.635 0.525867 ## nox -10.313535 5.275536 -1.955 0.051152 . ## rm 0.430131 0.612830 0.702 0.483089 ## age 0.001452 0.017925 0.081 0.935488 ## dis -0.987176 0.281817 -3.503 0.000502 *** ## rad 0.588209 0.088049 6.680 6.46e-11 *** ## tax -0.003780 0.005156 -0.733 0.463793 ## ptratio -0.271081 0.186450 -1.454 0.146611 ## black -0.007538 0.003673 -2.052 0.040702 * ## lstat 0.126211 0.075725 1.667 0.096208 . ## medv -0.198887 0.060516 -3.287 0.001087 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.439 on 492 degrees of freedom ## Multiple R-squared: 0.454, Adjusted R-squared: 0.4396 ## F-statistic: 31.47 on 13 and 492 DF, p-value: &lt; 2.2e-16 ``` First things first, we see that the F-statistic is very large. I choose $\\alpha=.005$. Under this, we can reject the null hypothesis on `dis`, `rad`, and `medv`. c. We can see that every variable would reject the null hypothesis in part a. However, in part b, we only reject the null hypothesis for 3 variables. d. The automation pays off here. ```r plot(stats[,1],coef(multi_mod)[-1]) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-63-1.png&quot; width=&quot;672&quot; /&gt; It&#39;s hard to see what&#39;s happening in the plot, so let&#39;s just look at the coefficients next to each other. ```r cbind(stats[,1],coef(multi_mod)[-1]) ``` ``` ## [,1] [,2] ## zn -0.07393498 0.044855215 ## indus 0.50977633 -0.063854824 ## chas -1.89277655 -0.749133611 ## nox 31.24853120 -10.313534912 ## rm -2.68405122 0.430130506 ## age 0.10778623 0.001451643 ## dis -1.55090168 -0.987175726 ## rad 0.61791093 0.588208591 ## tax 0.02974225 -0.003780016 ## ptratio 1.15198279 -0.271080558 ## black -0.03627964 -0.007537505 ## lstat 0.54880478 0.126211376 ## medv -0.36315992 -0.198886821 ``` We can see from the table that the simple linear models did indeed give much larger coefficients than the multiple linear regression did. For some of them, we can even see that the multiple linear regression gave a coefficient that was opposte in sign, such as `ptratio`. d. "],
["chap4.html", "Chapter 4 Classification", " Chapter 4 Classification Note that \\[\\begin{align*} 1 - p(X) &amp;= 1 - \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} \\\\ &amp;= \\frac{1 + e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} - \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}} \\\\ &amp;= \\frac{1}{1 + e^{\\beta_0+\\beta_1X}}\\, . \\end{align*}\\] Then \\[\\begin{align*} \\frac{p(X)}{1-p(X)} &amp;= \\frac{\\frac{e^{\\beta_0+\\beta_1X}}{1 + e^{\\beta_0+\\beta_1X}}}{\\frac{1}{1+e^{\\beta_0+\\beta_1X}}} \\\\ &amp;= e^{\\beta_0+\\beta_1X}\\, . \\end{align*}\\] Since \\(p_k(x)\\) has the same denominator for all \\(k\\), so we can ignore it, so we are left with \\[p_k^*(x) = \\pi_k \\frac{1}{\\sqrt{2 \\pi }\\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, .\\] In addition, the \\(\\frac{1}{\\sqrt{2 \\pi}\\sigma}\\) term is the same, so we just need to maximize \\[p_k^{**}(x) = \\pi_k \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, . \\] Logarithms preserve order, i.e. if \\(p^{**}_i(x) &lt; p^{**}_j(x)\\), then \\(\\log(p^{**}_i(x)) &lt; \\log(p^{**}_j(x))\\). Taking a log of \\(p^{**}_k(x)\\) gives \\[\\begin{align*} \\log(p^{**}_k(x)) &amp;= \\log \\left( \\pi_k \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right) \\right) \\\\ &amp;= \\log(\\pi_k) -\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\\\ &amp;= \\log(\\pi_k) -\\frac{1}{2 \\sigma^2}(x^2 - 2x \\mu_k + \\mu_k^2) \\\\ &amp;= \\log(\\pi_k) - \\frac{x^2}{2 \\sigma^2} + \\frac{x \\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} \\, . \\end{align*}\\] Since \\(\\sigma^2\\) is constant, we have that the \\(\\frac{x^2}{2 \\sigma^2}\\) term is the same for all \\(\\log(\\pi^{**}(x))\\) terms, so we can ignore it when maximizing. Removing this term from the last equation gives \\[ \\delta_k(x) = \\log(\\pi_k) + \\frac{x \\mu_k}{\\sigma^2} -\\frac{\\mu_k^2}{2 \\sigma^2} \\, .\\] From this we can conclude that maximizing our original equation is equivalent to maximizing the last equation. We can follow the arguments as the last problem, but we will end up with \\[\\begin{equation} p_k^*(x) = \\pi_k \\frac{1}{\\sqrt{2 \\pi }\\sigma_k} \\exp \\left(-\\frac{1}{2 \\sigma^2}(x - \\mu_k)^2 \\right)\\, . \\tag{4.1} \\end{equation}\\] Note that we can’t drop the \\(\\sigma_k\\) as in problem 2 since it is unique for each population. Taking the log of the (4.1) we get \\[\\begin{align} \\log(p_k^*(x)) &amp;= \\log(\\pi_k) - \\log(\\sqrt(2 \\pi) \\sigma_k) + \\frac{1}{2 \\sigma_k^2} (x - \\mu_k)^2 \\nonumber \\\\ &amp;= \\log(\\pi_k) - \\log(\\sqrt(2 \\pi) \\sigma_k) + \\frac{1}{2 \\sigma_k^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\nonumber \\\\ \\end{align}\\] Note that we cannot drop the \\(1/(2 \\sigma_k^2)\\) term since it varies between each group. Therefore, the above equation is quadratic. The question is essentially asking us to compute \\(P(|X_2-X_1|&lt;.05)\\) where \\(X_2\\) is our current observation and \\(X_1\\) is our data. However, there is some issues along the edges of the interval that make it annoying to compute. I am going to ignore these since it won’t really add much more value even though the answer is more accurate. Given some \\(x\\), the probability that our observation falls within \\(\\pm 0.05\\) of \\(x\\) is \\(P(x-0.05 \\leq X \\leq x+0.05) = .1\\). Same as before, given \\((x_1,x_2) \\in \\mathbb{R}^2\\), \\(P(x_1 - 0.05 \\leq X_1 \\leq x_1 + 0.05, x_2 - 0.05 \\leq X_2 \\leq x_2 + 0.05)\\) \\(=P(x_1 - 0.05 \\leq X_1 \\leq x_1)P(x_2 - 0.05 \\leq X_2 \\leq x_2 + 0.05)\\) \\(=P(x_1 - 0.05 \\leq X_1 \\leq x_1)^2 = 0.1^2 = 0.01\\). I used independence to separate the probability into two probabilities, and then use i.i.d. assumption to conclude that the probabilities were equal. It’s clear what the pattern is at this point, \\(P(x_1 - 0.05 \\leq X_1 \\leq x_1 + 0.05, \\dots, x_{100} - 0.05 \\leq X_{100} \\leq x_{100} + 0.05) = P(x_1 - 0.05 \\leq X_1 \\leq x_1 + 0.05) ^ 100 = 0.1^{100}\\). That number is miniscule. We can expect to find a very small amount of training data “near” our data set when \\(p\\) is large. This means that we are using observations that are completely unlike our prediction value to try and guess the characteristics of \\(Y\\). If \\(l\\) is the length of hypercube (\\(l \\leq 1\\)), then \\(P(x_1 - \\frac{l}{2} \\leq X_1 \\leq x_1 + \\frac{l}{2}, \\dots, x_p - \\frac{l}{2} \\leq X_p \\leq x_p + \\frac{l}{2}) = l^p\\). Thus, if we want 10%, we need to find \\(l\\) such that \\(\\sqrt[p]{.1} = l\\). I use R to calculate the lengths for some values of \\(p\\). p=c(1,2,3,10,25,50,100) (cbind(p,.1^(1/p))) ## p ## [1,] 1 0.1000000 ## [2,] 2 0.3162278 ## [3,] 3 0.4641589 ## [4,] 10 0.7943282 ## [5,] 25 0.9120108 ## [6,] 50 0.9549926 ## [7,] 100 0.9772372 Initially, I expected the QDA to outperform the LDA on every training set (similar to least squares), but below is an example where the QDA performs worse on the training set. require(MASS) ## Loading required package: MASS set.seed(1) n &lt;- 1000 n1 &lt;- 500 n2 &lt;- n-n1 x &lt;- c(rnorm(n1,-.25,1),rnorm(n2,.25,1)) y &lt;- factor(rep(c(0,1),times=c(n1,n2))) lda_wrong &lt;- sum(predict(lda(y~x))$class!=y) qda_wrong &lt;- sum(predict(qda(y~x))$class!=y) cbind(LDA=lda_wrong,QDA=qda_wrong) ## LDA QDA ## [1,] 414 419 It’s a little hard to say which is expected to do better, but I think the the LDA is better than QDA in general. The reason I suspect this has to do with the way the variance is calculated. If the Bayes decision boundary is linear, then know that for all \\(k\\), \\(\\sigma_k=\\sigma\\). So when we estimate \\(\\sigma\\), we can use the observations from all the different populations. But when we do QDA, we have to estimate \\(\\sigma_k\\) differently for each \\(k\\). This means that we have to divide up our observations to estimate each \\(\\sigma_k\\). Thus, when using QDA, we might have a bunch of \\(\\sigma_k\\) that are slightly off from \\(\\sigma\\), creating a source of error. I think it is safe to concldue that LDA would do better on the test set. Otherwise, why even bother doing LDA? This question depends on the non-linearity of the boundary. In general, we would expect QDA to outperform LDA on both training and test sets. However, one can image a tiny pertubration to linearity might still be better suited for LDA. For example, if \\(\\sigma_1=\\sigma+\\epsilon\\) and \\(\\sigma_2=\\sigma\\) where \\(\\epsilon\\) is some number small relative to \\(\\sigma\\). We would expect the test accuracy of QDA to improve versus LDA. This is because as we get more \\(n\\), we get much better estimates to \\(\\sigma_k\\). If it turns out that \\(\\sigma_k=\\sigma\\) for all \\(k\\), then QDA will still capture this really well. Especially since variance estimates have diminishing returns relative to \\(n\\). False, for the reasons stated in a. b_0 &lt;- -5; b_1 &lt;- 0.05; b_2 &lt;- 1 p &lt;- exp(b_0 + b_1*40 + b_2*3.5) p/(1+p) ## [1] 0.6224593 Note if \\(p = .5\\), that means that \\[ \\frac{1}{2} = \\frac{\\exp(\\beta \\cdot x)}{1+\\exp(\\beta \\cdot x)} \\] or that \\(\\exp(\\beta \\cdot x) = 1\\). Note that I switched to vector notation. Taking logs of both sides, we have \\(\\beta \\cdot x = 0\\). Going back to our problem-specific model, we have \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0\\). Solving for \\(x_2\\) gives \\[x_2 = \\frac{-\\beta_0 - \\beta_2 x_2}{\\beta_1}\\] (-b_0 - b_2*3.5)/b_1 ## [1] 30 sigma &lt;- 6 mu1 &lt;- 10 mu2 &lt;- 0 pi1 &lt;- .8 pi2 &lt;- 1 - pi1 f1 &lt;- dnorm(4,mu1,sigma) f2 &lt;- dnorm(4,mu2,sigma) pi1*f1/(pi1*f1+pi2*f2) ## [1] 0.7518525 Since we can expect a 1-nearest neighbor to have perfect test error, we know and the data set is divided evenly, that the test error was 36%, so we would go with the logsitic regression. Let \\(O\\) be the odds. \\(1/(1-p) = O\\) gives \\(p=O/(1+O)\\). .37/(1+.37) ## [1] 0.270073 .16/(1-.16) ## [1] 0.1904762 data(Weekly,package=&quot;ISLR&quot;) head(Weekly) ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## 1 1990 0.816 1.572 -3.936 -0.229 -3.484 0.1549760 -0.270 Down ## 2 1990 -0.270 0.816 1.572 -3.936 -0.229 0.1485740 -2.576 Down ## 3 1990 -2.576 -0.270 0.816 1.572 -3.936 0.1598375 3.514 Up ## 4 1990 3.514 -2.576 -0.270 0.816 1.572 0.1616300 0.712 Up ## 5 1990 0.712 3.514 -2.576 -0.270 0.816 0.1537280 1.178 Up ## 6 1990 1.178 0.712 3.514 -2.576 -0.270 0.1544440 -1.372 Down a. ```r summary(Weekly) ``` ``` ## Year Lag1 Lag2 Lag3 ## Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 ## 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 ## Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 ## Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 ## 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 ## Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 ## Lag4 Lag5 Volume ## Min. :-18.1950 Min. :-18.1950 Min. :0.08747 ## 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 ## Median : 0.2380 Median : 0.2340 Median :1.00268 ## Mean : 0.1458 Mean : 0.1399 Mean :1.57462 ## 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 ## Max. : 12.0260 Max. : 12.0260 Max. :9.32821 ## Today Direction ## Min. :-18.1950 Down:484 ## 1st Qu.: -1.1540 Up :605 ## Median : 0.2410 ## Mean : 0.1499 ## 3rd Qu.: 1.4050 ## Max. : 12.0260 ``` ```r cor(Weekly[,-c(1,9)]) ``` ``` ## Lag1 Lag2 Lag3 Lag4 Lag5 ## Lag1 1.000000000 -0.07485305 0.05863568 -0.071273876 -0.008183096 ## Lag2 -0.074853051 1.00000000 -0.07572091 0.058381535 -0.072499482 ## Lag3 0.058635682 -0.07572091 1.00000000 -0.075395865 0.060657175 ## Lag4 -0.071273876 0.05838153 -0.07539587 1.000000000 -0.075675027 ## Lag5 -0.008183096 -0.07249948 0.06065717 -0.075675027 1.000000000 ## Volume -0.064951313 -0.08551314 -0.06928771 -0.061074617 -0.058517414 ## Today -0.075031842 0.05916672 -0.07124364 -0.007825873 0.011012698 ## Volume Today ## Lag1 -0.06495131 -0.075031842 ## Lag2 -0.08551314 0.059166717 ## Lag3 -0.06928771 -0.071243639 ## Lag4 -0.06107462 -0.007825873 ## Lag5 -0.05851741 0.011012698 ## Volume 1.00000000 -0.033077783 ## Today -0.03307778 1.000000000 ``` This graph is all pairwise comparisons of the predictors with the color denoting the direction. It&#39;s hard to see a trend here. ```r pairs(Weekly[-(8:9)],col=Weekly$Direction) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-74-1.png&quot; width=&quot;672&quot; /&gt; Plotting the predictors against the `Today` variable also makes it hard to see any trends. ```r par(mfcol=c(3,2)) predictors &lt;- c(paste0(&quot;Lag&quot;,1:5),&quot;Volume&quot;) for (p in predictors){ plot(Weekly[,p],Weekly[,&quot;Today&quot;], ylab=&quot;Today&quot;, xlab=p, col=Weekly[,&quot;Direction&quot;]) } ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-75-1.png&quot; width=&quot;672&quot; /&gt; Boxplots prove unhelpful too. ```r par(mfcol=c(3,2)) for(p in predictors){ boxplot(as.formula(paste0(p,&quot;~Direction&quot;)),data=Weekly) } ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-76-1.png&quot; width=&quot;672&quot; /&gt; I&#39;m out of ideas for plots. This is not an easy data set to work with. b. ```r glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,data=Weekly, family=binomial) summary(glm.fit) ``` ``` ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 ``` The Lag2 is statistically significant at a p-value of .03, but this is a pretty low bar considering we have 6 predictors. c. ```r glm.probs &lt;- predict(glm.fit,type=&quot;response&quot;) glm.pred &lt;- rep(&quot;Down&quot;,nrow(Weekly)) glm.pred[glm.probs &gt; .5] &lt;- &quot;Up&quot; table(glm.pred,Weekly$Direction) #confusion table ``` ``` ## ## glm.pred Down Up ## Down 54 48 ## Up 430 557 ``` ```r sum(glm.pred==Weekly$Direction)/nrow(Weekly) #overall fraction of correct answers ``` ``` ## [1] 0.5610652 ``` From the confusion table, we can see that our model is simply predicting `Up` a lot of times. Our overall accuracy of 56% isn&#39;t bad. Since the data is roughly divided between down and up, the overall accuracy is a good measure of how good our model is. Our model seems to say that just saying the market is going up is a pretty good bet most of the time. d. ```r train.inds &lt;- Weekly$Year &lt;= 2008 train.data &lt;- Weekly[train.inds,] test.data &lt;- Weekly[!train.inds,] test.n &lt;- nrow(test.data) lag2.mod &lt;- glm(Direction ~ Lag2, data=Weekly, family=binomial, subset=train.inds) summary(lag2.mod) ``` ``` ## ## Call: ## glm(formula = Direction ~ Lag2, family = binomial, data = Weekly, ## subset = train.inds) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.536 -1.264 1.021 1.091 1.368 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.20326 0.06428 3.162 0.00157 ** ## Lag2 0.05810 0.02870 2.024 0.04298 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1354.7 on 984 degrees of freedom ## Residual deviance: 1350.5 on 983 degrees of freedom ## AIC: 1354.5 ## ## Number of Fisher Scoring iterations: 4 ``` ```r lag2.pred &lt;- rep(&quot;Down&quot;,test.n) lag2.probs &lt;- predict(glm.fit,test.data,type=&quot;response&quot;) lag2.pred[lag2.probs&gt;.5] &lt;- &quot;Up&quot; table(lag2.pred,test.data$Direction) ``` ``` ## ## lag2.pred Down Up ## Down 17 13 ## Up 26 48 ``` ```r sum(lag2.pred==test.data$Direction)/test.n ``` ``` ## [1] 0.625 ``` e. ```r library(MASS) #For lda function lag2.lda&lt;-lda(Direction ~ Lag2, data=train.data) lag2.lda.pred &lt;- predict(lag2.lda, test.data)$class table(lag2.lda.pred,test.data$Direction) ``` ``` ## ## lag2.lda.pred Down Up ## Down 9 5 ## Up 34 56 ``` ```r sum(lag2.lda.pred==test.data$Direction)/test.n ``` ``` ## [1] 0.625 ``` f. ```r library(MASS) #For lda function lag2.qda&lt;-qda(Direction ~ Lag2, data=train.data) lag2.qda.pred &lt;- predict(lag2.qda, test.data)$class table(lag2.qda.pred,test.data$Direction) ``` ``` ## ## lag2.qda.pred Down Up ## Down 0 0 ## Up 43 61 ``` ```r sum(lag2.qda.pred==test.data$Direction)/test.n ``` ``` ## [1] 0.5865385 ``` g. ```r library(class) set.seed(1) train.X &lt;- as.matrix(train.data[,&quot;Lag2&quot;]) train.Y &lt;- as.matrix(train.data[,&quot;Direction&quot;]) test.X &lt;- as.matrix(test.data[,&quot;Lag2&quot;]) test.Y &lt;- as.matrix(test.data[,&quot;Direction&quot;]) lag2.knn.pred &lt;- knn(train.X,test.X,train.Y,k=1) table(lag2.knn.pred,test.Y) ``` ``` ## test.Y ## lag2.knn.pred Down Up ## Down 21 30 ## Up 22 31 ``` ```r sum(lag2.knn.pred == test.Y)/test.n ``` ``` ## [1] 0.5 ``` h. It appears that both the LDA and the logistic regression performed similarly on the test set. Let&#39;s see which one has lower training error. ```r glm.probs &lt;- predict(lag2.mod) #logistic regression glm.preds &lt;- rep(&quot;Down&quot;,length(glm.probs)) glm.preds[glm.probs&gt;.5] &lt;- &quot;Up&quot; glm.train.acc &lt;- sum(glm.preds == train.data$Direction)/nrow(train.data) lda.preds &lt;- predict(lag2.lda,type=&quot;response&quot;)$class lda.train.acc &lt;- sum(lda.preds == train.data$Direction)/nrow(train.data) cbind(lda.train.acc,glm.train.acc) ``` ``` ## lda.train.acc glm.train.acc ## [1,] 0.5543147 0.4568528 ``` Since the LDA has higher training accuraccy, I will say that the LDA provides the best results. i. I will simply experiment with different values of $K$. ```r k.vals &lt;- 1:50 err &lt;- double(50) for (k in k.vals){ train.X &lt;- as.matrix(train.data[,&quot;Lag2&quot;]) train.Y &lt;- as.matrix(train.data[,&quot;Direction&quot;]) test.X &lt;- as.matrix(test.data[,&quot;Lag2&quot;]) test.Y &lt;- as.matrix(test.data[,&quot;Direction&quot;]) lag2.knn.pred &lt;- knn(train.X,test.X,train.Y,k=k) table(lag2.knn.pred,test.Y) err[k]&lt;-sum(lag2.knn.pred == test.Y)/test.n } plot(k.vals,err,type=&quot;b&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-85-1.png&quot; width=&quot;672&quot; /&gt; The results are everywhere, but surprisingly, it seems like $k=1$ gives the best result on the test data. data(Auto,package=&quot;ISLR&quot;) head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 a. ```r Auto01 &lt;- data.frame(Auto,mpg01=(as.integer(Auto$mpg &gt;= median(Auto$mpg)))) ``` b. After fiddling around, I felt like boxplots were the best plot for almost all the views. The only view I don&#39;t use a boxplot for is `mpg01 ~ origin`. It seems like a lot of the variables are pretty good predictors. ```r par(mfcol=c(2,3)) for (p in 2:7){ var&lt;-names(Auto01)[p] f &lt;- as.formula(paste0(var,&quot;~mpg01&quot;)) boxplot(f ,xlab=names(Auto01)[p], data = Auto01) } ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-88-1.png&quot; width=&quot;672&quot; /&gt; ```r plot(factor(Auto01$mpg01),factor(Auto01$origin),xlab=&quot;mpg01&quot;,ylab=&quot;origin&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-88-2.png&quot; width=&quot;672&quot; /&gt; c. I will split the data into a 60% training set and a 40% test set. ```r set.seed(927) n &lt;- nrow(Auto01) sample.inds&lt;-sample.int(n, round(n*.6)) train&lt;-Auto01[sample.inds,] test&lt;-Auto01[-sample.inds,] ``` d. I exclude acceleration because the association appears weak. In addition, I leave out origin since I am not sure how it handles factor variables. The test error comes out to 11.5%. ```r require(MASS) form &lt;- mpg01 ~ cylinders + horsepower + displacement + weight + year lda.mod &lt;- lda(form, data=train) lda.pred &lt;- predict(lda.mod,test)$class sum(lda.pred != test$mpg01)/nrow(test) # test error ``` ``` ## [1] 0.1146497 ``` e. The test error for the QDA s 12.7%. ```r qda.mod &lt;- qda(form, data=train) qda.pred &lt;- predict(qda.mod,test)$class sum(qda.pred != test$mpg01)/nrow(test) ``` ``` ## [1] 0.1273885 ``` f. Logistic regression gives 9.5% test error. ```r log.mod &lt;- glm(form, data=train, family=binomial) log.pred &lt;- predict(log.mod, test, type=&quot;response&quot;) log.pred &lt;- ifelse(log.pred &gt; .5, 1, 0) sum(log.pred != test$mpg01)/nrow(test) ``` ``` ## [1] 0.0955414 ``` g. It appears that $k=10$ is probably the best choice. require(class) set.seed(927) res&lt;-data.frame(ks=1:20,errs=1:20) preds&lt;-c(2:4,6) for(i in seq(along=res$ks)){ k &lt;- res$ks[i] knn.pred &lt;- knn(train[,preds],test[,preds],train[,&quot;mpg01&quot;],k=k) res$errs[i] &lt;- sum(knn.pred!=test$mpg01)/nrow(test) } plot(res$ks,res$err,type=&quot;b&quot;) head(res[order(res$errs),]) ## ks errs ## 10 10 0.1019108 ## 11 11 0.1019108 ## 3 3 0.1082803 ## 5 5 0.1082803 ## 12 12 0.1082803 ## 13 13 0.1082803 Power &lt;- function() print(2^3) Power() ## [1] 8 Power2 &lt;- function(x,a) print(x^a) Power2(3,8) ## [1] 6561 Power2(10,3) ## [1] 1000 Power2(8,17) ## [1] 2.2518e+15 Power2(131,3) ## [1] 2248091 Power3 &lt;- function(x,a) x^a plot(1:10,Power3(1:10,2),main=&quot;f(x)=x^2&quot;,xlab=&quot;x&quot;,ylab=&quot;x^2&quot;,log=&quot;y&quot;) PlotPower &lt;- function(x, a) plot(x,Power3(x,a),main=paste0(&quot;f(x)=x^&quot;,a),ylab=paste0(&quot;x^&quot;,a)) PlotPower(1:10,3) data(Boston,package=&quot;MASS&quot;) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 create our indicator variable to see if crime is above or below the median Boston$crim01 &lt;- Boston$crim &gt; median(Boston$crim) summary(Boston$crim01) ## Mode FALSE TRUE ## logical 253 253 Looking for relationships: bp_vars &lt;- c(&quot;zn&quot;, &quot;indus&quot;, &quot;nox&quot;, &quot;rm&quot;,&quot;age&quot;,&quot;dis&quot;,&quot;tax&quot;,&quot;ptratio&quot;,&quot;black&quot;,&quot;lstat&quot;,&quot;medv&quot;) # variables I want to make boxplots of par(mfcol=c(3,4)) for (v in bp_vars) { boxplot(as.formula(paste0(v,&quot;~ crim01&quot;)),data=Boston,main=v) } plot(table(Boston$crim01,Boston$chas),main=&quot;chas&quot;) I will compare hand picked variables versus using all of them on train and test sets for all the different methods. I will use accuracy as our error metric since our data is very evenly split between above and below median. require(caret) ## Loading required package: caret ## Loading required package: lattice ## Loading required package: ggplot2 set.seed(1) train.inds &lt;- createDataPartition(Boston$crim01,p=.6,list=FALSE,times=1) train &lt;- Boston[train.inds,] test &lt;- Boston[-train.inds,] vars.hand &lt;- c(&quot;zn&quot;, &quot;tax&quot;, &quot;lstat&quot;, &quot;Indus&quot;, &quot;age&quot;, &quot;ptratio&quot;, &quot;nox&quot;, &quot;dis&quot;) hand.form &lt;- as.formula(paste0(&quot;crim01 ~&quot;,vars.hand)) hand.mod &lt;- glm(hand.form, data=train, family=binomial) all.mod &lt;- glm(crim01 ~ . - crim, data=train, family=binomial) #generates warning ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred hand.pred &lt;- predict(hand.mod, test, type=&quot;response&quot;) &gt; .5 all.pred &lt;- predict(all.mod, test, type=&quot;response&quot;) &gt; .5 table(hand.pred, test$crim01) ## ## hand.pred FALSE TRUE ## FALSE 51 6 ## TRUE 50 95 table(all.pred, test$crim01) ## ## all.pred FALSE TRUE ## FALSE 92 12 ## TRUE 9 89 mean(hand.pred == test$crim01) ## [1] 0.7227723 mean(all.pred==test$crim01) ## [1] 0.8960396 It seems like using all the variables is better. I’m curious if reducing via the step function will increase the accuracy. step.mod &lt;- step(all.mod) ## Start: AIC=144.74 ## crim01 ~ (crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + black + lstat + medv) - crim ## ## Df Deviance AIC ## - rm 1 117.39 143.39 ## - medv 1 118.01 144.01 ## - chas 1 118.18 144.18 ## - zn 1 118.70 144.71 ## &lt;none&gt; 116.73 144.74 ## - age 1 119.06 145.06 ## - indus 1 119.27 145.27 ## - ptratio 1 119.98 145.98 ## - dis 1 120.05 146.05 ## - lstat 1 120.36 146.37 ## - tax 1 121.01 147.01 ## - black 1 128.26 154.26 ## - rad 1 137.42 163.42 ## - nox 1 148.98 174.98 ## ## Step: AIC=143.39 ## crim01 ~ zn + indus + chas + nox + age + dis + rad + tax + ptratio + ## black + lstat + medv ## ## Df Deviance AIC ## - chas 1 118.70 142.71 ## - zn 1 119.26 143.26 ## &lt;none&gt; 117.39 143.39 ## - indus 1 119.82 143.82 ## - lstat 1 120.37 144.37 ## - dis 1 121.41 145.41 ## - tax 1 121.57 145.57 ## - age 1 122.75 146.75 ## - ptratio 1 122.82 146.82 ## - medv 1 127.40 151.40 ## - black 1 129.03 153.03 ## - rad 1 138.64 162.64 ## - nox 1 150.55 174.55 ## ## Step: AIC=142.7 ## crim01 ~ zn + indus + nox + age + dis + rad + tax + ptratio + ## black + lstat + medv ## ## Df Deviance AIC ## - indus 1 120.52 142.52 ## &lt;none&gt; 118.70 142.71 ## - zn 1 121.19 143.19 ## - lstat 1 122.40 144.40 ## - dis 1 122.57 144.57 ## - ptratio 1 123.15 145.15 ## - tax 1 123.91 145.91 ## - age 1 125.09 147.09 ## - black 1 129.60 151.60 ## - medv 1 129.69 151.69 ## - rad 1 143.59 165.59 ## - nox 1 150.60 172.60 ## ## Step: AIC=142.52 ## crim01 ~ zn + nox + age + dis + rad + tax + ptratio + black + ## lstat + medv ## ## Df Deviance AIC ## &lt;none&gt; 120.52 142.52 ## - lstat 1 123.12 143.12 ## - zn 1 123.34 143.34 ## - dis 1 124.32 144.32 ## - ptratio 1 124.48 144.48 ## - age 1 126.57 146.57 ## - tax 1 128.85 148.85 ## - medv 1 130.60 150.60 ## - black 1 130.62 150.62 ## - rad 1 150.74 170.74 ## - nox 1 150.92 170.92 step.pred &lt;- predict(step.mod, test, type=&quot;response&quot;) &gt; .5 mean(step.pred == test$crim01) ## [1] 0.8861386 I guess using all the predictors is the best choice here. Let’s compare with all the other models. require(MASS) lda.mod &lt;- lda(crim01 ~ . - crim, data=train) qda.mod &lt;- qda(crim01 ~ . - crim, data=train) lda.pred &lt;- predict(lda.mod, test)$class qda.pred &lt;- predict(qda.mod, test)$class mean(lda.pred == test$crim01) ## [1] 0.8366337 mean(qda.pred == test$crim01) ## [1] 0.8663366 require(class) train[,-15]&lt;-scale(train[,-15]) accs &lt;- double() for(k in 1:30){ knn.pred &lt;- knn(train[,-c(1,15)],test[,-c(1,15)],train$crim01,k=k) accs[k]&lt;-mean(knn.pred == test$crim01) } best.k &lt;- which(accs==max(accs),arr.ind=TRUE)[1] accs[best.k] ## [1] 0.5940594 It looks like logistic regression wins on just using all the predictors. "],
["chap5.html", "Chapter 5 Resampling Methods", " Chapter 5 Resampling Methods Using the property that \\(Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\), \\[\\begin{equation} Var(\\alpha X + (1-\\alpha)Y) = \\alpha^2 \\sigma_X^2 + (1-\\alpha)^2 \\sigma_Y^2 + 2\\alpha (1-\\alpha) \\sigma_{XY} \\tag{5.1} \\end{equation}\\] Taking the derivative with respect to \\(\\alpha\\) gives \\[\\begin{align*} \\frac{d}{d\\alpha} (\\alpha^2 \\sigma_X^2 + (1-\\alpha)^2 \\sigma_Y^2 + 2\\alpha (1-\\alpha) \\sigma_{XY}) &amp;= 2\\alpha \\sigma_X^2 - 2(1-\\alpha)\\sigma_Y^2 - 2(1-2\\alpha) \\sigma_{XY}\\\\ &amp;= 2\\alpha(\\sigma_X^2 + \\sigma_Y^2 + 2 \\sigma_{XY}) - 2( \\sigma_Y^2 - \\sigma_{XY}) \\end{align*}\\] Setting equal to zero and solving we get \\[\\begin{align*} \\alpha(\\sigma_X^2 + \\sigma_Y^2 + 2 \\sigma_{XY}) &amp;= \\sigma_Y^2 - \\sigma_{XY}\\\\ \\alpha &amp;= \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 + 2 \\sigma_{XY}} \\end{align*}\\] If we choose one at random, then the chance it is the \\(j\\)th observation is \\(\\frac{1}{n}\\), but we want the complement of that, so the probability is \\(1-\\frac{1}{n}\\) We are choosing with replacement, so that means we can draw the first observation again, so the probability stays the same, \\(1-\\frac{1}{n}\\). The chance of not choosing \\(j\\) two times in a row is \\((1-\\frac{1}{n})(1-\\frac{1}{n})=(1-\\frac{1}{n})^2\\) since these are independent since we draw with replacement. We are going to draw \\(n\\) times, so it will be \\((1-\\frac{1}{n})^n\\). (1-1/5)^5= 0.32768 (1-1/100)^100= 0.3660323 (1-1/10000)^10000= 0.367861 It’s converging. Note it’s horizontal for a lot of the values. plot(1:100000,sapply(1:100000,FUN=function(x) (1-1/x)^x), type=&quot;l&quot;) set.seed(927) store &lt;- rep(NA,10000) for (i in 1:10000){ store[i]=sum(sample(1:100,rep=TRUE)==4)&gt;0 } mean(store) ## [1] 0.6327 Note that 1-mean(store)= 0.3673, which is close to our estimate of 100. We divide up our data into \\(k\\) sets. for each \\(i\\), we train the data on the sets \\(1, \\dots, i-1, i+1, \\dots, k\\). We then use the \\(i\\)th to get the mean test error. After we are done, we have \\(k\\) mean test errors. The validation set approach will only give us a single value estimate for the error. In addition, the error on the validation set is a poor reflection of the performance of the full model where we use all the data. \\(k\\)-fold CV also uses every single piece of data as both a train and test data point. The disadvantage if that each of our \\(k\\) has more variability individually since our test errors are run on smaller sets. \\(k\\)-fold CV is much less computationally intensive than LOOCV in general. \\(k\\)-fold also tends to give better estimates of the error over LOOCV. LOOCV however is computationally faster for linear approximators (i.e. they can be represented by a linear system such as \\(\\hat{y} = Ay\\)). We would use bootstrapping. In order to use it, we take a bootstrap sample of our \\(X\\) data. We then run the model and estimate \\(Y\\) at \\(X\\). We do that 10,000 (or however many) times to get \\(Y_{(b)}\\).. Once we do that, we can calculate the standard deviation of \\(Y_{(1)}, \\dots, Y_{(10,000)}\\). set.seed(927) data(Default,package=&quot;ISLR&quot;) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 a. ```r log.mod &lt;- glm(default ~ income + balance, data=Default, family=binomial) ``` b. ```r run.validation &lt;- function() { n&lt;-nrow(Default) train.inds &lt;- sample(n,n*.5,replace=FALSE) #i valid.mod &lt;- glm(default ~ income + balance, data=Default, subset=train.inds, family=binomial) #ii valid.pred &lt;- ifelse(predict(valid.mod, Default[-train.inds,], type=&quot;response&quot;)&gt;.5,&quot;Yes&quot;,&quot;No&quot;) #iii mean(valid.pred == Default$default[-train.inds]) #iv } run.validation() ``` ``` ## [1] 0.9716 ``` c. ```r for(i in 1:3){ print(run.validation()) } ``` ``` ## [1] 0.975 ## [1] 0.9756 ## [1] 0.9726 ``` d. ```r for(i in 1:4){ n&lt;-nrow(Default) train.inds &lt;- sample(n,n*.5,replace=FALSE) #i valid.mod &lt;- glm(default ~ income + balance + student, data=Default, subset=train.inds, family=binomial) #ii valid.pred &lt;- ifelse(predict(valid.mod, Default[-train.inds,], type=&quot;response&quot;)&gt;.5,&quot;Yes&quot;,&quot;No&quot;) #iii print(mean(valid.pred == Default$default[-train.inds])) #iv } ``` ``` ## [1] 0.9726 ## [1] 0.975 ## [1] 0.9724 ## [1] 0.9784 ``` The two error rates look pretty similar overall, so adding in student didn&#39;t help much. We can see this in the `summary` out of the model. ```r summary(valid.mod) ``` ``` ## ## Call: ## glm(formula = default ~ income + balance + student, family = binomial, ## data = Default, subset = train.inds) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9885 -0.1644 -0.0707 -0.0293 3.5299 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.006e+01 6.466e-01 -15.553 &lt;2e-16 *** ## income 6.927e-06 1.135e-05 0.610 0.542 ## balance 5.166e-03 2.870e-04 18.001 &lt;2e-16 *** ## studentYes -5.065e-01 3.242e-01 -1.562 0.118 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1517.1 on 4999 degrees of freedom ## Residual deviance: 873.5 on 4996 degrees of freedom ## AIC: 881.5 ## ## Number of Fisher Scoring iterations: 8 ``` set.seed(927) data(Default,package=&quot;ISLR&quot;) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 a. ```r glm.mod &lt;- glm(default ~ income + balance, data=Default, family=binomial) summary(glm.mod) ``` ``` ## ## Call: ## glm(formula = default ~ income + balance, family = binomial, ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 &lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## balance 5.647e-03 2.274e-04 24.836 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 ``` b. ```r require(boot) ``` ``` ## Loading required package: boot ``` ``` ## ## Attaching package: &#39;boot&#39; ``` ``` ## The following object is masked from &#39;package:lattice&#39;: ## ## melanoma ``` ```r boot.fn &lt;- function(data.set, inds) { glm.mod &lt;- glm(default ~ income + balance, data=data.set, subset=inds, family=binomial) coef(glm.mod) } ``` c. ```r boot(Default, boot.fn,R=1000) ``` ``` ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Default, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -1.154047e+01 -3.538983e-02 4.415928e-01 ## t2* 2.080898e-05 -1.565000e-07 4.819321e-06 ## t3* 5.647103e-03 2.289379e-05 2.322362e-04 ``` d. They are extremely close. They are the same magnitude, and agree on the first digit. data(Weekly,package=&quot;ISLR&quot;) a. (What is the point of this partof the exercise?) ```r all.data &lt;- glm(Direction ~ Lag1 + Lag2, data=Weekly, family=binomial) ``` b. ```r sans.first &lt;- glm(Direction ~ Lag1 + Lag2, data=Weekly[-1,], family=binomial) ``` c. ```r (pred&lt;-ifelse(predict(sans.first, Weekly[1,],type=&quot;response&quot;) &gt; .5,&quot;Up&quot;,&quot;Down&quot;)) ``` ``` ## 1 ## &quot;Up&quot; ``` ```r pred == Weekly$Direction[1] ``` ``` ## 1 ## FALSE ``` The model incorrectly classified the first result. d. ```r errs &lt;- double(nrow(Weekly)) for(i in 1:nrow(Weekly)){ i.mod&lt;-glm(Direction ~ Lag1 + Lag2, data=Weekly[-i,], family=binomial) pred&lt;-ifelse(predict(i.mod,Weekly[i,],type=&quot;response&quot;)&gt;.5,&quot;Up&quot;,&quot;Down&quot;) errs[i] &lt;- pred != Weekly[i,]$Direction } ``` e. ```r mean(errs) ``` ``` ## [1] 0.4499541 ``` set.seed(1) x&lt;-rnorm(100) y &lt;- x-2*x^2 + rnorm(100) \\(n=100\\), \\(p=1\\). The model is \\(Y = X- 2*X^2+\\epsilon\\). plot(x,y) simulated &lt;- data.frame(x,y) cv.est &lt;- double(4) set.seed(927) for (i in 1:4){ cv.mod &lt;- glm(y ~ poly(x,degree=i), data=simulated) cv.err &lt;- cv.glm(simulated, cv.mod) cv.est[i] &lt;- cv.err$delta[1] } cv.est ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 These shouldn’t differ since there is no randomization with LOOCV. simulated &lt;- data.frame(x,y) cv.est &lt;- double(4) set.seed(1) for (i in 1:4){ cv.mod &lt;- glm(y ~ poly(x,degree=i), data=simulated) cv.err &lt;- cv.glm(simulated, cv.mod) cv.est[i] &lt;- cv.err$delta[1] } cv.est ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 The smallest LOOCV estimate is the second model which is the quadratic model. This is what we expect since the original data set was simulated using a quadratic relationship. sums &lt;- list(4) for (i in 1:4){ cv.mod &lt;- glm(y ~ poly(x,degree=i), data=simulated) sums[[i]] &lt;- summary(cv.mod)$coefficients } sums ## [[1]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.2600138 -5.961308 3.953542e-08 ## poly(x, degree = i) 6.188826 2.6001382 2.380191 1.923846e-02 ## ## [[2]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.09580323 -16.179231 2.656229e-29 ## poly(x, degree = i)1 6.188826 0.95803228 6.459934 4.184810e-09 ## poly(x, degree = i)2 -23.948305 0.95803228 -24.997388 4.584330e-44 ## ## [[3]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.5500226 0.09626318 -16.101926 4.995066e-29 ## poly(x, degree = i)1 6.1888256 0.96263178 6.429068 4.971565e-09 ## poly(x, degree = i)2 -23.9483049 0.96263178 -24.877950 1.216703e-43 ## poly(x, degree = i)3 0.2641057 0.96263178 0.274358 7.843990e-01 ## ## [[4]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.5500226 0.09590514 -16.1620379 5.169227e-29 ## poly(x, degree = i)1 6.1888256 0.95905143 6.4530695 4.590732e-09 ## poly(x, degree = i)2 -23.9483049 0.95905143 -24.9708243 1.593826e-43 ## poly(x, degree = i)3 0.2641057 0.95905143 0.2753822 7.836207e-01 ## poly(x, degree = i)4 1.2570950 0.95905143 1.3107691 1.930956e-01 We get pretty high p-values for \\(\\beta_3\\) and \\(\\beta_4\\) coefficients, so our results agree with the conclusions based on the cross-validation results. data(Boston, package=&quot;MASS&quot;) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 a. ```r (mu.hat &lt;- mean(Boston$medv)) ``` ``` ## [1] 22.53281 ``` b. ```r (mu.hat.se &lt;- sd(Boston$medv)/sqrt(nrow(Boston))) ``` ``` ## [1] 0.4088611 ``` c. ```r set.seed(927) (mu.hat.boot&lt;-boot(Boston$medv, function(x,i) mean(x[i]) , R=10000)) ``` ``` ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(x, i) mean(x[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 22.53281 0.005333617 0.4001154 ``` The two estimates differ by less than 0.01. d. ```r boot.ci(mu.hat.boot,type=&quot;norm&quot;) ``` ``` ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = mu.hat.boot, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (21.74, 23.31 ) ## Calculations and Intervals on Original Scale ``` e ```r (mu.med &lt;- median(Boston$medv)) ``` ``` ## [1] 21.2 ``` f. ```r (mu.med.boot &lt;- boot(Boston$medv, function(x,i) median(x[i]), R=10000)) ``` ``` ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(x, i) median(x[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 21.2 -0.006375 0.3800218 ``` ```r boot.ci(mu.med.boot,type=&quot;norm&quot;) ``` ``` ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = mu.med.boot, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (20.46, 21.95 ) ## Calculations and Intervals on Original Scale ``` We can expect our median error to be within the CI given above. g. ```r quantile(Boston$medv,probs=.1) ``` ``` ## 10% ## 12.75 ``` f. ```r boot(Boston$medv, function(x,i) quantile(x[i],probs=.1), R=10000) ``` ``` ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(x, i) quantile(x[i], ## probs = 0.1), R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 12.75 0.004175 0.4982235 ``` "],
["chap6.html", "Chapter 6 Linear Model Selection and Regularization", " Chapter 6 Linear Model Selection and Regularization Best subset will have the best training RSS. This is because it exhaustively searches every possible choice, whereas forward and backward-selection do not, so they might miss the optimal choice. There is not enough information to conclude which one will have the smallest test RSS. True. Forward selection works by retaining the selectors chosen in the previous step. True. Backward selection removes the worst predictor at each step, retaining the predictors at the previous step. False. Backwards might select a different set than the forward selection at a given \\(k\\). False. False. Best subset might replace a predictor going from \\(k\\) to \\(k+1\\). See Table 6.1 on page 209. iii is Correct. The tuning parameter allows us to select a simpler model. Since simpler models have less bias, we expect the error to go down as long as the increase in variance is less than the increase in bias. iii, for the same reason. ii is correct for non-linear models. They can fit a wider class of functions than just linear models (often linear is a subset). This means the model does not bias towards a specific class of functions. However, this comes at the cost of increased variance. is correct. As we increase \\(s\\), we allow the \\(\\beta_j\\) more freedom is minimizing the square residuals. The more freedom we allow, the better we will fit the training data. is correct. At this point, we are facing the bias-variance trade-off which comes in the usual U-shaped curve. We allow more flexibility in the model which means we will have more variance. As we increase \\(\\lambda\\), we bias away from only allowing a few large (relative to their least squares values) coefficients. The irreducible error is not affected by the choice of model. These answers are the same as 3. My solution to this is a little hand-wavey and not nearly rigorous enough. I will have to come back to it if it really irks me. We wish to minimize \\[\\begin{equation} (y_1 - \\beta_1 x_{11} - \\beta_2 x_{12})^2 + (y_2 - \\beta_1 x_{21} - \\beta_2 x_{22})^2 - \\lambda( \\beta_1^2 + \\beta_2^2). \\end{equation}\\] If \\(x_{11} = x_{12}\\) and \\(x_{21}=x_{22}\\), then we can write \\[\\begin{equation} \\begin{split} (y_1 - \\beta_1 x_{11} - \\beta_2 x_{12})^2 + (y_2 - \\beta_1 x_{21} - \\beta_2 x_{22})^2 = \\\\ (y_1 - x_{11}(\\beta_1 + \\beta_2))^2 + (y_2 - x_{21}(\\beta_1 + \\beta_2 ))^2 \\end{split} \\tag{6.1} \\end{equation}\\] Then we want to minimize \\[\\begin{equation} (y_1 - x_{11}(\\beta_1 + \\beta_2))^2 + (y_2 - x_{21}(\\beta_1 + \\beta_2 ))^2 + \\lambda(\\beta_1^2+\\beta_2^2) \\end{equation}\\] there is some \\(\\beta_1 + \\beta_2 = c\\) that minimizes the above solution. However, there are ininfitely many \\(\\beta_1,\\beta_2\\) combinations such that \\(\\beta_1 + \\beta_2 = c\\). Then note that \\(\\beta_1 = c - \\beta_2\\). Plugging into \\(\\lambda(\\beta_1^2 - (c-\\beta_1)^2)\\) which has a known minimum at \\(\\beta_1 = \\beta_2\\), thus there is only one solution, that is \\(\\beta_1 = \\beta_2\\). \\[\\begin{equation} (y_1 - \\beta_1x_{11} - \\beta_2 x_{12})^2 + (y_2 - \\beta_1 x_{21} - \\beta_2 x_{22})^2 - \\lambda( |\\beta_1| + |\\beta_2|). \\end{equation}\\] Same argument as before, but now we have that \\[\\begin{equation} \\begin{split} (y_1 - \\beta_1x_{11} - \\beta_2 x_{12})^2 + (y_2 - \\beta_1 x_{21} - \\beta_2 x_{22})^2 = \\\\ (y_1 - x_{11}(\\beta_1 + \\beta_2))^2 + (y_2 - x_{21}(\\beta_1 + \\beta_2 )) \\end{split} \\end{equation}\\] and \\[\\begin{equation} (y_1 - x_{11}(\\beta_1 + \\beta_2))^2 + (y_2 - x_{21}(\\beta_1 + \\beta_2 ))^2 + \\lambda(|\\beta_1|+|\\beta_2|). \\end{equation}\\] Now note that there is some \\(\\beta_1+\\beta_2 = c\\) that minimizes the above equation. However, now \\(|\\beta_1| + |\\beta_2|\\) has an infiniten umber of minimizers. Any such \\(\\beta_1\\) and \\(\\beta_2\\) such that \\(\\beta_1+\\beta_2=c\\) works. Note that if \\(\\beta_2 = c\\), then \\(\\beta_1=0\\). Likewise, we can have \\(\\beta_1 = c\\) and \\(\\beta_2 = 0\\). The valid solutions are on the line \\(\\beta_1 + \\beta_2 = c\\) such that \\(0 \\leq \\beta_1,\\beta_2 \\leq c\\). The plot below shows \\(\\beta\\) versus \\((y_1 - \\beta)^2 + \\lambda \\beta^2\\) on the solid black line. The red dotted line corresponds to where \\(y_1/(1+\\lambda)\\) is. We can see that the ridge regression value is minimized at the red dotted line. y1 &lt;- 1 #y l &lt;- 10 #lambda betas &lt;- seq(0,1,.01) estimates &lt;- (y1-betas)^2+l*betas^2 plot(betas,estimates,type=&quot;l&quot;) abline(v=y1/(1+l),col=&quot;red&quot;,lty=2) b.The same plot is shown below. Since there is three cases, I made a plot for each par(mfcol=c(2,2)) ys=c(-1,0,1) for(y1 in ys){ l &lt;- 1 #lambda betas &lt;- seq(-1,1,.01) estimates &lt;- (y1-betas)^2+l*abs(betas) plot(betas,estimates,type=&quot;l&quot;) best_est &lt;- y1 - l/2 if(y1 &lt; -l/2) best_est &lt;- y1 + l/2 else if(abs(y1) &lt;= l/2) best_est&lt;-0; abline(v=best_est,col=&quot;red&quot;,lty=2) } The likelihood for this function will be \\[\\begin{equation} \\begin{split} L &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(y_i - (\\beta_0 + \\sum_{j=1}^px_{ij}\\beta_j))^2}{2\\sigma^2} \\right)\\\\ &amp;= \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j))^2}{2 \\sigma^2} \\right) \\end{split} \\end{equation}\\] The posterior is \\[\\begin{equation} \\begin{split} &amp;\\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j))^2}{2 \\sigma^2} \\right) \\prod_{j=1}^n \\frac{1}{2b} \\exp \\left(\\frac{-|\\beta_j|}{b} \\right)\\\\ =&amp; \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j))^2 - \\frac{2\\sigma^2}{b} \\sum_{j=1}^n |\\beta_j|}{2 \\sigma^2} \\right) \\end{split} \\tag{6.2} \\end{equation}\\] Looking at this last equation, we can see that if we let \\(\\lambda = \\frac{2 \\sigma^2}{b}\\), then the when we minimize \\[\\begin{equation} \\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j))^2 - \\lambda \\sum_{j=1}^n |\\beta_j| \\end{equation}\\] then we maximize (6.2). Maximizing (6.2) is equivalent to the mode. \\[\\begin{equation} \\begin{split} &amp;\\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j))^2}{2 \\sigma^2} \\right) \\prod_{j=1}^n \\frac{1}{\\sqrt{2 \\pi c}} \\exp \\left(-\\frac{\\beta_j^2}{2c} \\right) \\\\ =&amp; \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j))^2 + \\frac{\\sigma^2}{c} \\sum_{j=1}^p \\beta_j^2}{2 \\sigma^2} \\right) \\end{split} \\tag{6.3} \\end{equation}\\] Now let \\(\\lambda = \\frac{\\sigma^2}{c}\\). \\[\\begin{equation} \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right)^n \\exp \\left(-\\frac{\\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j))^2 + \\lambda \\sum_{j=1}^p \\beta_j^2}{2 \\sigma^2} \\right) \\end{equation}\\] Ridge regression minimizes \\[\\begin{equation} \\sum_{i=1}^n (y_i - (\\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j))^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\tag{6.4} \\end{equation}\\] and therefore minimizes (6.3), so is the mode. It is also the mean since the exponent (6.4) is a quadratic function which is symmetric around its minimum. set.seed(927) x &lt;- rnorm(100) eps &lt;- rnorm(100) b_0 &lt;- 2; b_1 &lt;- 2; b_2 &lt;- .5; b_3 &lt;- .5 y &lt;- b_0 + b_1 * x + b_2 * x^2 + b_3 * x^3 library(leaps) regfit.full &lt;- regsubsets(y ~ poly(x, degree=10, raw=TRUE), data=data.frame(x,y),nvmax=11) reg.summary &lt;- summary(regfit.full) print(&quot;Best adjr2 model&quot;) ## [1] &quot;Best adjr2 model&quot; coef(regfit.full, id=which.max(reg.summary$adjr2)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best BIC model&quot;) ## [1] &quot;Best BIC model&quot; coef(regfit.full, id=which.min(reg.summary$bic)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best Cp model&quot;) ## [1] &quot;Best Cp model&quot; coef(regfit.full, id=which.min(reg.summary$cp)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 We can see that they all return the 3 variable model, which is good. Below is a plot of the three different measures. par(mfcol=c(2,2)) plot(reg.summary$adjr2,xlab=&quot;Number of Variables&quot;, ylab=&quot;Adjusted R^2&quot;, type=&quot;l&quot;) plot(reg.summary$bic,xlab=&quot;Number of Variables&quot;, ylab=&quot;BIC&quot;, type=&quot;l&quot;) plot(reg.summary$cp,xlab=&quot;Number of Variables&quot;, ylab=&quot;AIC&quot;, type=&quot;l&quot;) We can see that the plots all cap out or minimize basically at three. regfit.fwd &lt;- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data.frame(x,y),nvmax=11, method=&quot;forward&quot;) reg.summary.f &lt;- summary(regfit.fwd) print(&quot;Best foward adjr2 model&quot;) ## [1] &quot;Best foward adjr2 model&quot; coef(regfit.full, id=which.max(reg.summary.f$adjr2)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best forward BIC model&quot;) ## [1] &quot;Best forward BIC model&quot; coef(regfit.full, id=which.min(reg.summary.f$bic)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best forward Cp model&quot;) ## [1] &quot;Best forward Cp model&quot; coef(regfit.full, id=which.min(reg.summary.f$cp)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 The forward still returns the three variable models. regfit.bwd &lt;- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data.frame(x,y),nvmax=11, method=&quot;backward&quot;) reg.summary.b &lt;- summary(regfit.bwd) print(&quot;True coefficients&quot;) ## [1] &quot;True coefficients&quot; c(b_0,b_1,b_2,b_3) ## [1] 2.0 2.0 0.5 0.5 print(&quot;Best backward adjr2 model&quot;) ## [1] &quot;Best backward adjr2 model&quot; coef(regfit.full, id=which.max(reg.summary.b$adjr2)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best backward BIC model&quot;) ## [1] &quot;Best backward BIC model&quot; coef(regfit.full, id=which.min(reg.summary.b$bic)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 print(&quot;Best backward Cp model&quot;) ## [1] &quot;Best backward Cp model&quot; coef(regfit.full, id=which.min(reg.summary.b$cp)) ## (Intercept) poly(x, degree = 10, raw = TRUE)1 ## 2.0 2.0 ## poly(x, degree = 10, raw = TRUE)2 poly(x, degree = 10, raw = TRUE)3 ## 0.5 0.5 The backwards also chooses the 3 variable models. Let’s compare the results for the BIC to see if they were the same for all the models. rbind(full=reg.summary$bic, forward=reg.summary.f$bic, backward=reg.summary.b$bic) ## [,1] [,2] [,3] [,4] [,5] [,6] ## full -294.9239 -360.2096 -7056.899 -7052.388 -7048.760 -7045.127 ## forward -294.9239 -360.2096 -7056.899 -7052.388 -7048.760 -7044.375 ## backward -294.9239 -360.2096 -7056.899 -7052.359 -7048.708 -7044.578 ## [,7] [,8] [,9] [,10] ## full -7040.571 -7036.856 -7033.163 -7028.707 ## forward -7040.171 -7036.856 -7033.163 -7028.707 ## backward -7040.274 -7036.856 -7033.163 -7028.707 library(glmnet) ## Loading required package: Matrix ## Loading required package: foreach ## Loaded glmnet 2.0-13 set.seed(1) x.mod &lt;- model.matrix(y ~ poly(x,degree=10,raw=TRUE), data=data.frame(x=y,y=y))[,-1] cv.out &lt;- cv.glmnet(x.mod,y,alpha=1) plot(cv.out) bestlam&lt;-cv.out$lambda.min lambda.mod &lt;- glmnet(x.mod,y,alpha=1) predict(lambda.mod,type=&quot;coefficients&quot;,s=bestlam) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.04818897 ## poly(x, degree = 10, raw = TRUE)1 0.96800733 ## poly(x, degree = 10, raw = TRUE)2 . ## poly(x, degree = 10, raw = TRUE)3 . ## poly(x, degree = 10, raw = TRUE)4 . ## poly(x, degree = 10, raw = TRUE)5 . ## poly(x, degree = 10, raw = TRUE)6 . ## poly(x, degree = 10, raw = TRUE)7 . ## poly(x, degree = 10, raw = TRUE)8 . ## poly(x, degree = 10, raw = TRUE)9 . ## poly(x, degree = 10, raw = TRUE)10 . The results of the Lasso aren’t great. I imagine this is because the linear model is perfect, so I don’t see how the Lasso would improve on anything. b_7 &lt;- 2.5 y7 &lt;- b_0 + b_0 * x^7 + eps deg7.full &lt;- regsubsets(y7 ~ poly(x,degree=10,raw=TRUE),data.frame(x,y7)) print(&quot;Best Cp model&quot;) ## [1] &quot;Best Cp model&quot; coef(deg7.full, id=which.min(reg.summary$cp)) ## (Intercept) poly(x, degree = 10, raw = TRUE)3 ## 1.86157516 0.18909447 ## poly(x, degree = 10, raw = TRUE)5 poly(x, degree = 10, raw = TRUE)7 ## -0.09457088 2.00877704 x7.mod &lt;- model.matrix(y7 ~ poly(x,degree=10), data=data.frame(x,y7))[,-1] cv.out &lt;- cv.glmnet(x7.mod,y7,alpha=1) bestlam &lt;- cv.out$lambda.min lambda.mod &lt;- glmnet(x.mod,y7,alpha=1) predict(lambda.mod,type=&quot;coefficients&quot;,s=bestlam) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -5.11224579 ## poly(x, degree = 10, raw = TRUE)1 17.43961036 ## poly(x, degree = 10, raw = TRUE)2 -7.78762588 ## poly(x, degree = 10, raw = TRUE)3 0.87266323 ## poly(x, degree = 10, raw = TRUE)4 -0.01318214 ## poly(x, degree = 10, raw = TRUE)5 . ## poly(x, degree = 10, raw = TRUE)6 . ## poly(x, degree = 10, raw = TRUE)7 . ## poly(x, degree = 10, raw = TRUE)8 . ## poly(x, degree = 10, raw = TRUE)9 . ## poly(x, degree = 10, raw = TRUE)10 . Once again, the regular linear regression out performs the lasso. Lasso is performing pretty terribly. Am I doing something wrong? data(College,package=&quot;ISLR&quot;) n&lt;-nrow(College) a. ```r set.seed(1) train &lt;- sample(1:n, n/2) test &lt;- (-train) #reference MSE if using just the mean y.test &lt;- College$Apps[test] y.train &lt;- College$Apps[train] (ref.err&lt;-mean((mean(y.test)-y.test)^2)) ``` ``` ## [1] 11205007 ``` b. ```r lm.mod &lt;- lm(Apps ~ ., data=College[train,]) lm.pred &lt;- predict(lm.mod, College[test,]) (lm.err&lt;-mean((lm.pred-y.test)^2)) ``` ``` ## [1] 1108531 ``` c. ```r set.seed(1) x.mod &lt;- model.matrix(Apps ~ ., data=College)[,-1] ridge.mod.cv&lt;-cv.glmnet(x.mod[train,],y.train,alpha=0) ridge.best.lam &lt;- ridge.mod.cv$lambda.min ridge.mod &lt;- glmnet(x.mod[train,],y.train,alpha=0) ridge.pred &lt;- predict(ridge.mod,newx=x.mod[test,],s=ridge.best.lam) (ridge.err&lt;-mean((ridge.pred-y.test)^2)) ``` ``` ## [1] 1037616 ``` d. ```r set.seed(1) x.mod &lt;- model.matrix(Apps ~ ., data=College)[,-1] lasso.mod.cv &lt;- cv.glmnet(x.mod[train,],y.train,alpha=1) lasso.best.lam &lt;- lasso.mod.cv$lambda.min lasso.mod &lt;- glmnet(x.mod[train,],y.train,alpha=1) lasso.pred &lt;- predict(lasso.mod,newx=x.mod[test,],s=lasso.best.lam) (lasso.err&lt;-mean((lasso.pred-y.test)^2)) ``` ``` ## [1] 1030941 ``` e. ```r library(pls) ``` ``` ## ## Attaching package: &#39;pls&#39; ``` ``` ## The following object is masked from &#39;package:caret&#39;: ## ## R2 ``` ``` ## The following object is masked from &#39;package:stats&#39;: ## ## loadings ``` ```r set.seed(1) pcr.mod &lt;- pcr(Apps ~ ., data=College, subset=train, scale=TRUE, validation=&quot;CV&quot;) validationplot(pcr.mod,val.type=&quot;MSEP&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-155-1.png&quot; width=&quot;672&quot; /&gt; ```r summary(pcr.mod) ``` ``` ## Data: X dimension: 388 17 ## Y dimension: 388 1 ## Fit method: svdpc ## Number of components considered: 17 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 4335 4179 2364 2374 1996 1844 1845 ## adjCV 4335 4182 2360 2374 1788 1831 1838 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 1850 1863 1809 1809 1812 1815 1825 ## adjCV 1844 1857 1801 1800 1804 1808 1817 ## 14 comps 15 comps 16 comps 17 comps ## CV 1810 1823 1273 1281 ## adjCV 1806 1789 1260 1268 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 31.216 57.68 64.73 70.55 76.33 81.30 85.01 ## Apps 6.976 71.47 71.58 83.32 83.44 83.45 83.46 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 88.40 91.16 93.36 95.38 96.94 97.96 98.76 ## Apps 83.47 84.53 84.86 84.98 84.98 84.99 85.24 ## 15 comps 16 comps 17 comps ## X 99.40 99.87 100.00 ## Apps 90.87 93.93 93.97 ``` ```r pcr.pred &lt;- predict(pcr.mod,College[test,],ncomp=16) (pcr.err&lt;-mean((pcr.pred-y.test)^2)) ``` ``` ## [1] 1166897 ``` f. For Partial Least Squares, a lot of the componenets have roughly the same CV after 4 components. ```r set.seed(1) pls.mod &lt;- plsr(Apps ~ ., data=College, subset=train, validation=&quot;CV&quot;) validationplot(pls.mod, val.type=&quot;MSEP&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-156-1.png&quot; width=&quot;672&quot; /&gt; ```r summary(pls.mod) ``` ``` ## Data: X dimension: 388 17 ## Y dimension: 388 1 ## Fit method: kernelpls ## Number of components considered: 17 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 4335 2123 1986 1792 1411 1375 1383 ## adjCV 4335 2115 1896 1767 1398 1366 1373 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 1390 1397 1314 1311 1300 1296 1281 ## adjCV 1379 1385 1321 1300 1289 1283 1269 ## 14 comps 15 comps 16 comps 17 comps ## CV 1281 1283 1283 1281 ## adjCV 1269 1271 1272 1268 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 42.26 51.47 90.73 97.48 98.65 99.02 99.27 ## Apps 78.91 87.10 88.93 91.89 91.99 92.05 92.12 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 99.97 99.99 100.00 100.00 100.00 100.00 100.00 ## Apps 92.13 92.30 93.08 93.38 93.73 93.74 93.75 ## 15 comps 16 comps 17 comps ## X 100.00 100.00 100.00 ## Apps 93.75 93.76 93.97 ``` ```r pls.pred &lt;- predict(pls.mod,College[test,],ncomp=4) (pls.err&lt;-mean((pls.pred-y.test)^2)) ``` ``` ## [1] 1232841 ``` g. The lasso and ridge performed better than the rest of the methods. We can see that the lasso did the best, whereas PLS did the worst. In addition, just a linear model outperformed both PCR and PLS. ```r rbind(ref.err,lm.err,ridge.err,lasso.err,pcr.err,pls.err) ``` ``` ## [,1] ## ref.err 11205007 ## lm.err 1108531 ## ridge.err 1037616 ## lasso.err 1030941 ## pcr.err 1166897 ## pls.err 1232841 ``` set.seed(1) X &lt;- rnorm(20*1000) dim(X) &lt;- c(1000,20) (B &lt;- sample(c(-2,-1,-.5,0,.5,1,2),size=20,replace=TRUE,prob=c(1,1,1,10,1,1,1))) ## [1] 0.0 0.0 0.0 0.0 0.0 -1.0 0.0 1.0 1.0 0.0 0.0 1.0 0.0 0.0 ## [15] 0.5 0.0 -2.0 0.0 0.5 -2.0 Y&lt;-X%*%B+rnorm(20) train &lt;- sample.int(1000,size=100) test &lt;- - train library(leaps) full.fit &lt;- regsubsets(Y~.,data=data.frame(Y,X)[train,],nvmax=20) X.mod &lt;- model.matrix(Y ~ X) val.errors &lt;- double(20) for(i in 1:20){ coefi&lt;-coef(full.fit,id=i) pred &lt;- X.mod[train,names(coefi)]%*%coefi val.errors[i]=mean((Y[train]-pred)^2) } plot(val.errors,type=&#39;l&#39;) points(val.errors,pch=4) abline(v=which.min(val.errors),col=&quot;red&quot;,lty=2) The minimum occurs at 8 variables, which is true to the original \\(\\beta\\). test.errs &lt;- double(20) for(i in 1:20){ coefi&lt;-coef(full.fit,id=i) pred &lt;- X.mod[test,names(coefi)]%*%coefi test.errs[i]=mean((Y[test]-pred)^2) } plot(test.errs,type=&#39;l&#39;) points(test.errs,pch=4) abline(v=which.min(test.errs),lty=2,col=&quot;red&quot;) The minimum value occurs at 8 variables, which is true to our model, sum(B==0)= 12. Comparing the coefficients: names(B) &lt;- colnames(X.mod)[2:21] True.B &lt;- B[B!=0] True.B &lt;- c(`(Intercept)`=0,True.B) Fitted &lt;- coef(full.fit,id=8) rbind(True.B, Fitted, Err=(True.B-Fitted)) ## (Intercept) X6 X8 X9 X12 X15 ## True.B 0.000000 -1.0000000 1.00000000 1.00000000 1.0000000 0.50000000 ## Fitted 0.260937 -0.7966302 0.97115844 1.03481958 1.1459732 0.42127106 ## Err -0.260937 -0.2033698 0.02884156 -0.03481958 -0.1459732 0.07872894 ## X17 X19 X20 ## True.B -2.00000000 0.5000000 -2.000000000 ## Fitted -2.04178071 0.3910316 -2.007931041 ## Err 0.04178071 0.1089684 0.007931041 norms &lt;- double(20) B2 &lt;- c(0,B) names(B2) &lt;- colnames(X.mod) for(i in 1:20){ coefi &lt;- coef(full.fit,id=i) temp&lt;-rep(0,21) names(temp)&lt;-colnames(X.mod) temp[names(coefi)] &lt;- coefi norms[i]=sqrt(sum((B2-temp)^2)) } plot(norms,xlab=&quot;r&quot;,ylab=&quot;norm&quot;,type=&quot;l&quot;) We can see that it looks pretty similar to the test MSE plot. The minimum of the plot occurs at 8, which is the same as the index of the test errors, 8. data(Boston,package=&quot;MASS&quot;) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 Boston.mat &lt;- model.matrix(crim ~ ., Boston) a. First, split up the set into a test set and a training set. ```r set.seed(1) train&lt;-sample.int(nrow(Boston),size=nrow(Boston)*.5) test&lt;--train y.train &lt;- Boston$crim[train] y.test &lt;- Boston$crim[test] ``` First linear models. I use best subsets to select the best train set using BIC, then get the MSE on the test set. ```r full.fit &lt;- regsubsets(crim ~ ., data=Boston[train,],nvmax=ncol(Boston)) lm.best.coef &lt;- coef(full.fit,id=which.min(summary(full.fit)$bic)) lm.pred &lt;- Boston.mat[test,names(lm.best.coef)]%*%lm.best.coef (lm.err &lt;- mean((lm.pred-y.test)^2)) ``` ``` ## [1] 39.46705 ``` ```r full.lm.mod &lt;- lm(crim ~ ., Boston, subset=train) lm.full.pred &lt;- predict(full.lm.mod, Boston[test,]) (lm.full.err &lt;- mean((lm.full.pred - y.test)^2)) ``` ``` ## [1] 39.27592 ``` Next, Ridge and Lasso using cross validation. ```r library(glmnet) runGlmNet &lt;- function(alpha) { set.seed(927) cv &lt;- cv.glmnet(Boston.mat[train,],y.train,alpha=alpha) best.lam &lt;- cv$lambda.min mod &lt;- glmnet(Boston.mat[train,],y.train,alpha=alpha) pred &lt;- predict(mod,newx=Boston.mat[test,],s=best.lam) err&lt;-mean((pred-y.test)^2) list(mod=mod,cv=cv,best.lam=best.lam,err=err) } #Ridge first ridge.mod&lt;-runGlmNet(0) ridge.mod$err ``` ``` ## [1] 38.36353 ``` ```r #Lasso lasso.mod&lt;-runGlmNet(1) lasso.mod$err ``` ``` ## [1] 38.34268 ``` Last, PCR. 9 components looks pretty good. ```r library(pls) set.seed(927) pcr.mod &lt;- pcr(crim~.,data=Boston,subset=train,scale=TRUE,validation=&quot;CV&quot;) summary(pcr.mod) ``` ``` ## Data: X dimension: 253 13 ## Y dimension: 253 1 ## Fit method: svdpc ## Number of components considered: 13 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 8.892 7.481 7.468 7.177 7.200 7.180 7.240 ## adjCV 8.892 7.476 7.463 7.171 7.185 7.172 7.227 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 7.236 7.214 7.165 7.124 7.158 7.211 7.079 ## adjCV 7.220 7.209 7.144 7.101 7.133 7.182 7.051 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 49.04 60.72 69.75 76.49 83.02 88.40 91.73 ## crim 30.39 30.93 36.63 37.31 37.35 37.98 38.85 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## X 93.77 95.73 97.36 98.62 99.57 100.00 ## crim 39.94 41.89 42.73 42.73 43.55 45.48 ``` ```r validationplot(pcr.mod,val.type = &quot;MSEP&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-169-1.png&quot; width=&quot;672&quot; /&gt; ```r pcr.pred &lt;- predict(pcr.mod,Boston[test,],ncomp=9) (pcr.err &lt;- mean((pcr.pred-y.test)^2)) ``` ``` ## [1] 39.52134 ``` Now let&#39;s summarize the errors: ```r lasso.err &lt;- lasso.mod$err ridge.err &lt;- ridge.mod$err rbind(lm.err, lm.full.err, ridge.err, lasso.err, pcr.err) ``` ``` ## [,1] ## lm.err 39.46705 ## lm.full.err 39.27592 ## ridge.err 38.36353 ## lasso.err 38.34268 ## pcr.err 39.52134 ``` It appears Lasso outperforms the rest of the models, but ridge is close behind. PCR doesn&#39;t do too well. Our best subsets didn&#39;t improve on the full lm either using the validation set approach, but they are close that it might not be significant. b. I&#39;ll just propose the Lasso model ```r coef(lasso.mod$mod, s=lasso.mod$best.lam) ``` ``` ## 15 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 6.581848709 ## (Intercept) . ## zn 0.036372942 ## indus -0.030858830 ## chas -0.509062651 ## nox -9.125346045 ## rm 1.156872387 ## age . ## dis -0.939767796 ## rad 0.513033837 ## tax . ## ptratio -0.204441060 ## black -0.002435222 ## lstat 0.175839323 ## medv -0.187702907 ``` c. We can see that Lasso only removed two coefficients. "],
["chap7.html", "Chapter 7 Moving Beyond Linearity", " Chapter 7 Moving Beyond Linearity Note that if we have \\(x \\leq \\xi\\), then \\((x- \\xi)^3_+= 0\\), so \\(f(x)\\) reduces to \\[\\begin{align} f(x) &amp;= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4(x-\\xi)^3_+ \\\\ &amp;= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\, . \\end{align}\\] This gives coefficients \\(a_1 = \\beta_0\\), \\(b_1 = \\beta_1\\), \\(c_1 = \\beta_2\\), \\(d_1 = \\beta_3\\). Remember that \\((x-\\xi)^3 = x^3 - 3x^2 \\xi + 3x \\xi^2 - \\xi^3\\). Then expanding \\((x-\\xi)^3_+\\) within \\(f(x)\\) gives \\[\\begin{align} f(x) &amp;= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4(x-\\xi)^3_+ \\nonumber \\\\ &amp;= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4(x^3 - 3x^2 \\xi + 3x \\xi^2 - \\xi^3) \\nonumber \\\\ &amp;= \\beta_0 - \\beta_4\\xi^3 + (\\beta_1+ 3 \\beta_4 \\xi^2)x + (\\beta_2 - 3 \\beta_4 \\xi)x^2 + (\\beta_3 + \\beta_4) x^3 \\, . \\tag{7.1} \\end{align}\\] Equating \\(f_2(x)\\) with (7.1), we get that \\(a_2 = \\beta_0 - \\beta_4 \\xi^3\\), \\(b_2 = \\beta_1 + 3 \\beta_4 \\xi^3\\), \\(c_2 = \\beta_2 - 3 \\beta_4 \\xi\\), and \\(d_4 = \\beta_3 + \\beta_4\\). Directly plugging into \\(f_1(x)\\), \\[\\begin{equation} f_1(\\xi) = \\beta_0 + \\beta_1 \\xi + \\beta_2 \\xi^2 + \\beta_3 \\xi^3\\, . \\end{equation}\\] Plugging into \\(f_2(x)\\), \\[\\begin{align} f_2(\\xi) &amp;= \\beta_0 - \\beta_4 \\xi^3 + (\\beta_1 + 3\\beta_4 \\xi^2)\\xi +(\\beta_2 - 3 \\beta_4 \\xi)\\xi^2 + (\\beta_3 + \\beta_4) \\xi^3 \\\\ &amp;= \\beta_0 - \\beta_4 \\xi^3 + \\beta_1\\xi + 3\\beta_4\\xi^3 + \\beta_2\\xi^2 - 3\\beta_4\\xi^3 + \\beta_3 + \\beta_4 \\xi^3 \\\\ &amp;= \\beta_0 - \\beta_1 \\xi + \\beta_2 \\xi^2 + \\beta_3\\xi^3\\, . \\end{align}\\] This gives us that \\(f_1(\\xi) = f_2(\\xi)\\). First note that \\[\\begin{align} f_1&#39;(x) &amp;= \\beta_1+2\\beta_2x + 3\\beta_3x^2 \\, ,\\\\ f_2&#39;(x) &amp;= \\beta_1 + 3\\beta_4\\xi^2 + 2(\\beta_2- 3\\beta_4\\xi)x + 3(\\beta_3+\\beta_4)x^2 \\, . \\end{align}\\] Plugging in \\(\\xi\\) into \\(f_1&#39;(x)\\), \\[\\begin{equation} f_1&#39;(\\xi) = \\beta_1+2\\beta_2 \\xi + 3\\beta_3 \\xi^2 \\end{equation}\\] Plugging \\(\\xi\\) into \\(f_2&#39;(x)\\), \\[\\begin{align} f_2&#39;(\\xi) &amp;= \\beta_1 + 3\\beta_4\\xi^2 + 2(\\beta_2- 3\\beta_4\\xi)\\xi + 3(\\beta_3+\\beta_4)\\xi^2\\\\ &amp;= \\beta_1 + 3 \\beta_4 \\xi^2 + 2\\beta_2 \\xi - 6\\beta_4 \\xi^2 + 3\\beta_3\\xi^2 + 3\\beta_4\\xi^2\\\\ &amp;= \\beta_1 + 2\\beta_2 \\xi + 3\\beta_3 \\xi^2 \\end{align}\\] which is \\(f_1&#39;(\\xi)\\). We have \\[\\begin{align} f_1&#39;&#39;(x) &amp;= 2\\beta_2 + 6\\beta_3 x \\, , \\\\ f_2&#39;&#39;(x) &amp;= 2\\beta_2 - 6\\beta_4 \\xi + (6\\beta_3 + 6\\beta_4) x \\, . \\end{align}\\] So, \\[\\begin{equation} f_1&#39;&#39;(\\xi) = 2\\beta_2 + 6\\beta_3 \\xi \\, . \\end{equation}\\] And plugging \\(\\xi\\) into \\(f_2&#39;&#39;(x)\\), we have \\[\\begin{align} f_2&#39;&#39;(\\xi) &amp;= 2\\beta_2 - 6\\beta_4 \\xi + (6\\beta_3 + 6\\beta_4) \\xi \\\\ &amp;= 2 \\beta_2 - 6\\beta_4 \\xi + 6 \\beta_3 \\xi + 6 \\beta_4 \\xi \\\\ &amp;= 2\\beta_2 + 6\\beta_3 \\xi \\, . \\end{align}\\] This shows that \\(f_1&#39;&#39;(\\xi) = f_2&#39;&#39;(\\xi)\\). I’m not interested in sketching as much as describing the class of solutions and which solution is best within each one. If \\(m=0\\), then we are saying then if \\(g(x)&gt;0\\) at any point (ignoring issues of measure), then \\(\\lambda \\int g(x)^2 dx \\rightarrow \\infty\\) as \\(\\lambda \\rightarrow \\infty\\), so the only solution of \\(\\hat{g}\\) is \\(\\hat{g}(x) = 0\\). This must be a constant function since \\(f&#39;(x) = 0\\) for any constant function \\(f\\). In this case, the solution that minimizes the error would be \\(\\hat{g}(x) = \\bar{y}\\). In this case, it is a linear function since \\(f&#39;&#39;(x) = 0\\) for any linear function. This reduces to the linear least squares solution. This would be a quadratic solution linear least squares fit. Since \\(\\lambda=0\\), we would probably be the same in the case of \\(\\lambda=0\\) and \\(m=2\\), so we would get an interpolating spline. x &lt;- seq(-2,2,.01) y &lt;- 1 + 1*x + (x-1)^2*(x&gt;=1) plot(x,y,type=&quot;l&quot;) Before we even plot, we can note that \\(b_2\\) actually has no effect for \\(x \\in [-2,2]\\). x &lt;- seq(-2, 2, .01) b1 &lt;- function(x) (x &gt;= 0 &amp; x&lt;=2)-(x-1)*(x&gt;=1 &amp; x&lt;=2) b2 &lt;- function(x) (x-3)*(x&gt;=3 &amp; x&lt;=4)+(x&gt;4 &amp; x&lt;=5) y &lt;- 1 + b1(x) + 3*b2(x) plot(x,y,type=&quot;l&quot;) As \\(\\lambda \\rightarrow \\infty\\), \\(\\hat{g}_4\\) will have the smaller training RSS. That is because it is allowed to have more variability. As \\(\\lambda \\rightarrow \\infty\\), we cannot know if \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) will have lower test RSS. We do not know what the true response is, so it is possible it is a very high degree polynomial, in which case \\(\\hat{g}_2\\) will fit it better. However, if the true response is a line, then \\(\\hat{g}_1\\) will have a better test RSS. For \\(\\lambda=0\\), the two models degenerate into the same model, so they will have the same training and test RSS. data(Wage,package=&quot;ISLR&quot;) head(Wage) ## year age maritl race education ## 231655 2006 18 1. Never Married 1. White 1. &lt; HS Grad ## 86582 2004 24 1. Never Married 1. White 4. College Grad ## 161300 2003 45 2. Married 1. White 3. Some College ## 155159 2003 43 2. Married 3. Asian 4. College Grad ## 11443 2005 50 4. Divorced 1. White 2. HS Grad ## 376662 2008 54 2. Married 1. White 4. College Grad ## region jobclass health health_ins ## 231655 2. Middle Atlantic 1. Industrial 1. &lt;=Good 2. No ## 86582 2. Middle Atlantic 2. Information 2. &gt;=Very Good 2. No ## 161300 2. Middle Atlantic 1. Industrial 1. &lt;=Good 1. Yes ## 155159 2. Middle Atlantic 2. Information 2. &gt;=Very Good 1. Yes ## 11443 2. Middle Atlantic 2. Information 1. &lt;=Good 1. Yes ## 376662 2. Middle Atlantic 2. Information 2. &gt;=Very Good 1. Yes ## logwage wage ## 231655 4.318063 75.04315 ## 86582 4.255273 70.47602 ## 161300 4.875061 130.98218 ## 155159 5.041393 154.68529 ## 11443 4.318063 75.04315 ## 376662 4.845098 127.11574 a. ```r library(boot) set.seed(1) cv.errors &lt;- double(10) for(i in 1:10) { glm.fit &lt;- glm(wage ~ poly(age,i), data=Wage) cv.errors[i] &lt;- cv.glm(Wage, glm.fit, K=10)$delta[1] } plot(cv.errors,type=&quot;l&quot;,xlab=&quot;degree&quot;,ylab=&quot;error&quot;) abline(v=which.min(cv.errors),col=&quot;red&quot;,lty=2) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-175-1.png&quot; width=&quot;672&quot; /&gt; A polynomial of degree 4 is the best fit to the data. ```r lm.mods&lt;-list() for(i in 1:10) { lm.fit &lt;- lm(wage ~ poly(age,i),data=Wage) lm.mods[[i]] &lt;- lm.fit } do.call(&quot;anova&quot;,lm.mods) ``` ``` ## Analysis of Variance Table ## ## Model 1: wage ~ poly(age, i) ## Model 2: wage ~ poly(age, i) ## Model 3: wage ~ poly(age, i) ## Model 4: wage ~ poly(age, i) ## Model 5: wage ~ poly(age, i) ## Model 6: wage ~ poly(age, i) ## Model 7: wage ~ poly(age, i) ## Model 8: wage ~ poly(age, i) ## Model 9: wage ~ poly(age, i) ## Model 10: wage ~ poly(age, i) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2998 5022216 ## 2 2997 4793430 1 228786 143.7638 &lt; 2.2e-16 *** ## 3 2996 4777674 1 15756 9.9005 0.001669 ** ## 4 2995 4771604 1 6070 3.8143 0.050909 . ## 5 2994 4770322 1 1283 0.8059 0.369398 ## 6 2993 4766389 1 3932 2.4709 0.116074 ## 7 2992 4763834 1 2555 1.6057 0.205199 ## 8 2991 4763707 1 127 0.0796 0.777865 ## 9 2990 4756703 1 7004 4.4014 0.035994 * ## 10 2989 4756701 1 3 0.0017 0.967529 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ``` From the analysis of variance, we can see that the stops being significant at the 4th degree polynomial. This agrees with the cross-validaiton results. A plot of the polynomial fit is below. ```r plot(Wage$age,Wage$wage,xlab=&quot;age&quot;,ylab=&quot;wage&quot;) lines(predict(lm.mods[[i]],list(age=seq(from=min(Wage$age),to=max(Wage$age),length.out=100))),col=&quot;red&quot;) legend(&quot;topright&quot;,c(&quot;data&quot;,&quot;fit&quot;),pch=c(1,NA),lty=c(NA,1),col=c(&quot;black&quot;,&quot;red&quot;)) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-177-1.png&quot; width=&quot;672&quot; /&gt; b. ```r set.seed(1) cv.errors &lt;- double(10) for(i in 1:10) { n_ints &lt;- i+2 k_folds &lt;- 10 folds &lt;- cut(seq(1,nrow(Wage)),breaks=k_folds,labels=FALSE) errors &lt;- double(k_folds) breaks &lt;- quantile(Wage$age,p=seq(0,1,by=1/n_ints)) Wage$age.cut &lt;- cut(Wage$age,breaks=n_ints) for(k in 1:k_folds){ train &lt;- folds != k glm.fit &lt;- lm(wage ~ age.cut, data=Wage, subset=train) y.pred &lt;- predict(glm.fit,Wage[!train,]) errors[k] &lt;- mean((y.pred-Wage$wage[!train])^2) } cv.errors[i] &lt;-mean(errors) } plot(3:12,cv.errors,type=&quot;l&quot;,xlab=&quot;number of intervals&quot;) abline(v=which.min(cv.errors)+2,col=&quot;red&quot;,lty=2) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-178-1.png&quot; width=&quot;672&quot; /&gt; A plot of the model is below. ```r plot(Wage$age,Wage$wage,xlab=&quot;age&quot;,ylab=&quot;wage&quot;) lm.fit &lt;- lm(wage ~ cut(age,breaks=8), data=Wage) age.grid &lt;- seq(min(Wage$age),max(Wage$age)) y.pred&lt;-predict(lm.fit,list(age=age.grid)) lines(age.grid,y.pred,col=&quot;red&quot;) legend(&quot;topright&quot;,c(&quot;data&quot;,&quot;fit&quot;),pch=c(1,NA),lty=c(NA,1),col=c(&quot;black&quot;,&quot;red&quot;)) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-179-1.png&quot; width=&quot;672&quot; /&gt; data(Auto,package=&quot;ISLR&quot;) head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 We will find the relationship between horsepower, weight, and mpgs. library(splines) set.seed(1) wgt.cv.errs &lt;- double(20) for(i in 1:20){ folds &lt;- cut(seq(1,nrow(Auto)),breaks=k_folds,labels=FALSE) errors &lt;- double(10) for(k in 1:10) { train &lt;- folds != k mod&lt;-lm(mpg ~ ns(weight,df=i),data=Auto,subset=train) y.pred&lt;-predict(mod,Auto[!train,]) errors[k]&lt;-mean((y.pred-Auto$mpg[!train])^2) } wgt.cv.errs[i] &lt;- mean(errors) } plot(1:length(wgt.cv.errs),wgt.cv.errs,xlab=&quot;DF&quot;,ylab=&quot;Cross validation error&quot;,type=&quot;l&quot;) abline(v=which.min(wgt.cv.errs),lty=2,col=&quot;red&quot;) Although 8 is the minimum, 2 appears just as good in addition to being simpler. mod &lt;- lm(mpg ~ ns(weight,df=2),data=Auto) wgt.grid = seq(min(Auto$weight),max(Auto$weight),by=1) mpg.pred&lt;-predict(mod,list(weight=wgt.grid)) plot(Auto$weight,Auto$mpg,xlab=&quot;weight&quot;,ylab=&quot;mpg&quot;) lines(wgt.grid,mpg.pred,col=&quot;red&quot;) library(splines) set.seed(1) fit &lt;- smooth.spline(Auto$horsepower,Auto$mpg,cv=TRUE) ## Warning in smooth.spline(Auto$horsepower, Auto$mpg, cv = TRUE): cross- ## validation with non-unique &#39;x&#39; values seems doubtful plot(fit,type=&quot;l&quot;,col=&quot;red&quot;) points(Auto$horsepower,Auto$mpg) Now try with a GAM, but use ANOVA to see if we are adding anything by using the smoothing spline with the natural spline. library(gam) ## Loaded gam 1.14-4 gam.m1 &lt;- gam(mpg ~ ns(weight,2),data=Auto) gam.m2 &lt;- gam(mpg ~ ns(weight,2) + s(horsepower,fit$lambda), data=Auto) anova(gam.m1,gam.m2) ## Analysis of Deviance Table ## ## Model 1: mpg ~ ns(weight, 2) ## Model 2: mpg ~ ns(weight, 2) + s(horsepower, fit$lambda) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 389 6781.1 ## 2 388 6244.6 1 536.52 7.754e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 data(Boston,package=&quot;MASS&quot;) head(Boston) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 ## 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 ## 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 ## 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 ## 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 ## 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## lstat medv ## 1 4.98 24.0 ## 2 9.14 21.6 ## 3 4.03 34.7 ## 4 2.94 33.4 ## 5 5.33 36.2 ## 6 5.21 28.7 a. ```r poly.fit &lt;- lm(nox ~ poly(dis,degree=3), data=Boston) summary(poly.fit) ``` ``` ## ## Call: ## lm(formula = nox ~ poly(dis, degree = 3), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.121130 -0.040619 -0.009738 0.023385 0.194904 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.554695 0.002759 201.021 &lt; 2e-16 *** ## poly(dis, degree = 3)1 -2.003096 0.062071 -32.271 &lt; 2e-16 *** ## poly(dis, degree = 3)2 0.856330 0.062071 13.796 &lt; 2e-16 *** ## poly(dis, degree = 3)3 -0.318049 0.062071 -5.124 4.27e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06207 on 502 degrees of freedom ## Multiple R-squared: 0.7148, Adjusted R-squared: 0.7131 ## F-statistic: 419.3 on 3 and 502 DF, p-value: &lt; 2.2e-16 ``` Plotting the resulting regression: ```r dis.grid &lt;- with(Boston, seq(min(dis), max(dis), by=(max(dis)-min(dis))/1000)) y.pred &lt;- predict(poly.fit,list(dis=dis.grid)) with(Boston, plot(dis,nox)) lines(dis.grid, y.pred, type=&quot;l&quot;, col=&quot;red&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-187-1.png&quot; width=&quot;672&quot; /&gt; b. Note that we expect the RSS to drop for every higher degree of polynomial here because we will fit to the model better with least squares. ```r rss &lt;- double(10) for(i in 1:10){ poly.fit &lt;- lm(nox ~ poly(dis,degree=i), data=Boston) poly.sum &lt;- summary(poly.fit) rss[i] &lt;- sum(poly.sum$residuals^2) } data.frame(degree=1:10, rss) ``` ``` ## degree rss ## 1 1 2.768563 ## 2 2 2.035262 ## 3 3 1.934107 ## 4 4 1.932981 ## 5 5 1.915290 ## 6 6 1.878257 ## 7 7 1.849484 ## 8 8 1.835630 ## 9 9 1.833331 ## 10 10 1.832171 ``` c. ```r set.seed(1) cv.errs &lt;- double(10) for(i in 1:10){ k.folds &lt;- 10 folds &lt;- rep(1:k_folds,length.out = nrow(Boston)) errors &lt;- double(10) for(k in 1:10) { train &lt;- folds != k mod&lt;-lm(nox ~ poly(dis,degree=i),data=Boston,subset=train) y.pred&lt;-predict(mod,Boston[!train,]) errors[k]&lt;-mean((y.pred-Boston$nox[!train])^2) } cv.errs[i] &lt;- mean(errors) } plot(1:10,cv.errs,xlab=&quot;degree&quot;,ylab=&quot;CV error&quot;,type=&quot;l&quot;) abline(v=which.min(cv.errs),col=&quot;red&quot;,lty=2) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-189-1.png&quot; width=&quot;672&quot; /&gt; The cross-validation suggests that a 3rd degree polynomial gives the best results, but the 2nd degree could also be used for more simplicity. d. ```r library(splines) bs.fit &lt;- lm(nox ~ bs(dis,df=4), data=Boston) y.pred &lt;- predict(bs.fit, list(dis=dis.grid)) with(Boston,plot(dis,nox)) lines(dis.grid, y.pred, type=&quot;l&quot;, col=&quot;red&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-190-1.png&quot; width=&quot;672&quot; /&gt; I let `bs` choose the knot for me. `bs` chooses the median in this case. e. We expect it to be lower again since we are fitting a more flexible model to the data. ```r dfs &lt;- c(3, 4, 5, 7, 10, 15, 25, 50) rss &lt;- double(length(dfs)) for(i in seq_along(dfs)){ df &lt;- dfs[i] bs.fit &lt;- lm(nox ~ bs(dis,df=df), data=Boston) y.pred &lt;- predict(bs.fit) rss[i] &lt;- sum((y.pred-Boston$nox)^2) } data.frame(df=dfs,rss) ``` ``` ## df rss ## 1 3 1.934107 ## 2 4 1.922775 ## 3 5 1.840173 ## 4 7 1.829884 ## 5 10 1.792535 ## 6 15 1.782798 ## 7 25 1.769957 ## 8 50 1.679047 ``` f. ```r set.seed(927) dfs &lt;- 3:50 cv.errs &lt;- double(length(dfs)) for(i in seq_along(dfs)) { df &lt;- dfs[i] k.folds &lt;- 10 folds &lt;- rep(1:k_folds,length.out = nrow(Boston)) errors &lt;- double(10) for(k in 1:10) { train &lt;- folds != k mod&lt;-lm(nox ~ bs(dis,df=df),data=Boston,subset=train) y.pred&lt;-predict(mod,Boston[!train,]) errors[k]&lt;-mean((y.pred-Boston$nox[!train])^2) } cv.errs[i] &lt;- mean(errors) } plot(dfs,cv.errs,type=&quot;l&quot;) abline(v=dfs[which.min(cv.errs)],lty=2,col=&quot;red&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-192-1.png&quot; width=&quot;672&quot; /&gt; The minimum occurs at 12. A plot of it is below. ```r best.bs.fit &lt;- lm(nox ~ bs(dis,df=dfs[which.min(cv.errs)]), data=Boston) y.pred &lt;- predict(best.bs.fit, list(dis=dis.grid)) with(Boston,plot(dis,nox)) lines(dis.grid,y.pred,col=&quot;red&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-193-1.png&quot; width=&quot;672&quot; /&gt; That kind of looks like overfitting. It appears that a df of 5 also gives comparable results while retaining much more accuracy. ```r bs.5.fit &lt;- lm(nox ~ bs(dis,df=5), data=Boston) y.pred &lt;- predict(bs.5.fit, list(dis=dis.grid)) with(Boston,plot(dis,nox)) lines(dis.grid,y.pred,col=&quot;red&quot;) ``` &lt;img src=&quot;ISLR-exercises_files/figure-html/unnamed-chunk-194-1.png&quot; width=&quot;672&quot; /&gt; Let’s visualize these relationships before choosing how we want to model these. library(gam) sig.vars &lt;- c(&quot;Private&quot;, &quot;Accept&quot;, &quot;Enroll&quot;, &quot;Room.Board&quot;,&quot;Terminal&quot;, &quot;perc.alumni&quot;, &quot;Expend&quot;, &quot;Grad.Rate&quot;) par(mfcol=c(3,3)) for( v in sig.vars){ plot(College[train,v],College[train,&quot;Outstate&quot;],xlab=v,ylab=&quot;Outstate&quot;) } Now build a gam. I really have no intuition for how to select which ones. I suppose there must be a lot of cross-validation, then comparing which cross-validation worked best for each technique. gam.mod &lt;- gam(Outstate ~ Private + s(Room.Board) + lo(Expend,span=.1) + lo(Accept,span=.1) + s(Terminal,df=10) + bs(Grad.Rate, df=4) + lo(Enroll,span=.1) + ns(perc.alumni,df=6), data=College, subset=train) ## Warning in general.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : general.wam convergence not obtained in 30 iterations ## Warning in general.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : general.wam convergence not obtained in 30 iterations ## Warning in general.wam(x, z, wz, fit$smooth, which, fit$smooth.frame, ## bf.maxit, : general.wam convergence not obtained in 30 iterations par(mfcol=c(3,3)) plot(gam.mod) gam.pred &lt;- predict(gam.mod, College[-train,]) ## Warning in gam.lo(data[[&quot;lo(Expend, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : eval 56233 ## Warning in gam.lo(data[[&quot;lo(Expend, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : upperlimit 45915 ## Warning in gam.lo(data[[&quot;lo(Expend, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : extrapolation not allowed with blending ## Warning in gam.lo(data[[&quot;lo(Accept, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : eval 26330 ## Warning in gam.lo(data[[&quot;lo(Accept, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : upperlimit 18837 ## Warning in gam.lo(data[[&quot;lo(Accept, span = 0.1)&quot;]], z, w, span = 0.1, ## degree = 1, : extrapolation not allowed with blending lm.mod &lt;- lm(as.formula(paste(c(&quot;Outstate ~ &quot;, sig.vars),&quot;+&quot;,TRUE)),data=College, subset=train) lm.pred &lt;- predict(lm.mod, College[-train,]) mean((gam.pred-College[-train,&quot;Outstate&quot;])^2) ## [1] 3679466 mean((lm.pred-College[-train,&quot;Outstate&quot;])^2) ## [1] 15574088 In either case, our model performs better than the linear model. Terminal, Grad.Rate, and Expend seem to be non-linear. set.seed(927) x1 &lt;- rnorm(100,20,5) x2 &lt;- rnorm(100,10,5) y &lt;- 3 + 2*x1 + x2 + rnorm(100) b1 &lt;- -1 a &lt;- y-b1*x1 b2 &lt;- lm(a~x2)$coef[2] a &lt;- y-b2*x2 b1 &lt;- lm(a~x1)$coef[2] It converges very quickly, so I only plot the first 10 or so. n&lt;-10 b0s &lt;- double(n) b1s &lt;- double(n) b2s &lt;- double(n) b0s[1] &lt;- mean(y - b1*x1 - b2*x2) b1s[1] &lt;- b1 b2s[1] &lt;- b2 for(i in 2:n){ a &lt;- y-b1*x1 b2 &lt;- lm(a~x2)$coef[2] a &lt;- y-b2*x2 b1 &lt;- lm(a~x1)$coef[2] b0 &lt;- mean(y - b1*x1 - b2*x2) b0s[i] &lt;- b0 b1s[i] &lt;- b1 b2s[i] &lt;- b2 } matplot(x=1:n,y=cbind(b0s,b1s,b2s),type=&quot;l&quot;,lty=c(2,2,2)) legend(x=&quot;topright&quot;,c(&quot;beta_0&quot;,&quot;beta_1&quot;,&quot;beta_2&quot;),lty=c(2,2,2),col=c(1,2,3)) y.lm &lt;- lm(y ~ x1 + x2) matplot(x=1:n,y=cbind(b0s,b1s,b2s),type=&quot;l&quot;,lty=c(2,2,2)) legend(x=&quot;topright&quot;,c(&quot;beta_0&quot;,&quot;beta_1&quot;,&quot;beta_2&quot;),lty=c(2,2,2),col=c(1,2,3)) abline(h=y.lm$coef[1],lty=1,col=&quot;black&quot;) abline(h=y.lm$coef[2],lty=1,col=&quot;red&quot;) abline(h=y.lm$coef[3],lty=1,col=&quot;green&quot;) p &lt;- 100 n&lt;- 1000 iters &lt;- 25 B &lt;- sample(-5:5,p,replace=TRUE) #true coefficients Bs &lt;- matrix(nrow=iters,ncol=p) #Store the coefficient estimates here for each iteration Bs[1,] &lt;- rep(-10,p) #First guess is -100 for each coefficient X &lt;- rnorm(p*n) dim(X) &lt;- c(n,p) y &lt;- X%*%B #add noise for (i in 1:(iters-1)){ for(j in 1:p){ a &lt;- y-X[,-j]%*%as.matrix(Bs[i,-j]) #remove all but the jth estimate Bs[i+1,j] &lt;- lm(a ~ X[,j])$coef[2] #fit the jth dimension on the data and store the result } } #plot the L2 errors for each iteration L2_errs&lt;-sqrt((rowSums(sweep(Bs,2,STATS=B)^2))) plot(1:iters,L2_errs, type=&quot;l&quot;) "],
["chap8.html", "Chapter 8 Moving Beyond Linearity", " Chapter 8 Moving Beyond Linearity Skipping. If we use \\(d=1\\) decision trees, then they will only split on one variable at a time. Let’s say that the data dimensions were ordered in terms of importance, that is we split \\(x=(x_1,\\dots,x_p)\\) first on \\(x_1\\), then on \\(x_2\\). Then the decision trees will only ever split along one of these variables. One the first step, we will get that function \\(\\hat{f}(x)=\\hat{f}^1(x_1)\\). Next, we split on the residuals, so we will split on \\(x_2\\), so \\(\\hat{f}(x) = \\hat{f}^1(x_1) + \\hat{f}^2(x)\\). We repeat this process possibly for all \\(p\\), so \\(\\hat{f}(x) = \\sum_{i=1}^p \\hat{f}^i(x_i)\\). Of course we expect to split variable dimensions multiple times, at least for some of them. Let’s say we split \\(x_1\\) twice, then we get some other function \\(\\hat{g}^1(x_1)\\). This is added so we get \\(\\hat{f}(x) = \\sum_{i=1}^p \\hat{f}^i(x_i)+ \\hat{g}(x_1)\\). We can then update \\(\\hat{f}^{1*}(x_1) = \\hat{f}^1(x_1) + \\hat{g}^1(x_1)\\). Then our tree is still additive. As a refresher, here are the equations \\[\\begin{align} E &amp;= 1 - \\max(\\hat{p}_{m1}, \\hat{p}_{m2}) = \\max(\\hat{p}_{m1}, 1-\\hat{p}_{m1}) &amp;\\text{Classification error} \\\\ G &amp;= \\sum_{k=1}^2 \\hat{p}_{mk}(1-\\hat{p}_{mk}) = 2\\hat{p}_{m1}(1-\\hat{p}_{m1}) &amp;\\text{Gini index} \\\\ D &amp;= - \\sum_{k=1}^2 \\hat{p}_{mk} \\log \\hat{p}_{mk} = \\hat{p}_{m1} \\log \\hat{p}_{m1} + (1 -\\hat{p}_{m1}) \\log (1\\hat{p}_{m1}) &amp;\\text{Entropy} \\end{align}\\] E &lt;- function(p) { 1-pmax(p,1-p) } G &lt;- function(p) { 2*p*(1-p) } D &lt;- function(p) { -(p*log(p) + (1-p)*log(1-p)) } p &lt;- seq(0,1,.01) matplot(p,cbind(E(p),G(p),D(p)),type=&quot;l&quot;) legend(x=&quot;topright&quot;,c(&quot;E&quot;,&quot;G&quot;,&quot;D&quot;),lty=1:3,col=1:3) I don’t know a good way to do this yet. I will put X1 on the x-axis and X2 on the y-axis. plot(x=NULL,y=NULL,xlim=c(-1,2),ylim=c(0,3),ylab=&quot;X2&quot;,xlab=&quot;X1&quot;) abline(h=1) #root, X2 &lt; 1 abline(h=2) #first level, second branch, X2 &lt; 2 text(x=.5,y=2.5,&quot;2.49&quot;) #this is the right most terminal node lines(x=c(0,0),y=c(1,2)) #third level bramch. text(x=-.5,y=1.5,&quot;-1.06&quot;) #left branch left terminal node text(x=1.25,y=1.5,&quot;0.21&quot;) #left branch right terminal mode lines(x=c(1,1),y=c(-2,1)) text(x=0,y=.5,&quot;-1.80&quot;) text(x=1.5,y=.5,&quot;0.63&quot;) I will assume that the chance of getting a red and green is the same, or that \\(P(\\text{green}) = 1 - P(\\text{red}) = .5\\). Since there are six trees that are voting red, we would say that the majority vote method would yield Red as the estimate. If we average the probabilities, we get 0.45, so we would vote Green. Pass for now. library(randomForest) set.seed(1) data(Boston,package=&quot;MASS&quot;) train &lt;- sample(1:nrow(Boston), nrow(Boston)/2) set.seed(927) mtries &lt;- 2:(ncol(Boston)-1) trees &lt;-seq(25,500,by=25) errors &lt;- matrix(nrow=length(trees),ncol=length(mtries)) boston.test=Boston[-train,&quot;medv&quot;] for (i in seq_along(mtries)) { mtry &lt;- mtries[i] for (j in seq_along(trees)){ ntrees &lt;- trees[j] bag.boston &lt;- randomForest(medv ~ .,data=Boston,subset=train,mtry=mtry,importance=TRUE,ntree=ntrees) yhat.bag &lt;- predict(bag.boston,newdata=Boston[-train,]) errors[j,i]&lt;-mean((yhat.bag-boston.test)^2) } } par(mar=c(5,4,4,5)+0.1)#Add more space to the right. Note that the default is c(5,4,4,2)+0.1 matplot(trees,errors,type=&quot;l&quot;,lty=1:length(mtries),col=1:length(mtries)) legend(x=525,y=16.4,legend=mtries,xpd=TRUE,lty=1:length(mtries),col=1:length(mtries)) The smallest is output by the code below. (best&lt;-arrayInd(which.min(errors),dim(errors))) ## [,1] [,2] ## [1,] 3 6 So it appears that \\(p=\\) 7 and 75 trees gives the best result. Averaging the error along the columns also gives the usual bias variance trade-off curve. The minimum occurs at \\(p=6.\\) avgErr &lt;- colMeans(errors) plot(mtries,avgErr, type=&quot;l&quot;,ylab=&quot;Average error&quot;,xlab=&quot;p&quot;) points(mtries[which.min(avgErr)],avgErr[which.min(avgErr)],col=&quot;red&quot;,pch=&quot;x&quot;) data(Carseats,package=&quot;ISLR&quot;) set.seed(927) train&lt;-sample.int(nrow(Carseats),size=nrow(Carseats)/2) library(tree) tree.mod &lt;- tree(Sales ~ ., data=Carseats, subset=train) plot(tree.mod) text(tree.mod,pretty=0) From the tree, we can see that shelf location is pretty important and results in generally lower sales. After that, we can see that price is second most important since it is the variable in both branches below the root. y.pred&lt;-predict(tree.mod, Carseats[-train,]) y.test&lt;-Carseats[-train,]$Sales mean((y.pred-y.test)^2) ## [1] 5.155457 set.seed(927) tree.mod.cv &lt;- cv.tree(tree.mod) tree.mod.cv ## $size ## [1] 16 15 14 12 11 10 9 8 7 6 5 4 3 2 1 ## ## $dev ## [1] 822.3025 838.0288 833.1498 805.5810 792.3865 820.8750 852.7767 ## [8] 904.5314 912.5343 926.3326 1090.8848 1086.3376 1106.7379 1100.6977 ## [15] 1479.5594 ## ## $k ## [1] -Inf 14.98722 16.47062 18.21130 18.46919 22.60747 25.26386 ## [8] 31.85974 36.16413 46.26508 85.18177 88.45656 106.21737 147.19886 ## [15] 387.01956 ## ## $method ## [1] &quot;deviance&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; plot(tree.mod.cv) The size with the smallest deviance is 11. Let’s see if it improves the test MSE. best.size &lt;- tree.mod.cv$size[which.min(tree.mod.cv$dev)] prune.mod &lt;- prune.tree(tree.mod,best=best.size) y.pred&lt;-predict(prune.mod, Carseats[-train,]) mean((y.pred-y.test)^2) ## [1] 5.034429 It improved the test MSE somewhat, but not considerably. I’m going to plot the test error as a function of the size. errors &lt;- double(length(tree.mod.cv$size[-1])) for(i in seq_along(tree.mod.cv$size[-1])){ size &lt;- tree.mod.cv$size[i] prune.mod &lt;- prune.tree(tree.mod,best=size) y.pred&lt;-predict(prune.mod, Carseats[-train,]) errors[i] &lt;- mean((y.pred-y.test)^2) } plot(tree.mod.cv$size[-1],errors,type=&quot;b&quot;) library(randomForest) set.seed(927) bag.mod &lt;- randomForest(Sales ~ ., data=Carseats, subset=train, mtry=(ncol(Carseats)-1), importance=TRUE) y.pred&lt;-predict(bag.mod,Carseats[-train,]) mean((y.pred-y.test)^2) ## [1] 2.955195 The bagging method improves upon the previous methods dramatically. Here is the importace. importance(bag.mod) ## %IncMSE IncNodePurity ## CompPrice 26.3089451 162.159898 ## Income 7.1203118 83.821531 ## Advertising 11.2133318 84.206868 ## Population 1.7575225 66.737701 ## Price 49.6763043 393.586030 ## ShelveLoc 62.6657938 470.259185 ## Age 12.7540444 107.060198 ## Education 1.1755872 37.428097 ## Urban -2.3668327 7.366765 ## US 0.5030577 5.677343 As before, it indicates that price and shelf location are most important. mtry&lt;-1:10 errors &lt;- double(length(mtry)) set.seed(927) for(i in seq_along(mtry)){ m &lt;- mtry[i] rf.mod &lt;- randomForest(Sales ~ ., data=Carseats, subset=train, mtry=m) y.pred&lt;-predict(rf.mod,Carseats[-train,]) errors[i]&lt;-mean((y.pred-y.test)^2) } plot(mtry,errors,type=&quot;b&quot;,pch=&quot;x&quot;) points(mtry[which.min(errors)],min(errors),col=&quot;red&quot;,pch=&quot;x&quot;) The effect start to flatten out after 6 variables. The average of the error for \\(m=6, \\dots 10\\) is 2.9627049. The results are pretty similar to bagging. data(OJ,package=&quot;ISLR&quot;) set.seed(927) train&lt;-sample.int(nrow(OJ),size=800) First fit the model, library(tree) tree.mod &lt;- tree(Purchase ~ ., data=OJ, subset=train) y.test &lt;- OJ[-train,&quot;Purchase&quot;] summary(tree.mod) ## ## Classification tree: ## tree(formula = Purchase ~ ., data = OJ, subset = train) ## Variables actually used in tree construction: ## [1] &quot;LoyalCH&quot; &quot;DiscMM&quot; &quot;PriceDiff&quot; &quot;ListPriceDiff&quot; ## Number of terminal nodes: 7 ## Residual mean deviance: 0.7698 = 610.5 / 793 ## Misclassification error rate: 0.1638 = 131 / 800 There is seven terminal nodes. The train misclassification rate is below. y.pred &lt;- predict(tree.mod, OJ[train,], type=&quot;class&quot;) y.train&lt;-OJ[train,&quot;Purchase&quot;] (unpruned.err &lt;- mean(y.pred != y.train)) ## [1] 0.16375 tree.mod ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 800 1071.000 CH ( 0.60875 0.39125 ) ## 2) LoyalCH &lt; 0.48285 297 324.400 MM ( 0.23569 0.76431 ) ## 4) LoyalCH &lt; 0.0356415 54 9.959 MM ( 0.01852 0.98148 ) * ## 5) LoyalCH &gt; 0.0356415 243 290.000 MM ( 0.28395 0.71605 ) ## 10) DiscMM &lt; 0.47 218 272.200 MM ( 0.31651 0.68349 ) * ## 11) DiscMM &gt; 0.47 25 0.000 MM ( 0.00000 1.00000 ) * ## 3) LoyalCH &gt; 0.48285 503 460.200 CH ( 0.82903 0.17097 ) ## 6) LoyalCH &lt; 0.764572 238 295.000 CH ( 0.68908 0.31092 ) ## 12) PriceDiff &lt; -0.165 36 35.470 MM ( 0.19444 0.80556 ) * ## 13) PriceDiff &gt; -0.165 202 214.300 CH ( 0.77723 0.22277 ) ## 26) ListPriceDiff &lt; 0.135 31 42.680 MM ( 0.45161 0.54839 ) * ## 27) ListPriceDiff &gt; 0.135 171 152.500 CH ( 0.83626 0.16374 ) * ## 7) LoyalCH &gt; 0.764572 265 97.720 CH ( 0.95472 0.04528 ) * Looking at terminal 27 and 26, we can see that the list price difference matters at that point in the true. plot(tree.mod) text(tree.mod,pretty=0) We can see that the root node, loyalty to the branch is very important, and is the deciding factor at the top 3 splits. However, even if loyalty is high (by going right, then left, and then right down the tree), list price still can be a deciding factor. Looking at the table, we can see that y.pred &lt;- predict(tree.mod, newdata=OJ[-train,],type=&quot;class&quot;) table(y.pred,y.test) ## y.test ## y.pred CH MM ## CH 129 13 ## MM 37 91 unpruned.test.err &lt;- mean(y.pred != y.test) The test error rate is mean(y.pred != y.test) = 0.1851852. and g. set.seed(927) tree.mod.cv &lt;- cv.tree(tree.mod,FUN=prune.misclass) with(tree.mod.cv,plot(size,dev,type=&quot;b&quot;)) The size with the lowest CV misclassification rate is 4. best&lt;-with(tree.mod.cv,size[which.min(dev)]) tree.mod.prune &lt;- prune.tree(tree.mod,best=best) plot(tree.mod.prune) text(tree.mod.prune,pretty=0) y.pred.prune &lt;- predict(tree.mod.prune,OJ[train,],type=&quot;class&quot;) pruned.err &lt;- mean(y.pred.prune != y.train) cbind(unpruned.err, pruned.err) ## unpruned.err pruned.err ## [1,] 0.16375 0.1675 The pruned error for the training set is slightly larger. y.pred.prune &lt;- predict(tree.mod.prune,OJ[train,],type=&quot;class&quot;) pruned.test.err &lt;- mean(y.pred.prune != y.train) cbind(unpruned.test.err, pruned.test.err) ## unpruned.test.err pruned.test.err ## [1,] 0.1851852 0.1675 The pruned test error is much better than the unpruned. data(Hitters,package=&quot;ISLR&quot;) Hitters &lt;- na.omit(Hitters) Hitters$Salary &lt;- log(Hitters$Salary) train&lt;-1:200 The plot below suggests that there is some overfitting going on considering that the training error just keeps going down. library(gbm) ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## aml ## The following object is masked from &#39;package:caret&#39;: ## ## cluster ## Loading required package: parallel ## Loaded gbm 2.1.3 set.seed(927) shrinkages &lt;- 10^(seq(-4,0,by=.1)) errors &lt;- double(length(shrinkages)) for(i in seq_along(shrinkages)) { s&lt;-shrinkages[i] boost.mod &lt;- gbm(Salary ~ .,data=Hitters[train,],distribution=&quot;gaussian&quot;,n.trees=1000,shrinkage=s) y.pred &lt;- predict(boost.mod,n.trees=1000) errors[i] &lt;- mean((y.pred-Hitters[train,]$Salary)^2) } plot(shrinkages,errors,type=&quot;l&quot;,log=&quot;x&quot;) It appears that the test error seems to bottom out at just below .01. set.seed(927) shrinkages &lt;- 10^(seq(-4,0,by=.1)) test.errs &lt;- double(length(shrinkages)) for(i in seq_along(shrinkages)) { s &lt;- shrinkages[i] boost.mod &lt;- gbm(Salary ~ .,data=Hitters[train,],distribution=&quot;gaussian&quot;,n.trees=1000,shrinkage=s) y.pred &lt;- predict(boost.mod,Hitters[-train,],n.trees=1000) test.errs[i] &lt;- mean((y.pred-Hitters[-train,]$Salary)^2) } plot(shrinkages,test.errs,type=&quot;l&quot;,log=&quot;x&quot;) best.shrink &lt;- shrinkages[which.min(test.errs)] First, a simple linear regression y.test &lt;- Hitters[-train,&quot;Salary&quot;] lm.mod &lt;- lm(Salary ~ ., data=Hitters, subset=train) lm.pred &lt;- predict(lm.mod, Hitters[-train,]) (lm.err &lt;- mean((lm.pred-y.test)^2)) ## [1] 0.4917959 Now for the Lasso. library(glmnet) x&lt;-model.matrix(Salary~.,Hitters,subset=train)[,-1] y&lt;-Hitters$Salary lasso.cv &lt;- cv.glmnet(x[train,],y[train],alpha=1) bestlam &lt;- lasso.cv$lambda.min lasso.mod &lt;- glmnet(x[train,],y[train],alpha=1) lasso.pred &lt;- predict(lasso.mod,s=bestlam,newx=x[-train,]) (lasso.err &lt;- mean((lasso.pred-y.test)^2)) ## [1] 0.4709719 Comparing the results. boost.mod &lt;- gbm(Salary ~ .,data=Hitters[train,],distribution=&quot;gaussian&quot;,n.trees=1000,shrinkage=best.shrink) y.pred &lt;- predict(boost.mod,Hitters[-train,],n.trees=1000) boost.err &lt;- mean((y.pred-Hitters[-train,]$Salary)^2) rbind(lm.err,lasso.err,boost.err) ## [,1] ## lm.err 0.4917959 ## lasso.err 0.4709719 ## boost.err 0.2883988 We can see that the boosting outperformed that linear regression and lasso considerably. summary(boost.mod,plotit=FALSE) ## var rel.inf ## CAtBat CAtBat 17.5891423 ## CRBI CRBI 8.9092457 ## Years Years 7.8985139 ## CRuns CRuns 7.5641642 ## PutOuts PutOuts 6.8435706 ## CHits CHits 6.8225708 ## CHmRun CHmRun 6.4953350 ## Walks Walks 6.2231730 ## CWalks CWalks 5.9689286 ## Hits Hits 5.2046670 ## Assists Assists 3.8874446 ## HmRun HmRun 3.6736184 ## RBI RBI 3.3908016 ## AtBat AtBat 3.0983749 ## Errors Errors 2.7066416 ## Runs Runs 2.2884151 ## Division Division 0.7817312 ## NewLeague NewLeague 0.5083451 ## League League 0.1453164 At bats is the most important. library(randomForest) bag.mod &lt;- randomForest(Salary ~ ., data=Hitters, subset=train,mtry=ncol(Hitters)-1) bag.pred&lt;-predict(bag.mod,Hitters[-train,]) (bag.err &lt;- mean((bag.pred-y.test)^2)) ## [1] 0.2246912 Bagging performs the best. data(Caravan,package=&quot;ISLR&quot;) train&lt;-1:1000 The most important variables are listed below. library(randomForest) set.seed(927) bag.mod &lt;- randomForest(Purchase ~ ., data=Caravan,subset=train,mtry=ncol(Caravan)-1,shrinkage=0.01) imp.vec&lt;-importance(bag.mod) head(imp.vec[order(imp.vec,decreasing=TRUE),]) ## MOSTYPE PPERSAUT MGODGE MOPLHOOG MGODPR PBRAND ## 4.365011 4.149740 4.115921 3.546324 3.236479 3.016892 MOSTYPE is customer subtype. MGODGE is no religion (proportion?). PPERSAUT is contribution car policies. MGODPR is protestant. MOPLHOOG is high level education. MKOOPKLA is purchasing power class. The rest of the variable definitions can be found at http://www.liacs.nl/~putten/library/cc2000/data.html. y.test &lt;- Caravan[-train,]$Purchase y.pred &lt;- ifelse(predict(bag.mod,Caravan[-train,],type=&quot;prob&quot;)[,&quot;Yes&quot;]&gt;.2,&quot;Yes&quot;,&quot;No&quot;) table(y.pred,y.test) ## y.test ## y.pred No Yes ## No 4120 231 ## Yes 413 58 The proportion of people predicted to make a purchase that do make a purchase is 0.1231423. Logistic regression is below. log.mod &lt;- glm(Purchase ~ ., family=binomial, data=Caravan,subset=train) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred log.pred &lt;- ifelse(predict(log.mod, Caravan[-train,], type=&quot;response&quot;) &gt; .2, &quot;Yes&quot;, &quot;No&quot;) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ## ifelse(type == : prediction from a rank-deficient fit may be misleading table(log.pred,y.test) ## y.test ## log.pred No Yes ## No 4183 231 ## Yes 350 58 The porportion for logistic regression is 0.1421569 so it performs a little better. Let’s try KNN. library(class) train.X &lt;- Caravan[train,-ncol(Caravan)] train.Y &lt;- Caravan[train,ncol(Caravan)] test.X &lt;- Caravan[-train,-ncol(Caravan)] knn.pred &lt;- knn(train.X,test.X,train.Y,k=2) table(knn.pred, y.test) ## y.test ## knn.pred No Yes ## No 4276 267 ## Yes 257 22 "],
["support-vector-machines.html", "Chapter 9 Support Vector Machines", " Chapter 9 Support Vector Machines x1&lt;-seq(-2,3,by=.01) x2 &lt;- 1+3*x1 plot(x1,x2,type=&quot;l&quot;,xlim=c(-1,2),ylim=c(-2,8)) polygon(x=c(-2,-2,3,3),y=c(x2[1],10,10,x2[length(x2)]),density=NA,col=rgb(1,0,0,.5)) text(x=.5,y=6,&quot;1 + 3 x1 - x2 &lt; 0&quot;) text(x=.5,y=-1,&quot;1 +3 x1 - x2 &gt; 0&quot;) x2 &lt;- 1 - .5*x1 plot(x1,x2,type=&quot;l&quot;,xlim=c(-1,2),ylim=c(-1,2)) polygon(x=c(-2,-2,3,3),y=c(x2[1],10,10,x2[length(x2)]),col=rgb(1,0,0,.5)) text(x=.5,y=1.5,&quot;-2 + x1 + 2 x2 &gt; 0&quot;) text(x=.5,y=0,&quot;-2 + x1 + 2 x2 &lt; 0&quot;) This is the equation of a circle centered at (-1,2) with radius 2. par(pty=&quot;s&quot;) #set plot to be square so we don&#39;t distort the circle. plot(x=c(-4,2),y=c(-1,5),type=&quot;n&quot;,xlab=&quot;x1&quot;,ylab=&quot;x2&quot;) x&lt;-seq(-3,1,by=.01) y &lt;- sqrt(4-(x+1)^2) xrev &lt;- x[length(x):1] yrev &lt;- y[length(y):1] polygon(c(x,xrev),c(y+2,-yrev+2)) The points \\((1+X_1)^2 + (2-X_2)^2 \\leq 4\\) are inside the circle. xrev &lt;- x[length(x):1] yrev &lt;- y[length(y):1] par(pty=&quot;s&quot;) #set plot to be square so we don&#39;t distort the circle. plot(x=c(-4,2),y=c(-1,5),type=&quot;n&quot;,xlab=&quot;x1&quot;,ylab=&quot;x2&quot;) rect(-5,-5,10,10,col=rgb(0,0,1,.5)) #Outside the circle is blue for part c polygon(c(x,xrev),c(y+2,-yrev+2),col=rgb(1,0,0,.5)) text(x=-1,y=2,&quot;(1+X_1)^2+(2-X_2)^2&lt;=4&quot;) \\((0,0)\\) is blue, \\((-1,1)\\) is red, \\((2,2)\\) is blue, \\((3,8)\\) is blue. If we expand the quadratic terms, we can see this, \\[\\begin{equation} (1+X_1)^2 + (2-X_2)^2 = 1 + 2X_1 + X_1^2 + 2 - 4X_2 + X_2^2 \\end{equation}\\] dat&lt;-data.frame( x1=c(3,2,4,1,2,4,4), x2=c(4,2,4,4,1,3,1), y =rep(c(&quot;red&quot;,&quot;blue&quot;),length.out=7,each=4) ) plot(dat$x1,dat$x2,col=as.character(dat$y)) The optimal hyperplane goes through (2,1.5) and (4,3.5). Using linear equations then the equation for the hyperplane becomes \\(-0.5+X_1-X_2=0\\). plot(dat$x1,dat$x2,col=as.character(dat$y)) abline(-0.5,1) We would classify as red if \\(-0.5+X_1-X_2&lt;0\\) and blue otherwise. The values the \\(\\beta\\)’s are \\(\\beta_0=-0.5\\), \\(\\beta_1=1\\), and \\(\\beta_2=-1\\). plot(dat$x1,dat$x2,col=as.character(dat$y)) abline(-0.5,1) lines(c(2,2),c(1,2),lty=2) lines(c(4,4),c(3,4),lty=2) text(3.9,3.6,&quot;M=1&quot;) See the last plot. The seventh obsevation is in the bottom right so it’s far from the hyperplane. We can have a line go through the bottom most red observation and the top most blue observation. This is a worst case hyperplane. The equation of this would be \\(1 + .5X_1 - X_2 = 0\\) plot(dat$x1,dat$x2,col=as.character(dat$y)) abline(1,.5) dat2 &lt;- rbind(dat,c(2,2.5,&quot;blue&quot;)) with(dat2,plot(x1,x2,col=as.character(y))) I use a cubic polynomial to separate the data. x&lt;-sort(rnorm(100,1,2)) y&lt;-rnorm(100) f &lt;- function(x) 1.5*(x)*(x-1)*(x-3) labs&lt;-factor(ifelse(y&gt;f(x),&quot;black&quot;,&quot;red&quot;)) plot(x,y,col=labs) lines(x,f(x)) Now to classify with SVM. library(e1071) set.seed(1) train&lt;-sample.int(100,80) dat &lt;- data.frame(x,labs) (poly.tune&lt;-tune(svm,labs~.,data=dat[train,],kernel=&quot;polynomial&quot;,ranges=list(d=1:5,cost=c(.001,.01,.1,1,10)))) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## d cost ## 3 1 ## ## - best performance: 0.175 set.seed(1) (exp.tune &lt;-tune(svm,labs~.,data=dat[train,],kernel=&quot;radial&quot;,ranges=list(gamma=c(.01,.05,.1,.5,1,5),cost=c(.001,.01,.1,10)))) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## gamma cost ## 1 10 ## ## - best performance: 0.15 Now we test the best poly versus the best radial on the test set. poly.mod &lt;- svm(labs~.,data=dat[train,],kernel=&quot;polynomial&quot;,d=poly.tune$best.parameters$d,cost=poly.tune$best.parameters$cost) exp.mod &lt;- svm(labs~.,data=dat[train,],kernel=&quot;radial&quot;,gamma=exp.tune$best.parameters$gamma,cost=exp.tune$best.parameters$cost) library(ROCR) ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess ## Loading required package: methods rocplot=function(pred, truth, ...){ predob = prediction(pred, truth) perf = performance(predob , &quot;tpr&quot;, &quot;fpr&quot;) plot(perf,...) } fitted=attributes(predict(poly.mod,dat[-train,],decision.values=T))$decision.values rocplot(-fitted,dat[-train,&quot;labs&quot;],main=&quot;Test Data&quot;) #for some reason, I have to make the #fitted values negative for the ROC curve to display correctly. fitted=attributes(predict(exp.mod,dat[-train,],decision.values=T))$decision.values rocplot(-fitted,dat[-train,&quot;labs&quot;],add=T,col=&quot;red&quot;) The ROC curve tells us that the radial kernel works best. The tables below show how the final models classified. They ended up performing just as well as each other on the data. poly.fit=predict(poly.mod,dat[-train,]) table(poly.fit,dat[-train,]$labs) ## ## poly.fit black red ## black 9 2 ## red 1 8 exp.fit=predict(poly.mod,dat[-train,]) table(exp.fit,dat[-train,]$labs) ## ## exp.fit black red ## black 9 2 ## red 1 8 I will jus copy their example. set.seed(92) x1&lt;-runif(500)-0.5 x2&lt;-runif(500)-0.5 y&lt;-1*(x1^2-x2^2 &gt; 0) plot(x1,x2,col=y+1) log.mod &lt;- glm(y ~ x1+x2,family=binomial) y.pred&lt;-predict(log.mod,type=&quot;response&quot;) y.pred&lt;-1*(y.pred&gt;.5) plot(x1,x2,col=y.pred+1) Note that a single value was predicted to be 0. any(predict(log.mod,type=&quot;response&quot;)&lt;.5) ## [1] TRUE log.nl.mod &lt;- glm(y ~ poly(x1,d=2)*poly(x2,d=2),family=binomial) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred y.pred&lt;-predict(log.nl.mod,type=&quot;response&quot;) y.pred&lt;-1*(y.pred&gt;.5) plot(x1,x2,col=y.pred+1) Looks better, but there is still misclassification. "]
]
