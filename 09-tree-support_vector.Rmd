---
output:
  pdf_document: default
  html_document: default
---


# Support Vector Machines

1.
    a.
    ```{r echo=TRUE}
    x1<-seq(-2,3,by=.01)
    x2 <- 1+3*x1
    plot(x1,x2,type="l",xlim=c(-1,2),ylim=c(-2,8))
    polygon(x=c(-2,-2,3,3),y=c(x2[1],10,10,x2[length(x2)]),density=NA,col=rgb(1,0,0,.5))
    text(x=.5,y=6,"1 + 3 x1 - x2 < 0")
    text(x=.5,y=-1,"1 +3 x1 - x2 > 0")
    ```
    b.
    ```{r echo=TRUE}
    x2 <- 1 - .5*x1
    plot(x1,x2,type="l",xlim=c(-1,2),ylim=c(-1,2))
    polygon(x=c(-2,-2,3,3),y=c(x2[1],10,10,x2[length(x2)]),col=rgb(1,0,0,.5))
    text(x=.5,y=1.5,"-2 + x1 + 2 x2 > 0")
    text(x=.5,y=0,"-2 + x1 + 2 x2 < 0")
    ```
    
2.
    a. This is the equation of a circle centered at (-1,2) with radius 2.
    ```{r echo=TRUE}
    par(pty="s") #set plot to be square so we don't distort the circle.
    plot(x=c(-4,2),y=c(-1,5),type="n",xlab="x1",ylab="x2")
    x<-seq(-3,1,by=.01)
    y <- sqrt(4-(x+1)^2)
    xrev <- x[length(x):1]
    yrev <- y[length(y):1]
    polygon(c(x,xrev),c(y+2,-yrev+2))
    ```
    b. The points $(1+X_1)^2 + (2-X_2)^2 \leq 4$ are inside the circle.
    ```{r echo=TRUE}
    xrev <- x[length(x):1]
    yrev <- y[length(y):1]
    par(pty="s") #set plot to be square so we don't distort the circle.
    plot(x=c(-4,2),y=c(-1,5),type="n",xlab="x1",ylab="x2")
    rect(-5,-5,10,10,col=rgb(0,0,1,.5)) #Outside the circle is blue for part c
    polygon(c(x,xrev),c(y+2,-yrev+2),col=rgb(1,0,0,.5))
    text(x=-1,y=2,"(1+X_1)^2+(2-X_2)^2<=4")
    ```
    c. $(0,0)$ is blue, $(-1,1)$ is red, $(2,2)$ is blue, $(3,8)$ is blue.
    d. If we expand the quadratic terms, we can see this,
    \begin{equation}
    (1+X_1)^2 + (2-X_2)^2 = 1 + 2X_1 + X_1^2 + 2 - 4X_2 + X_2^2
    \end{equation}
    
3.
    a.
    ```{r echo=TRUE}
    dat<-data.frame(
      x1=c(3,2,4,1,2,4,4),
      x2=c(4,2,4,4,1,3,1),
      y =rep(c("red","blue"),length.out=7,each=4)
    )
    plot(dat$x1,dat$x2,col=as.character(dat$y))
    ```
    b. The optimal hyperplane goes through (2,1.5) and (4,3.5). Using linear equations then the equation for the hyperplane becomes $-0.5+X_1-X_2=0$.
    ```{r echo=TRUE}
    plot(dat$x1,dat$x2,col=as.character(dat$y))
    abline(-0.5,1)
    ```
    c. We would classify as red if $-0.5+X_1-X_2<0$ and blue otherwise. The values the $\beta$'s are $\beta_0=-0.5$, $\beta_1=1$, and $\beta_2=-1$.
    d.
    ```{r echo=TRUE}
    plot(dat$x1,dat$x2,col=as.character(dat$y))
    abline(-0.5,1)
    lines(c(2,2),c(1,2),lty=2)
    lines(c(4,4),c(3,4),lty=2)
    text(3.9,3.6,"M=1")
    ```
    e. See the last plot.
    f. The seventh obsevation is in the bottom right so it's far from the hyperplane.
    g. We can have a line go through the bottom most red observation and the top most blue observation. This is a worst case hyperplane. The equation of this would be $1 + .5X_1 - X_2 = 0$
    ```{r echo=TRUE}
    plot(dat$x1,dat$x2,col=as.character(dat$y))
    abline(1,.5)
    ```
    h.
    ```{r echo=TRUE}
    dat2 <- rbind(dat,c(2,2.5,"blue"))
    with(dat2,plot(x1,x2,col=as.character(y)))
    ```
    
4. I use a cubic polynomial to separate the data.
    ```{r echo=TRUE}
    x<-sort(rnorm(100,1,2))
    y<-rnorm(100)
    f <- function(x) 1.5*(x)*(x-1)*(x-3)
    labs<-factor(ifelse(y>f(x),"black","red"))
    plot(x,y,col=labs)
    lines(x,f(x))
    ```
    Now to classify with SVM.
    ```{r echo=TRUE}
    library(e1071)
    set.seed(1)
    train<-sample.int(100,80)
    dat <- data.frame(x,labs)
    (poly.tune<-tune(svm,labs~.,data=dat[train,],kernel="polynomial",ranges=list(d=1:5,cost=c(.001,.01,.1,1,10))))
    ```
    ```{r echo=TRUE}
    set.seed(1)
    (exp.tune <-tune(svm,labs~.,data=dat[train,],kernel="radial",ranges=list(gamma=c(.01,.05,.1,.5,1,5),cost=c(.001,.01,.1,10))))
    ```
    Now we test the best poly versus the best radial on the test set.
    ```{r echo=TRUE}
    poly.mod <- svm(labs~.,data=dat[train,],kernel="polynomial",d=poly.tune$best.parameters$d,cost=poly.tune$best.parameters$cost)
    exp.mod <- svm(labs~.,data=dat[train,],kernel="radial",gamma=exp.tune$best.parameters$gamma,cost=exp.tune$best.parameters$cost)
    library(ROCR)
    rocplot=function(pred, truth, ...){
      predob = prediction(pred, truth)
      perf = performance(predob , "tpr", "fpr")
      plot(perf,...)
    }
    fitted=attributes(predict(poly.mod,dat[-train,],decision.values=T))$decision.values
    rocplot(-fitted,dat[-train,"labs"],main="Test Data") #for some reason, I have to make the 
    #fitted values negative for the ROC curve to display correctly.
    fitted=attributes(predict(exp.mod,dat[-train,],decision.values=T))$decision.values
    rocplot(-fitted,dat[-train,"labs"],add=T,col="red")
    ```
    The ROC curve tells us that the radial kernel works best. The tables below show how the final models classified. They ended up performing just as well as each other on the data.
    ```{r echo=TRUE}
    poly.fit=predict(poly.mod,dat[-train,])
    table(poly.fit,dat[-train,]$labs)
    ```
    ```{r echo=TRUE}
    exp.fit=predict(poly.mod,dat[-train,])
    table(exp.fit,dat[-train,]$labs)
    ```

5.
    a. I will copy their example.
    ```{r echo=TRUE}
    set.seed(927)
    x1<-runif(500)-0.5
    x2<-runif(500)-0.5
    yn <- (x1^2-x2^2 > 0)*1
    y<-factor(yn)
    dat <- data.frame(y,x1,x2)
    ```
    b.
    ```{r echo=TRUE}
    plot(x1,x2,col=yn+1)
    ```
    c. 
    ```{r echo=TRUE}
    log.mod <- glm(y ~ x1+x2,family=binomial,data=dat)
    ```
    d.
    ```{r echo=TRUE}
    y.pred<-predict(log.mod,type="response")
    y.pred<-1*(y.pred>.5)
    plot(x1,x2,col=y.pred+1)
    ```
    Note that not a single value was predicted to be 0.
    ```{r echo=TRUE}
    any(predict(log.mod,type="response")<.5)
    ```
    e.
    ```{r echo=TRUE}
    log.nl.mod <- glm(y ~ poly(x1,d=3)*poly(x2,d=3),family=binomial)
    summary(log.nl.mod)
    ```
    Logistic regression does poorly at capturing the model.
    f.
    ```{r echo=TRUE}
    y.pred<-predict(log.nl.mod,type="response")
    y.pred<-1*(y.pred>.5)
    plot(x1,x2,col=y.pred+1)
    ```
    Looks better, but there is still misclassification.
    g.
    ```{r echo=TRUE}
    svm.lin.mod <- svm(y ~ x1 + x2, data=dat, kernel="linear")
    svm.lin.pred <- predict(svm.lin.mod)
    plot(x1,x2,col=svm.lin.pred)
    ```
    h. 
    ```{r echo=TRUE}
    svm.mod <- svm(y ~ x1 + x2, data=dat, kernel="polynomial", d=3, cost=1)
    svm.pred <- predict(svm.mod)
    plot(x1,x2,col=svm.pred)
    ```
    ```{r echo=TRUE}
    svm.mod <- svm(y ~ x1 + x2, data=dat, kernel="radial", cost=1)
    svm.pred <- predict(svm.mod)
    plot(x1,x2,col=svm.pred)
    ```
    ```{r echo=TRUE}
    svm.trn.mod <- svm(y~poly(x1,d=2)*poly(x2,d=2),data=dat,kernel="linear")
    svm.trn.pred <- predict(svm.trn.mod)
    plot(x1,x2,col=svm.trn.pred)
    ```
    i. The SvM with the transformed predictors and linear kernel seems to do the best. 

6. 
    a. + b.
    ```{r echo=TRUE}
    set.seed(927)
    n<-50
    x<-matrix(runif(n*2),ncol=2)
    y<-factor(x[,2]>x[,1])
    dat<-data.frame(x,y)
    costs <- 10^(seq(-3,3,by=.5))
    res<-tune(svm,y~X1+X2,data=dat,kernel="linear",ranges=list(cost=costs))
    summary(res)
    ```
    The best cost is `r res$best.parameters$cost`.
    c.
    ```{r echo=TRUE}
    set.seed(1)
    best.cost <- res$best.parameters$cost
    test.x <- matrix(runif(10*n*2),ncol=2)
    test.y<-factor(test.x[,2]>test.x[,1])
    test.dat<-data.frame(test.x,y=test.y)
    cost.performances <- double(length(costs))
    for(i in seq_along(costs))
    {
      c <- costs[i]
      svm.mod <- svm(y ~ X1 + X2, data=dat, kernel="linear",cost=c)
      pred.y <- predict(svm.mod,test.dat)
      cost.performances[i] <- mean(pred.y==test.y)
    }
    cbind(costs,cost.performances)
    ```
    d. 
    
7.
    a.
    ```{r echo=TRUE}
    data(Auto,package="ISLR")
    Auto$mpg01 <- with(Auto,factor(mpg > median(mpg)))
    ```
    b.
    ```{r echo=TRUE}
    set.seed(1)
    costs <- 10^(seq(-2,4,by=.5))
    lin.res <- tune(svm, mpg01 ~ ., data=Auto, ranges=list(cost=costs), kernel="linear")
    lin.res
    ```
    c.
    ```{r echo=TRUE}
    set.seed(2)
    gammas <- 10^(seq(-3,-1,by=1))
    costs2 <- 10^(seq(0,4,by=1))
    rad.res <- tune(svm, mpg01 ~ ., data=Auto, ranges=list(cost=costs,gamma=gammas),kernel="radial")
    summary(rad.res)
    ```
    ```{r echo=TRUE}
    set.seed(3)
    poly.res <- tune(svm, mpg01 ~ ., data=Auto, ranges=list(cost=costs,d=2:4),kernel="polynomial")
    summary(poly.res)
    ```
    The error was 0.0101.
    d. We know that the mpg varies a lot with weight and acceleration.
    ```{r}
    
    svm.lin <- lin.res$best.model
    plot(svm.lin, Auto, weight ~ displacement)
    
    svm.rad <- rad.res$best.model
    plot(svm.rad, Auto, weight ~ displacement)
  
    svm.poly <- poly.res$best.model
    plot(svm.poly, Auto, weight ~ displacement)
    ```
    We can see that 
  
8.
    a.
    ```{r}
    data(OJ,package="ISLR")
    set.seed(927)
    train <- sample.int(nrow(OJ), size=800)
    
    ```
    b.
    ```{r}
    library(e1071)
    svm.class <- svm(Purchase ~ ., data=OJ[train,], cost=0.01, kernel="linear")
    summary(svm.class)
    ```
    c. 
    ```{r}
    #Training error
    class.train.pred <- predict(svm.class)
    pur.train <- OJ[train,"Purchase"]
    (class.train.err <- mean(pur.train != class.train.pred))
    ```
    ```{r}
    #Test error
    class.test.pred <- predict(svm.class, OJ[-train,])
    pur.test <- OJ[-train,"Purchase"]
    (class.test.err <- mean(pur.test != class.test.pred))
    ```
    d. 
    ```{r}
    set.seed(927)
    costs<-10^seq(-2,2,by=.25)
    class.res <- tune(svm, Purchase ~ ., data=OJ[train,], ranges=list(cost=costs), kernel="linear")
    summary(class.res)
    ```
    e.
    ```{r}
    #Training error
    #svm.best.class <- svm(Purchase ~ ., data=OJ[train,], cost=class.res$best.parameters$cost, kernel="linear")
    svm.best.class <- class.res$best.model
    svm.best.class.pred <- predict(svm.best.class)
    (best.class.train.err <- mean(pur.train != svm.best.class.pred))
    
    #Test error
    svm.best.class.test <- predict(svm.best.class, OJ[-train,])
    (best.class.test.err <- mean(pur.test != svm.best.class.test))
    ```
    