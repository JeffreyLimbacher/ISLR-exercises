---
output:
  pdf_document: default
  html_document: default
---
# Linear Regression {#chap}

1. The first null hypothesis, related to the `intercept` term stats that we can conclude that `sales` will not be zero if we spend no money on TV, radio, or newspaper advertisting. The rest of the null hypotheses test whether TV, radio, or newspaper have any effect on sales. From the `Intercept` p-value, we can conclude that if we spend no money on advertising, we would expect to some sales. The second and third p-values state that if we spend money or TV and radio, we can expect some sort of return on investment in terms of sales. From the `newspaper` p-value, we cannot reject the null hypothesis that radio has no effect on sales.

2. The KNN classifier attempts to fit the $Y$ into some set of categories. For example, if we have the MPG, weight, and horsepower of a car, we might try to determine if it is a SUV, sedan, sports car, etc. KNN regression attempts to try and guess some quantitative values from the neighbors of a given point. For example, if we had the height and gender of someone, we can guess their weight by trying to find the $k$ nearest neighbors in a height $\times$ gender and average the $k$ weight values to come up with a best guess for the weight of the someone.

3.
    a. Our equation would look like
    $$ 50+20X_1 + 0.7X_2 + 35 X_3 + 0.01X_1 X_2 - 10 X_1 X_3.$$
    To figure out when females earn more, we check
    $$50+20X_1 + 0.7X_2 + 35 + 0.01X_1 X_2 - 10 X_1  > 50+20X_1 + 0.7X_2 + 0.01X_1 X_2 $$
    $$ -10X_1 +35 > 0$$
    $$ X_1 < 3.5 $$
    In otherwords, in order for women to have a higher salary, they need to have a GPA of less than 3.5, so iii. is the correct answer.
    b. We can use R to calculate this.
    ```{r include=TRUE, echo=TRUE}
    x1 <- 4.0; x2 <- 110; x3 <- 1; 
    (50 + 20 * x1 + .7 * x2 + 35*x3 + 0.01*x1*x2 - 10 * x1*x3 )
    ```
    Or about $206,000.
    c. False. If the standard error for $\beta_4$ is very small ($<.0001$ for example), then the t-statistics will be very large giving a large p-value. We cannot say anything about the evidence of an interaction without the p-value. 
4.
    a. We would expect the cubic regression to have a lower RSS. Note that if we set $\beta_2=0$ and $\beta_3=0$, then we get the linear regression model. Thus, if the linear regression model had a lower RSS, then we wouldn't have gotten the *least squares* fit of cubic model. This is a contradiction, so we have that the cubic model must have a lower RSS.
    b. We would expect that the test RSS to be lower for the linear regression model. The cubic model would have fit to noise, and therefore would have higher variance.
    c. We would still expect the training RSS to be lower for the cubic polynomial. 
    d. There is not enough information to tell. For example, maybe it is linear for the most part, but curves slightly at one end of the data set. Then we would expect the linear model to perform very well except for maybe some bias towards the end. The cubic regression would still overfit on the bulk of the data, but capture this tiny curve at the end well, but overall would be worse. On the other hand, we could have a truly cubic relationship between $X$ and $Y$. 
    
5. \begin{align*}
\hat{y}_i &= x_i \hat{\beta} \\
&= x_i \left(\sum_{j=1}^n x_{j}y_j \right) / \left(\sum_{i'=1}^n x_{i'}^2 \right) \\
&= \sum_{j=1}^n \left( \frac{x_i x_j }{\sum_{i'=1}^n x_{i'}^2} \right)   y_j \\
&= \sum_{j=1}^n a_j y_j
\end{align*}
where $a_j = \frac{x_i x_j }{\sum_{i'=1}^n x_{i'}^2}$.

6. Our least squares line is $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x$. If we plug in $\bar{x}$ for $x$, we get $\hat{y} = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 \bar{x} = \bar{y}$. This means $(\bar{x},\bar{y})$ satisfies our equation, so the line passes through it.

7. This will take a lot of math. Maybe another time.

Lab Exercises.

8. 
```{r include=TRUE, echo=TRUE}
data(Auto,package="ISLR")
head(Auto)
```
    a. 
    ```{r include=TRUE, echo=TRUE}
    mpg_mod <- lm(mpg ~ horsepower, data=Auto)
    summary(mpg_mod)
    ```
        i. We see that the response has a p-value that is extremely small, so we reject the null hypothesis that `horsepower` does not influence `mpg`. 
        ii. The $R^2$ is .6095 indicating that the relationship is moderate. The RSE is almost 5, so we could expect to be off by about 5 MPGs.
        iii. Negative.
        iv. 
        ```{r include=TRUE, echo=TRUE}
        ans<-sapply(c("confidence","prediction"), function(x) predict(mpg_mod,data.frame(horsepower=98),interval=x))
        rownames(ans)<-colnames(predict(mpg_mod,data.frame(horsepower=98),interval="prediction"))
        ans
        ```
    b.
    ```{r include=TRUE, echo=TRUE}
    plot(Auto$horsepower,Auto$mpg)
    abline(mpg_mod)
    ```
    c. From the plots below, we can see that there is a clear non-linearity to the residual line versus the fitted values. It has a curve remniscient of the residuals from the `sales` example. In addition, this affects the normal QQ plot where we can see we have residuals that are too large and too small. 
    ```{r include=TRUE, echo=TRUE}
    plot(mpg_mod)
    ```
    
9.
    a.
    ```{r include=TRUE, echo=TRUE}
    pairs(Auto)
    ```
    b.
    ```{r include=TRUE, echo=TRUE}
    cor(Auto[,!(names(Auto) %in% c("name"))])
    ```
    c. I turn the origin into a factor variable since that it is what it is meant to be. 
    ```{r include=TRUE, echo=TRUE}
    Auto2 <- Auto
    Auto2$origin <- factor(Auto$origin)
    mpg_all<-lm(mpg ~ .-name, data=Auto2)
    summary(mpg_all)
    ```
        i. We can see the F-statistic is very large which indicates that there is a relationship between response and predictor.
        ii. It appears that the intercept, `displacement`, `weight`, `year`, and `origin` are all statistically significant at pretty low confidence levels. It is interesting to see that `horsepower` has a low p-value. This is because `horsepower` is highly colinear with `displacement` and `weight`.
        iii. The coefficient for the year variable is positive indicating that cars have gotten more gas efficient over the years.
    d. Looking at the diagnostic plots, we can see that mpg has a nonlinear trend after fitting the data. In addition, we see that some of the residuals have extremely large standardized residuals which seems to violate heteroscedascity. These large values appear towards the right side of the residuals vs fitted plot creating a funnel shape. In addition, we have a one (two?) high leverage points.
    ```{r echo=TRUE, include=TRUE}
    plot(mpg_all)
    ```
    e. I (somewhat arbitrarily) chose to interact origin and year, and then horsepower with weight.
    ```{r include=TRUE, echo=TRUE}
    mpg_int<-lm(mpg ~ .-name + origin*year + horsepower*weight, data=Auto2)
    summary(mpg_int)
    plot(mpg_int)
    ```
    It appears I may have gotten a little lucky with my choices! It appears that there is a strong interaction between year and origin2, which means European cars and year interact pretty well. In addition, the interaction with horsepower and weight ended up pretty significant. The model also managed to get diminished the non-linear effect on the residuals, but we still got the conical effect of the residuals growing vs the fitted values.
    f. I'm going to remove the high leverage point because it got a little ridiculous how much leverage it has. I compare the mpg against the inverse of horsepoewr and inverse of weight and its interaction. The result is that they come out statistically significant.
    ```{r include=TRUE, echo=TRUE}
    mpg_int<-lm(mpg ~ .-name + origin*year + I(1/horsepower)*I(1/weight), data=Auto2[-14,])
    summary(mpg_int)
    plot(mpg_int)
    ```

10. 
    ```{r include=TRUE, echo=TRUE}
    data(Carseats,package="ISLR")
    head(Carseats)
    ```
    a. 
    ```{r include=TRUE, echo=TRUE}
    sales_mod <- lm(Sales ~ Price + Urban + US, data=Carseats)
    summary(sales_mod)
    plot(sales_mod)
    ```
    b. The diagnostic plots look pretty good but the model isn't very good. The price coefficient says that we can expect less units to sell at a higher prince. No surprise there. The urban coefficient would suggest that we sell slightly less in an urban location, but the p-value on this is extremely large, so we can safely dismiss this coefficient. The location being within the US is very important to sales.
    c. I'm not sure what the point of this question is
    $$ \mathrm{Sales} = 13.0 - 0.0544X_1 - 0.0219X_2 + 1.20 X_3$$
    where $X_1$ is the Price, $X_2$ is 1 if we are in an Urban setting, and $X_3$ is 1 if we are in the United States.
    d. We can reject the null-hypothesis safely for `Price`, `USYes`.
    e. 
    ```{r include=TRUE, echo=TRUE}
    sales_mod2 <- lm(Sales ~ Price  + US, data=Carseats)
    summary(sales_mod2)
    ```
    f. We see that the two models have really low $R^2$, so fit the data not very well. 
    g. 
    ```{r include=TRUE, echo=TRUE}
    confint(sales_mod2)
    ```
    h. From the book we don't want our leveraget o be too much larger than $(p+1)/n = (2+1)/400 = .0075$. There is one observation that exceeds this by a lot, but its standarized residual isn't very large. As far as I can tell, the most problematic observation is possibly 368.
    ```{r include=TRUE, echo=TRUE}
    plot(sales_mod2,5)
    ```
    
11. 
```{r include=TRUE, echo=TRUE}
set.seed(1)
x=rnorm(100)
y=2*x+rnorm(100);
```
    a. 
    ```{r include=TRUE, echo=TRUE}
    y_mod <- lm(y ~ x + 0)
    summary(y_mod)
    confint(y_mod)
    ```
    We got the estimate $\hat{\beta} = 1.9939$. We can see that the 95% confidence interval contains our true slope of 2. 
    b. 
    ```{r include=TRUE, echo=TRUE}
    x_mod <- lm(x ~ y  + 0)
    summary(x_mod)
    confint(x_mod)
    ```
    We get a coefficient estimate of .391 with a standard error of .02. This is far from our expected value of .5. 
    c. We can see that the t-statistic for both tests is the same. 
    d. We start with
    $$ \frac{\frac{\sum_{i=1} x_i y_i}{\sum_{i'=1}^n x_{i'}^2}}{\sqrt{\frac{\sum_{i=1}^n(y_i - x_i \hat{\beta})^2}{(n-1)\sum_{i'=1}^n x_{i'}^2}}} = \frac{(\sqrt{n-1})\sum_{i=1}^n x_iy_i}{\sqrt{(\sum_{i'=1}x_{i'}^2) \sum_{i=1}^n(y_i - x_i \hat{\beta})^2}} $$
    We already got what we wanted on top, so let's focus on the bottom (and drop the square root)
    $$ (\sum_{i'=1}x_{i'}^2) \sum_{i=1}^n(y_i - x_i \hat{\beta})^2 = (\sum_{i'=1}x_{i'}^2) \sum_{i=1}^n y_i^2 - 2 y_i x_i \hat{\beta} + x_i^2\hat{\beta}^2 $$
    Substituting $\hat{\beta} = (\sum_{i=1}^n x_i y_i) / (\sum_{i'=1}^n x_{i'}^2)$ we get
    $$ (\sum_{i'=1}x_{i'}^2) \sum_{i=1}^n y_i^2 - 2 y_i x_i \frac{(\sum_{j=1}^n x_j y_j)}{(\sum_{i'=1}^n x_{i'}^2)} + x_i^2\frac{(\sum_{j=1}^n x_j y_j)^2}{(\sum_{i'=1}^n x_{i'}^2)^2} = (\sum_{i'=1}x_{i'}^2)(\sum_{i=1}^n y_i^2 ) - 2 (\sum_{i=1}^n y_i x_i)(\sum_{j=1}x_jy_j) + (\sum_{i=1}x_i^2)\frac{(\sum_{j=1}^n x_j y_j)^2}{\sum_{i'=1}x_{i'}^2} $$
    Cancelling terms, we get
    $$(\sum_{i'=1}x_{i'}^2)(\sum_{i=1}^n y_i^2 ) - 2 (\sum_{i=1}^n y_i x_i)(\sum_{j=1}x_jy_j) + (\sum_{i=1}x_i^2)\frac{(\sum_{j=1}^n x_j y_j)^2}{\sum_{i'=1}x_{i'}^2} = (\sum_{i'=1}x_{i'}^2)(\sum_{i=1}^n y_i^2 ) - 2 (\sum_{i=1}^n y_i x_i)^2 + (\sum_{j=1}^n x_j y_j)^2 = (\sum_{i'=1}x_{i'}^2)(\sum_{i=1}^n y_i^2 ) -  (\sum_{i=1}^n y_i x_i)^2 $$
    This gives us the identity we wanted.
    To confirm it numerically, 
    ```{r include=TRUE, echo=TRUE}
    n<-100; xy<-sum(x*y); x2<-sum(x^2); y2<-sum(y^2)
    beta<-xy/x2
    sebeta<-sqrt(sum((y-x*beta)^2)/((n-1)*x2))
    t1<-beta/sebeta
    t2<-sqrt(n-1)*xy/sqrt(x2*y2 - xy^2)
    print(c(t1,t2))
    ```
    e. We can see that the second form the t-statistic is symmetric with respect to $x$ and $y$, so there is no algebraic difference between the -statistic for `y` onto `x` and `x` onto `y`.
    f.
    ```{r include=TRUE, echo=TRUE}
    y_int <- lm(y ~ x)
    x_int <- lm(x ~ y)
    cbind(`y t-statistic`=coef(summary(y_int))["x","t value"],`x t-statistic`=coef(summary(x_int))["y","t value"])
    ```
12.
    a. Let $\hat{\beta}_y$ be the estimate for $\beta$ of `y` onto `x` and $\hat{\beta}_x$ be the estimate for $\beta$ for `x` onto `y`. Then
    $$\hat{\beta}_y = \hat{\beta}_x \implies \frac{\sum_{i=1}^n (x_iy_i)}{\sum_{i'=1} x_{i'}^2} = \frac{\sum_{i=1}^n (x_iy_i)}{\sum_{i'=1} y_{i'}^2} \implies \sum_{i'=1} x_{i'}^2 = \sum_{i'=1} y_{i'}^2$$
    b. The example from 11. has two different $\hat{\beta}$'s.
    c. The trick I do below is to create two samples. I then rescale the random `y` so that the sum of squares is the same as the `x` variable. This makes the two regression values the same.
    ```{r include=TRUE, echo=TRUE}
    set.seed(2)
    n <- 100
    x <- rnorm(n)
    y <- rnorm(n)
    y <- y/sqrt(sum(y^2))*sqrt(sum(x^2)) #Rescaling so sum of squares is same for x and y
    coef(summary(lm(y~x+0)))
    coef(summary(lm(x~y+0)))
    ```

13. 
    a.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    n <- 100
    x <- rnorm(n)
    head(x)
    ```
    b. 
    ```{r include=TRUE, echo=TRUE}
    eps <- rnorm(n, sd=0.25);
    ```
    c. The length of `y` is $n=100$. 
    ```{r include=TRUE, echo=TRUE}
    y <- -1+.5*x+eps
    ```
    d.
    ```{r include=TRUE, echo=TRUE}
    plot(x,y);abline(-1,.5)
    ```
    e. The coefficient estimates are $\hat{\beta}_0 = -1.00942$, $\hat{\beta}_1 =  0.49973$. These are pretty close. The true values are within 1 standard error.
    ```{r echo=TRUE}
    y_mod <- lm(y~x)
    summary(y_mod)
    ```
    f. The linear least squares line is in red. It almost perfectly overlaps the true $f$.
    ```{r include=TRUE, echo=TRUE}
    plot(x,y);abline(-1,.5);abline(coef(y_mod),col="red")
    legend(x="bottomright",
           c("True Line","Least Squares Line"),
           col=c("black","red"),
           lty=c(1,1))
    ```
    g. We can see evidence that the fit is better due to the $R^2$ being higher. The $R^2$ increased from 0.7784 to 0.7828.
    ```{r include=TRUE, echo=TRUE}
    y_quad <- lm(y ~ x+I(x^2))
    summary(y_quad)
    ```
    h. By reducing the standard deviation 5 times, we can see the $R^2$ went up dramatically in the model.The t-values of the coefficients also went way up, so we are much more confident in our guesses for the coefficients. 
    ```{r include=TRUE}
    set.seed(1)
    x <- rnorm(n,sd=.1)
    eps <- rnorm(n,sd=.1)
    y <- -1+.5*x+eps
    y_less <- lm(y ~ x)
    summary(y_less)
    plot(x,y);abline(-1,.5);abline(coef(y_less),col="red")
    legend(x="bottomright",
           c("True Line","Least Squares Line"),
           col=c("black","red"),
           lty=c(1,1))
    ```
    Now for the qudratic model.
    ```{r include=TRUE, echo=TRUE}
    y_less_q <- lm(y ~ x + I(x^2))
    summary(y_less_q)
    ```
    We see that the $R^2$ is basically the same as the linear model, suggesting that there was no improvement. In fact, the adjusted $R^2$ is lower by .0001, so this model is worse by that standard.
    i. I set $\sigma$ to 1.
    ```{r include=TRUE}
    set.seed(1)
    x <- rnorm(n)
    eps <- rnorm(n,sd=1)
    y2 <- -1+.5*x+eps
    y_more <- lm(y2 ~ x)
    summary(y_more)
    plot(x,y);abline(-1,.5);abline(coef(y_more),col="red")
    legend(x="bottomright",
           c("True Line","Least Squares Line"),
           col=c("black","red"),
           lty=c(1,1))
    ```
    The estimates for the coefficients are still good, but now the $R^2$ is pretty pitiful. Let's try the quadratic fit.
    ```{r include=TRUE, echo=TRUE}
    y_more_q <- lm(y2 ~ x + I(x^2))
    summary(y_less_q)
    ```
    Our $R^2$ went up a little, but more interestingly, the coefficient for the `x^2` term became statistically significant. However, the estimate for it is pretty tiny, it would be impercetible.

14.
    a.
    ```{r include=TRUE, echo=TRUE}
    set.seed(1)
    x1=runif(100)
    x2=0.5*x1+rnorm(100)/10
    y=2+2*x1+0.3*x2+rnorm(100)
    ```
    The linear model is $Y=2+2X_1+0.3X_2 + \epsilon$. The regression coefficients are $\beta_0=2$, $\beta_1 =2$, $\beta_2=0.3$.
    b. The two variables are highly correlated. we can also see that they have a linear relationship.
    ```{r include=TRUE, echo=TRUE}
    cor(x1,x2)
    plot(x1,x2)
    ```
    c. 
    ```{r include=TRUE, echo=TRUE}
    y_mod <- lm(y ~ x1 + x2)
    summary(y_mod)
    ```
    From the summary, $\hat{\beta}_0=2.1305$, $\hat{\beta}_1=1.4296$, $\hat{\beta}_2=0.9097$. We can see that the estimates for $\hat{\beta}_1$ and $\hat{\beta}_2$ are not terribly close to the true estimate. We can reject the null hypothesis that $H_0:\beta_1=0$ at $\alpha=0.05$, but we cannot reject $H_0 : \beta_2 =0$. 
    d.
    ```{r include=TRUE, echo=TRUE}
    y_mod <- lm(y ~ x1)
    summary(y_mod)
    ```
    The evidence against the null has become much stronger in this case, and we can safely reject $H_0 : \beta_1 = 0$ at any reasonable confidence level. The estimate for $\beta_1$ is much closer to the true value as well. 
    e.
    ```{r include=TRUE, echo=TRUE}
    y_mod <- lm(y ~ x2)
    summary(y_mod)
    ```
    The results here suggest that we can reject $H_0: \beta_2 = 0$, but it appears that the estimate for the coefficient is off by quite a bit.
    f. Due to colinearity, the results do contradict each other. We said that `x2` was not significant. Now we say it is highly significant. 
    g.
    ```{r include=TRUE, echo=TRUE}
    x1=c(x1,0.1)
    x2=c(x2,0.8)
    y=c(y,6)
    y_mod_12 <- lm(y ~ x1 + x2)
    summary(y_mod_12)
    ```
    For the multiple linear regression, we cam see `x2` has become significant while `x1` has become non-significant.
    ```{r include=TRUE, echo=TRUE}
    y_mod_1 <- lm(y ~ x1)
    summary(y_mod_1)
    ```
    For the linear regression of `y` onto `x1`, we can see that the estimate is now worse than before, and the standard error of $\hat{\beta}_1$ has increased. The $R^2$ dropped from .2024 to .1562, which is a serious drop for one extra observation.
    ```{r include=TRUE, echo=TRUE}
    y_mod_2 <- lm(y ~ x2)
    summary(y_mod_2)
    ```
    As for `x2`, now our estimate is even more off, but the $R^2$ got even higher.
    There are some interesting trade offs going between the `x1` and `x2` models. It seems like the one observation reversed the roles of the variables. To see why, let's investigate where our observation went versus the bulk of the data using a plot of `x2` vs `x1`.
    ```{r include=TRUE, echo=TRUE}
    plot(x1,x2)
    ```
    We can see from the plot that this observation is a serious outlier. It does not seem that far off from the rest of the data, so it is likely not a high leverage point.
    
15.
```{r include=TRUE, echo=TRUE}
data(Boston,package="MASS")
head(Boston)
```
    a. A good opportunity to automate things.
    ```{r include=TRUE, echo=TRUE}
    preds<-names(Boston)[-1]
    mods<-list()
    stats<-list()
    for (i in seq(along=preds)){
      p<-preds[i]
      mods[[i]] <- lm(as.formula(paste0("crim ~ ",p)),data=Boston)
    }
    stats<-t(sapply(mods, function(x) coef(summary(x))[2,]))
    r_squared<-sapply(mods,function(x) summary(x)$r.squared)
    stats<-cbind(stats,r_squared)
    rownames(stats) <-preds
    (stats2<-stats[order(abs(stats[,"t value"]),decreasing=TRUE),])
    ```
    As we can see, a lot of these variables come out statistically significant. The only one we might not reject the null hypothesis is `chas`. I'll plot the first four.
    ```{r include=TRUE, echo=TRUE}
    best_vars<-rownames(stats)[1:4]
    par(mfcol=c(2,2))
    for (best_var in best_vars) {
      plot(Boston[,best_var],Boston[,"crim"],ylab="crim",xlab=best_var)
    }
    ```
    Wow. These plots looks like trash. It appears that even though we have strong evidence against the null hypothesis, the relationships are not very good.
    b. 
    ```{r include=TRUE, echo=TRUE}
    multi_mod<-lm(crim ~ ., data=Boston)
    summary(multi_mod)
    ```
    First things first, we see that the F-statistic is very large. I choose $\alpha=.005$. Under this, we can reject the null hypothesis on `dis`, `rad`, and `medv`.
    c. We can see that every variable would reject the null hypothesis in part a. However, in part b, we only reject the null hypothesis for 3 variables. 
    d. The automation pays off here.
    ```{r include=TRUE, echo=TRUE}
    plot(stats[,1],coef(multi_mod)[-1])
    ```
    It's hard to see what's happening in the plot, so let's just look at the coefficients next to each other.
    ```{r include=TRUE, echo=TRUE}
    cbind(stats[,1],coef(multi_mod)[-1])
    ```
    We can see from the table that the simple linear models did indeed give much larger coefficients than the multiple linear regression did. For some of them, we can even see that the multiple linear regression gave a coefficient that was opposte in sign, such as `ptratio`.
    d.
    